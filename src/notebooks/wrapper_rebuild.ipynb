{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b07cecc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from typing import Iterable, Dict, Callable, Tuple, Union\n",
    "from copy import deepcopy\n",
    "# from model.unet import UNet2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49abcf80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a127f96",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 68>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# class ModelAdapter(nn.Module):\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#     def __init__(\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#         self, \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m \n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m#         return hook\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mModelAdapter\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;124;03m\"\"\"Wrapper class for segmentation models and feature transformations.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    Wraps (a copy of) the segmentation model and attaches feature\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m    hooks as well as different types for inference training and inspection.\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     79\u001b[0m         seg_model: nn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m         copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     83\u001b[0m     ):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# class ModelAdapter(nn.Module):\n",
    "#     def __init__(\n",
    "#         self, \n",
    "#         model: nn.Module,\n",
    "#         transformations: nn.ModuleDict,\n",
    "#         copy: bool = True\n",
    "#     ):\n",
    "#         super(ModelAdapter, self).__init__()\n",
    "#         self.model = deepcopy(model) if copy else model\n",
    "#         self.transformations = transformations\n",
    "\n",
    "#         # init handles for hooks to find/modify them later\n",
    "#         self.transformation_handles = {}\n",
    "#         # init temporary storage for loss components\n",
    "#         self.data = {}\n",
    "\n",
    "#     def hook_inference_transformations(\n",
    "#         self, transformations: Dict[str, nn.Module]\n",
    "#     ) -> None:\n",
    "#         for layer_id in transformations:\n",
    "#             layer = self.model.get_submodule(layer_id)\n",
    "#             hook = self._get_inference_transformation_hook(\n",
    "#                 transformations[layer_id]\n",
    "#             )\n",
    "#             self.transformation_handles[\n",
    "#                 layer_id\n",
    "#             ] = layer.register_forward_pre_hook(hook)\n",
    "\n",
    "\n",
    "#     def _get_inference_transformation_hook(\n",
    "#         self, \n",
    "#         transformation: nn.Module, \n",
    "#         layer_id: str\n",
    "#     ) -> Callable:\n",
    "#         def hook(module: nn.Module, x: Tuple[Tensor]) -> Tensor:\n",
    "#             x_in, *_ = x  # weird tuple, can use x_in = x[0] instead\n",
    "#             # x_out = transformation(x_in)\n",
    "\n",
    "#             if self.training:\n",
    "#                 batch_size = x_in.shape[0] // 2\n",
    "#                 x_orig, _  = torch.split(x_in, batch_size)\n",
    "#                 x_in_denoised = transformation(x_in)\n",
    "#                 mse = nn.functional.mse_loss(\n",
    "#                     x_in_denoised, \n",
    "#                     x_orig.repeat(2,1,1,1).detach(),\n",
    "#                     reduction=\"mean\"\n",
    "#                 )\n",
    "#                 self.data[layer_id] = mse\n",
    "            \n",
    "            \n",
    "\n",
    "#             return transformation(x_in)\n",
    "#             if n_samples == 0:\n",
    "#                 return x\n",
    "#             elif n_samples == -1:\n",
    "#                 x_in_new = transformation(x_in)\n",
    "#                 return x_in_new\n",
    "#             else:\n",
    "#                 x_in_new = x_in.unsqueeze(1).repeat(1, n_samples, 1, 1, 1).flatten(0, 1)\n",
    "#                 x_in_new = transformation(x_in_new)\n",
    "#                 return torch.cat([x_in, x_in_new], dim=0)\n",
    "\n",
    "#         return hook\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ModelAdapter(nn.Module):\n",
    "    \"\"\"Wrapper class for segmentation models and feature transformations.\n",
    "\n",
    "    Wraps (a copy of) the segmentation model and attaches feature\n",
    "    trasformations to it via hooks (at potentially various positions\n",
    "    simultaneously). Additionally, it provides control utilities for the\n",
    "    hooks as well as different types for inference training and inspection.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        seg_model: nn.Module,\n",
    "        transformations: nn.ModuleDict,\n",
    "        disabled_ids: list = [],\n",
    "        copy: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.seg_model = deepcopy(seg_model) if copy else seg_model\n",
    "\n",
    "        self.transformations = transformations\n",
    "        self.disabled_ids = disabled_ids\n",
    "        self.transformation_handles = {}\n",
    "        self.train_transformation_handles = {}\n",
    "        self.inspect_transformation_handles = {}\n",
    "        self.training_data = {}\n",
    "        self.inspect_data = {}\n",
    "\n",
    "\n",
    "    def hook_train_transformations(\n",
    "        self, \n",
    "        transformations: Dict[str, nn.Module]\n",
    "    ) -> None:\n",
    "        for layer_id in transformations:\n",
    "            layer = self.seg_model.get_submodule(layer_id)\n",
    "            hook = self._get_train_transformation_hook(\n",
    "                transformations[layer_id], layer_id\n",
    "            )\n",
    "            self.train_transformation_handles[\n",
    "                layer_id\n",
    "            ] = layer.register_forward_pre_hook(hook)\n",
    "\n",
    "\n",
    "    def hook_inference_transformations(\n",
    "        self, transformations: Dict[str, nn.Module], \n",
    "        n_samples: int\n",
    "    ) -> None:\n",
    "        for layer_id in transformations:\n",
    "            layer = self.seg_model.get_submodule(layer_id)\n",
    "            hook = self._get_inference_transformation_hook(\n",
    "                transformations[layer_id],\n",
    "                n_samples\n",
    "            )\n",
    "            self.transformation_handles[layer_id] = layer.register_forward_pre_hook(\n",
    "                hook\n",
    "            )\n",
    "            \n",
    "\n",
    "    def hook_inspect_transformation(\n",
    "        self, \n",
    "        transformations: Dict[str, nn.Module], \n",
    "    ) -> None:\n",
    "        for layer_id in transformations:\n",
    "            if layer_id not in self.disabled_ids:\n",
    "                layer = self.seg_model.get_submodule(layer_id)\n",
    "                hook  = self._get_inspect_transformation_hook(transformations[layer_id], layer_id)\n",
    "                self.inspect_transformation_handles[\n",
    "                    layer_id\n",
    "                ] = layer.register_forward_pre_hook(hook)\n",
    "\n",
    "\n",
    "    def _get_train_transformation_hook(\n",
    "        self,\n",
    "        transformation: nn.Module,\n",
    "        layer_id: str\n",
    "    ) -> Callable:\n",
    "        def hook(module: nn.Module, x: Tuple[Tensor]) -> Tensor:\n",
    "            x_in, *_ = x  # tuple, alternatively use x_in = x[0]\n",
    "            #print(x_in.shape, x_in.shape[0] // 2)\n",
    "            batch_size = x_in.shape[0] // 2\n",
    "            x_orig, _  = torch.split(x_in, batch_size)\n",
    "            x_in_denoised = transformation(x_in)\n",
    "            \n",
    "            if layer_id not in self.disabled_ids:\n",
    "                mse = nn.functional.mse_loss(\n",
    "                    x_in_denoised, \n",
    "                    x_orig.repeat(2,1,1,1).detach(),\n",
    "                    reduction=\"mean\"\n",
    "                )\n",
    "\n",
    "                training_data = {\n",
    "                    \"mse\": mse,\n",
    "                }\n",
    "\n",
    "                self.training_data[layer_id] = training_data\n",
    "\n",
    "            return torch.cat([x_orig, x_in_denoised[batch_size:]], dim=0)\n",
    "            \n",
    "        return hook\n",
    "    \n",
    "\n",
    "    def _get_inference_transformation_hook(\n",
    "        self, transformation: nn.Module, n_samples: int = 1\n",
    "    ) -> Callable:\n",
    "        def hook(module: nn.Module, x: Tuple[Tensor]) -> Tensor:\n",
    "            x_in, *_ = x  # weird tuple, can use x_in = x[0]\n",
    "            if n_samples == 0:\n",
    "                return x\n",
    "            elif n_samples == -1:\n",
    "                x_in_new = transformation(x_in)\n",
    "                return x_in_new\n",
    "            else:\n",
    "                x_in_new = x_in.unsqueeze(1).repeat(1, n_samples, 1, 1, 1).flatten(0, 1)\n",
    "                x_in_new = transformation(x_in_new)\n",
    "                return torch.cat([x_in, x_in_new], dim=0)\n",
    "\n",
    "        return hook\n",
    "    \n",
    "    \n",
    "    def _get_inspect_transformation_hook(\n",
    "            self, \n",
    "            transformation: nn.Module, \n",
    "            layer_id: str, \n",
    "        ) -> Callable:\n",
    "        \n",
    "        @torch.no_grad()\n",
    "        def hook(module: nn.Module, x: Tuple[Tensor]) -> Tensor:\n",
    "            x_in, *_ = x  # weird tuple, can use x_in = x[0]\n",
    "            x_orig = x_in[:1]\n",
    "            x_in_denoised = transformation(x_in)\n",
    "            residuals     = x_in_denoised - x_in\n",
    "                \n",
    "            if layer_id not in self.disabled_ids:\n",
    "                data = {\n",
    "                    'input'     : x_in,\n",
    "                    'denoised'  : x_in_denoised,\n",
    "                    'residuals' : residuals\n",
    "                }\n",
    "                \n",
    "                self.inspect_data[layer_id] = data\n",
    "            \n",
    "            return torch.cat([x_orig, x_in_denoised[1:]], dim=0)\n",
    "        \n",
    "        return hook\n",
    "    \n",
    "\n",
    "    def remove_train_transformation_hook(self, layer_id: str) -> None:\n",
    "        self.train_transformation_handles[layer_id].remove()\n",
    "\n",
    "    def remove_transformation_hook(self, layer_id: str) -> None:\n",
    "        self.transformation_handles[layer_id].remove()\n",
    "        \n",
    "    def remove_inspect_transformation_hook(self, layer_id: str) -> None:\n",
    "        self.inspect_transformation_handles[layer_id].remove()\n",
    "\n",
    "    def remove_all_hooks(self):\n",
    "        if hasattr(self, \"train_transformation_handles\"):\n",
    "            for handle in self.train_transformation_handles:\n",
    "                self.train_transformation_handles[handle].remove()\n",
    "            self.train_transformation_handles = {}\n",
    "\n",
    "        if hasattr(self, \"transformation_handles\"):\n",
    "            for handle in self.transformation_handles:\n",
    "                self.transformation_handles[handle].remove()\n",
    "            self.transformation_handles = {}\n",
    "            \n",
    "        if hasattr(self, 'inspect_transformation_handles'):\n",
    "            for handle in self.inspect_transformation_handles:\n",
    "                self.inspect_transformation_handles[handle].remove()\n",
    "            self.inspect_transformation_handles = {}\n",
    "        \n",
    "\n",
    "    def freeze_seg_model(self):\n",
    "        self.seg_model.eval()\n",
    "        for param in self.seg_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def set_number_of_samples_to(self, n_samples: int):\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "    def disable(self, layer_ids: list) -> None:\n",
    "        for layer_id in layer_ids:\n",
    "            self.transformations[layer_id].turn_off()\n",
    "\n",
    "    def enable(self, layer_ids: list) -> None:\n",
    "        for layer_id in layer_ids:\n",
    "            self.transformations[layer_id].turn_on()\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return self.seg_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805fe6de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
