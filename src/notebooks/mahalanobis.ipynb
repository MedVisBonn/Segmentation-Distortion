{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports, settings, globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set CUDA device\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Callable, Tuple, List\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch #\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "from torch import Tensor, nn #\n",
    "import sys\n",
    "from omegaconf import OmegaConf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "sys.path.append('../')\n",
    "from data_utils import get_train_loader, get_eval_data\n",
    "from model.unet import get_unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load basic config\n",
    "cfg = OmegaConf.load('../configs/basic_config.yaml')\n",
    "OmegaConf.update(cfg, 'run.iteration', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNet Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set dataset, either brain or heart\n",
    "DATA_KEY = 'heart'\n",
    "OmegaConf.update(cfg, 'run.data_key', DATA_KEY)\n",
    "\n",
    "\n",
    "### get model\n",
    "# available models:\n",
    "#     - default-8\n",
    "#     - default-16\n",
    "#     - monai-8-4-4\n",
    "#     - monai-16-4-4\n",
    "#     - monai-16-4-8\n",
    "#     - monai-32-4-4\n",
    "#     - monai-64-4-4\n",
    "#     - swinunetr\n",
    "\n",
    "#unet_name = 'monai-64-4-4'\n",
    "unet_name = 'default-8'\n",
    "args = unet_name.split('-')\n",
    "cfg.unet[DATA_KEY].pre = unet_name\n",
    "cfg.unet[DATA_KEY].arch = args[0]\n",
    "cfg.unet[DATA_KEY].n_filters_init = None if unet_name == 'swinunetr' else int(args[1])\n",
    "if args[0] == 'monai':\n",
    "    cfg.unet[DATA_KEY].depth = int(args[2])\n",
    "    cfg.unet[DATA_KEY].num_res_units = int(args[3])\n",
    "\n",
    "unet, state_dict = get_unet(\n",
    "    cfg,\n",
    "    update_cfg_with_swivels=False,\n",
    "    return_state_dict=True)\n",
    "unet.load_state_dict(state_dict)\n",
    "_ = unet.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset\n",
      "loading all case properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset\n",
      "loading all case properties\n",
      "loading dataset\n",
      "loading all case properties\n",
      "\n",
      "Available datasets are: ['train', 'val', 'A']\n"
     ]
    }
   ],
   "source": [
    "# Set debug mode for only a small fraction of the datasets. Speeds up this cell by alot\n",
    "cfg.debug = False\n",
    "\n",
    "# update config with default values\n",
    "OmegaConf.update(\n",
    "    cfg, \n",
    "    'eval',\n",
    "    OmegaConf.load('../configs/eval/unet_config.yaml')\n",
    ")\n",
    "\n",
    "# Wether and how you want to subset in case of Brain data. WARNING:\n",
    "# After subsetting the eval below will not work with surface\n",
    "# Dice anymore, because volumes are fragmented. \n",
    "APPLY_SUBSETTING = True\n",
    "OmegaConf.update(cfg, 'eval.data.subset.apply', APPLY_SUBSETTING)\n",
    "subset_params = {\n",
    "    'n_cases': 256,  # selects at most so many cases\n",
    "    'fraction': 0.1, # selects from the 10% worst cases w.r.t to a model\n",
    "}\n",
    "OmegaConf.update(\n",
    "    cfg, \n",
    "    'eval.data.subset.params', \n",
    "    OmegaConf.create(subset_params)\n",
    ")\n",
    "\n",
    "if cfg.eval.data.subset.apply:\n",
    "    subset_dict = OmegaConf.to_container(\n",
    "        cfg.eval.data.subset.params, \n",
    "        resolve=True, \n",
    "        throw_on_missing=True\n",
    "    )\n",
    "    subset_dict['unet'] = unet\n",
    "else:\n",
    "    subset_dict = None\n",
    "\n",
    "### select the datasets within the domain\n",
    "# get training data\n",
    "cfg.eval.data.training = True\n",
    "# get validation data\n",
    "cfg.eval.data.validation = True\n",
    "# get testing data\n",
    "# Options for Brain are any subset of [1, 2, 3, 4, 5] or 'all' \n",
    "# Options for Heart are any subset of ['A', 'B', 'C', 'D'] or 'all'\n",
    "cfg.eval.data.testing = ['A']\n",
    "\n",
    "\n",
    "raw_data = get_eval_data(\n",
    "    train_set=cfg.eval.data.training,\n",
    "    val_set=cfg.eval.data.validation,\n",
    "    test_sets=cfg.eval.data.testing,    \n",
    "    cfg=cfg,\n",
    "    subset_dict=subset_dict\n",
    ")\n",
    "\n",
    "\n",
    "print(f'\\nAvailable datasets are: {list(raw_data.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset\n",
      "loading all case properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset\n",
      "loading all case properties\n"
     ]
    }
   ],
   "source": [
    "# get data\n",
    "train_loader, val_loader = get_train_loader(\n",
    "    training='unet', \n",
    "    cfg=cfg\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old Mahalanobis Detector Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Callable, Tuple, List\n",
    "\n",
    "\n",
    "import torch #\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "from torch import Tensor, nn #\n",
    "from torchmetrics import (\n",
    "    SpearmanCorrCoef, \n",
    "    AUROC,\n",
    "    ROC\n",
    ")\n",
    "from sklearn.covariance import LedoitWolf #\n",
    "\n",
    "from losses import DiceScoreCalgary, DiceScoreMMS #\n",
    "\n",
    "\n",
    "class PoolingMahalabonisDetector(nn.Module):\n",
    "    \"\"\"\n",
    "    Evaluation class for OOD and ESCE tasks based on https://arxiv.org/abs/2107.05975.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        model: nn.Module, \n",
    "        layer_ids: List[str], \n",
    "        train_loader: DataLoader, \n",
    "        valid_loader: DataLoader,\n",
    "        net_out: str,\n",
    "        criterion: nn.Module = DiceScoreCalgary(),\n",
    "        device: str = 'cuda:0'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device       = device\n",
    "        self.model        = model.to(device)\n",
    "        self.model.eval()\n",
    "        self.layer_ids    = layer_ids\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.net_out      = net_out\n",
    "        self.criterion    = criterion\n",
    "        self.pool         = nn.AvgPool3d(kernel_size=(2,2,2), stride=(2,2,2))\n",
    "        self.auroc        = AUROC(task = 'binary')\n",
    "        \n",
    "        # Init score dict for each layer:\n",
    "        self.latents   = {layer_id: [] for layer_id in self.layer_ids}\n",
    "        self.mu        = {layer_id: None for layer_id in self.layer_ids}\n",
    "        self.sigma_inv = {layer_id: None for layer_id in self.layer_ids}\n",
    "        self.dist      = {layer_id : 0 for layer_id in self.layer_ids}\n",
    "        \n",
    "        self._get_latents()\n",
    "        self._fit_gaussian_to_latents()\n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def _get_hook_fn(self, layer_id: str, mode: str = 'collect') -> Callable:\n",
    "        \n",
    "        def hook_fn(module: nn.Module, x: Tuple[Tensor]):\n",
    "            x = x[0]\n",
    "            while torch.prod(torch.tensor(x.shape[1:])) > 1e4:\n",
    "                x = self.pool(x)\n",
    "            x = self.pool(x)\n",
    "            batch_size = x.shape[0]\n",
    "\n",
    "            if mode == 'collect':\n",
    "                self.latents[layer_id].append(x.view(batch_size, -1).detach().cpu())\n",
    "            elif mode == 'single':\n",
    "                self.dist[layer_id] = x.view(batch_size, -1).to(self.device)\n",
    "                \n",
    "        return hook_fn\n",
    "    \n",
    "    \n",
    "    @torch.no_grad()        \n",
    "    def _get_latents(self) -> None:\n",
    "        handles = {}\n",
    "        for layer_id in self.layer_ids:\n",
    "            layer = self.model.get_submodule(layer_id)\n",
    "            hook  = self._get_hook_fn(layer_id, mode='collect')\n",
    "            handles[layer_id] = layer.register_forward_pre_hook(hook)\n",
    "\n",
    "        for batch in self.train_loader:\n",
    "            input_ = batch['input'].to(self.device)\n",
    "            _ = self.model(input_)\n",
    "                \n",
    "        for layer_id in handles:\n",
    "            self.latents[layer_id] = torch.cat(self.latents[layer_id], dim=0)\n",
    "            handles[layer_id].remove()\n",
    "        \n",
    "        \n",
    "    @torch.no_grad()         \n",
    "    def _fit_gaussian_to_latents(self) -> None:\n",
    "        for layer_id in self.layer_ids:\n",
    "            self.mu[layer_id] = self.latents[layer_id].mean(0, keepdims=True).to(self.device)\n",
    "            latents_centered = (self.latents[layer_id] - self.mu[layer_id].cpu()).detach().numpy()\n",
    "            sigma = torch.from_numpy(LedoitWolf().fit(latents_centered).covariance_)\n",
    "            self.sigma_inv[layer_id] = torch.linalg.inv(sigma).unsqueeze(0).to(self.device)\n",
    "            \n",
    "            \n",
    "    @torch.no_grad()\n",
    "    def testset_ood_detection(self, test_loader: DataLoader) -> Dict[str, torch.Tensor]:\n",
    "        \n",
    "        self.pred = {}\n",
    "        self.target = {}\n",
    "        \n",
    "        valid_dists = {layer_id : [] for layer_id in self.layer_ids}\n",
    "        for batch in self.valid_loader:\n",
    "            input_ = batch['input']\n",
    "            #print(input_.shape)\n",
    "            if self.net_out == 'calgary':\n",
    "                dist_volume = []\n",
    "                for input_chunk in input_:\n",
    "                    dist, _ = self.forward(input_chunk.unsqueeze(0).to(self.device))\n",
    "                    dist_volume.append(dist.copy())\n",
    "                dist = default_collate(dist_volume)\n",
    "            elif self.net_out == 'mms': \n",
    "                dist, _ = self.forward(input_.to(self.device))\n",
    "            for layer_id in self.layer_ids:\n",
    "                if self.net_out == 'calgary':\n",
    "                    valid_dists[layer_id].append(dist[layer_id].mean())\n",
    "                elif self.net_out == 'mms':\n",
    "                    valid_dists[layer_id].append(dist[layer_id])\n",
    "        self.valid_dists = valid_dists\n",
    "        self.valid_labels = {layer_id: torch.zeros(len(self.valid_dists[layer_id]), dtype=torch.uint8) \n",
    "                             for layer_id in self.layer_ids}\n",
    "        #print(len(self.valid_dists['up3']), len(self.valid_labels['up3']))\n",
    "            \n",
    "#             self.thresholds = {layer_id : 0 for layer_id in self.layer_ids}\n",
    "#             for layer_id in self.layer_ids:\n",
    "#                 if self.net_out == 'calgary':\n",
    "#                     valid_dists[layer_id] = torch.tensor(valid_dists[layer_id]).cpu()\n",
    "#                 elif self.net_out == 'mms':\n",
    "#                     valid_dists[layer_id] = torch.cat(valid_dists[layer_id], dim=0).cpu()\n",
    "#                 self.thresholds[layer_id] = torch.sort(valid_dists[layer_id])[0][len(valid_dists[layer_id]) - (len(valid_dists[layer_id]) // 20) - 1]\n",
    "                \n",
    "                    \n",
    "        test_dists = {layer_id : [] for layer_id in self.layer_ids}\n",
    "        for batch in test_loader:\n",
    "            input_ = batch['input']\n",
    "            if self.net_out == 'calgary':\n",
    "                dist_volume = []\n",
    "                for input_chunk in input_:\n",
    "                    dist, _ = self.forward(input_chunk.unsqueeze(0).to(self.device))\n",
    "                    dist_volume.append(dist.copy())\n",
    "                dist = default_collate(dist_volume)\n",
    "            elif self.net_out == 'mms': \n",
    "                dist, _ = self.forward(input_.to(self.device))\n",
    "            for layer_id in self.layer_ids:\n",
    "                if self.net_out == 'calgary':\n",
    "                    test_dists[layer_id].append(dist[layer_id].mean())\n",
    "                elif self.net_out == 'mms':    \n",
    "                    test_dists[layer_id].append(dist[layer_id])\n",
    "        \n",
    "        self.test_dists = test_dists\n",
    "        self.test_labels = {layer_id: torch.ones(len(self.test_dists[layer_id]), dtype=torch.uint8) \n",
    "                             for layer_id in self.layer_ids}\n",
    "            \n",
    "            \n",
    "        AUROC = {layer_id : 0 for layer_id in self.layer_ids}\n",
    "        for layer_id in self.layer_ids:\n",
    "            if self.net_out == 'calgary':\n",
    "                self.valid_dists[layer_id] = torch.tensor(self.valid_dists[layer_id]).cpu()\n",
    "                self.test_dists[layer_id]  = torch.tensor(self.test_dists[layer_id]).cpu()\n",
    "            elif self.net_out == 'mms':\n",
    "                self.valid_dists[layer_id] = torch.cat(self.valid_dists[layer_id], dim=0).cpu()\n",
    "                self.test_dists[layer_id]  = torch.cat(self.test_dists[layer_id], dim=0).cpu()\n",
    "            self.pred[layer_id]   = torch.cat([self.valid_dists[layer_id], self.test_dists[layer_id]]).squeeze()\n",
    "            self.target[layer_id] = torch.cat([self.valid_labels[layer_id], self.test_labels[layer_id]]).squeeze()\n",
    "            \n",
    "            print(self.pred[layer_id].shape, self.target[layer_id].shape)\n",
    "            \n",
    "            AUROC[layer_id] = self.auroc(self.pred[layer_id], self.target[layer_id])\n",
    "            #accuracy[layer_id] = ((test_dists[layer_id] > self.thresholds[layer_id]).sum() / len(test_dists[layer_id]))\n",
    "                \n",
    "        return AUROC\n",
    "    \n",
    "    \n",
    "    \n",
    "    @torch.no_grad()        \n",
    "    def testset_correlation(self, test_loader: DataLoader) -> Dict[str, torch.Tensor]:\n",
    "        corr_coeffs = {layer_id: SpearmanCorrCoef() for layer_id in self.layer_ids}\n",
    "        for batch in test_loader:\n",
    "            input_ = batch['input']\n",
    "            target = batch['target']\n",
    "            if self.net_out == 'calgary':\n",
    "                dist_volume = []\n",
    "                net_out_volume = []\n",
    "                for input_chunk in input_:\n",
    "                    dist, net_out = self.forward(input_chunk.unsqueeze(0).to(self.device))\n",
    "                    dist_volume.append(dist.copy())\n",
    "                    net_out_volume.append(net_out.cpu())\n",
    "                dist = default_collate(dist_volume)            \n",
    "                net_out = torch.cat(net_out_volume, dim=0)\n",
    "            \n",
    "            if self.net_out == 'mms':\n",
    "                target[target == -1] = 0\n",
    "                # convert to one-hot encoding\n",
    "                target = F.one_hot(target.long(), num_classes=4).squeeze(1).permute(0,3,1,2)\n",
    "                dist, net_out = self.forward(input_.to(self.device))            \n",
    "            loss = self.criterion(net_out.cpu(), target)\n",
    "\n",
    "            loss = loss.mean().float().cpu()\n",
    "            for layer_id in self.layer_ids:\n",
    "                corr_coeffs[layer_id].update(dist[layer_id].cpu().mean().view(1), 1-loss.view(1))\n",
    "\n",
    "        return corr_coeffs\n",
    "\n",
    "\n",
    "    @torch.no_grad()  \n",
    "    def forward(self, input_: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        handles = {}\n",
    "        for layer_id in self.layer_ids:\n",
    "            layer = self.model.get_submodule(layer_id)\n",
    "            hook  = self._get_hook_fn(layer_id, mode='single')\n",
    "            handles[layer_id] = layer.register_forward_pre_hook(hook)\n",
    "        \n",
    "        net_out = self.model(input_)\n",
    "        \n",
    "        for layer_id in self.layer_ids:\n",
    "            latent_centered = self.dist[layer_id].view(self.dist[layer_id].shape[0], 1, -1) - \\\n",
    "                self.mu[layer_id].unsqueeze(0)\n",
    "            self.dist[layer_id] = latent_centered @ self.sigma_inv[layer_id] @ \\\n",
    "                latent_centered.permute(0,2,1)\n",
    "            handles[layer_id].remove()\n",
    "            \n",
    "        return self.dist, net_out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactor Mahalanobis Detection  \n",
    "\n",
    "\n",
    "class Adapter:\n",
    "\n",
    "    attr:\n",
    "        swivel\n",
    "        hook_type: forward or backward\n",
    "    \n",
    "    methods:\n",
    "        \n",
    "\n",
    "\n",
    "class Transformation:\n",
    "\n",
    "\n",
    "    methods:\n",
    "        transform (forward ?)\n",
    "        score (mahalanobis or log likelihood https://stats.stackexchange.com/questions/97408/relation-of-mahalanobis-distance-to-log-likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoolingMahalanobisDetector(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        swivel:     str,\n",
    "        ledoitWolf: bool = True,\n",
    "        # hook_fn:    str  = 'pre',\n",
    "        transform:  bool = False,\n",
    "        device:     str  = 'cuda:0'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # init args\n",
    "        self.swivel = swivel\n",
    "        self.ledoitWolf = ledoitWolf\n",
    "        # self.hook_fn = self.register_forward_pre_hook if hook_fn == 'pre' else self.register_forward_hook\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "        # other attributes\n",
    "        self.training_representations = []\n",
    "        # methods\n",
    "        self._pool = nn.AvgPool3d(kernel_size=(2,2,2), stride=(2,2,2))\n",
    "\n",
    "\n",
    "    ### private methods ###\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _reduce(self, x: Tensor) -> Tensor:\n",
    "        # reduce dimensionality with 3D pooling to below 1e4 entries\n",
    "        while torch.prod(torch.tensor(x.shape[1:])) > 1e4:\n",
    "            x = self._pool(x)\n",
    "        x = self._pool(x)\n",
    "        # reshape to (batch_size, 1, n_features)\n",
    "        x = x.reshape(x.shape[0], 1, -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _collect(self, x: Tensor) -> None:\n",
    "        # reduces dimensionality, moves to cpu and stores\n",
    "        x = self._reduce(x).cpu()\n",
    "        self.training_representations.append(x)\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _merge(self) -> None:\n",
    "        # concatenate batches from training data\n",
    "        self.training_representations = torch.cat(\n",
    "            self.training_representations,\n",
    "            dim=0\n",
    "        )\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _estimate_gaussians(self) -> None:\n",
    "        self.mu = self.training_representations.mean(0, keepdims=True).detach().to(self.device)\n",
    "        if self.ledoitWolf:\n",
    "            self.sigma = torch.from_numpy(\n",
    "                LedoitWolf().fit(\n",
    "                    self.training_representations.squeeze(1)\n",
    "                ).covariance_\n",
    "            )\n",
    "        else:\n",
    "            self.sigma = torch.cov(self.training_representations.squeeze(1).T)\n",
    "        self.sigma_inv = torch.linalg.inv(self.sigma).detach().unsqueeze(0).to(self.device)\n",
    "\n",
    "\n",
    "    def _distance(self, x: Tensor) -> Tensor:\n",
    "        assert self.sigma_inv is not None, 'fit the model first'\n",
    "        # assert self.device == x.device, 'input and model device must match'\n",
    "        x_reduced  = self._reduce(x)\n",
    "        x_centered = x_reduced - self.mu\n",
    "        dist       = x_centered @ self.sigma_inv @ x_centered.permute(0,2,1)\n",
    "\n",
    "        return torch.sqrt(dist)\n",
    "\n",
    "\n",
    "    ### public methods ###\n",
    "\n",
    "    def fit(self):\n",
    "        self._merge()\n",
    "        self._estimate_gaussians()\n",
    "        del self.training_representations\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if self.training:\n",
    "            self._collect(x)\n",
    "        \n",
    "        else:\n",
    "            self.batch_distances = self._distance(x)\n",
    "\n",
    "        # implements identity function from a hooks perspective\n",
    "        if self.transform:\n",
    "            raise NotImplementedError('Implement transformation functionality')\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "    \n",
    "class PoolingMahalanobisWrapper(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        adapters: nn.ModuleList,\n",
    "        copy: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model           = deepcopy(model) if copy else model\n",
    "        self.adapters        = adapters\n",
    "        self.adapter_handles = {}\n",
    "\n",
    "\n",
    "    def hook_adapters(\n",
    "        self,\n",
    "    ) -> None:\n",
    "        for adapter in self.adapters:\n",
    "            swivel = adapter.swivel\n",
    "            layer  = self.model.get_submodule(swivel)\n",
    "            hook   = self._get_hook(adapter)\n",
    "            self.adapter_handles[\n",
    "                swivel\n",
    "            ] = layer.register_forward_pre_hook(hook)\n",
    "\n",
    "\n",
    "    def _get_hook(\n",
    "        self,\n",
    "        adapter: nn.Module\n",
    "    ) -> Callable:\n",
    "        def hook_fn(\n",
    "            module: nn.Module, \n",
    "            x: Tuple[Tensor]\n",
    "        ) -> Tensor:\n",
    "            x, *_ = x # tuple, alternatively use x_in = x[0]\n",
    "            x = adapter(x)\n",
    "            return x\n",
    "        \n",
    "        return hook_fn\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        x: Tensor\n",
    "    ) -> Tensor:\n",
    "        return self.model(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples and debugging\n",
    "device = 'cuda:0'\n",
    "\n",
    "detector = PoolingMahalanobisDetector(\n",
    "    swivel='up3.0.conv_path.0.bn',\n",
    "    device=device,\n",
    "    ledoitWolf=False\n",
    ")\n",
    "detector.train()\n",
    "wrapper = PoolingMahalanobisWrapper(\n",
    "    model=unet,\n",
    "    adapters=nn.ModuleList([detector])\n",
    ")\n",
    "\n",
    "wrapper.hook_adapters()\n",
    "\n",
    "loaders = {\n",
    "    domain: DataLoader(\n",
    "        raw_data[domain], \n",
    "        batch_size=32, \n",
    "        shuffle=False, \n",
    "        drop_last=False\n",
    "    ) for domain in raw_data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [01:42<00:00,  2.45it/s]\n"
     ]
    }
   ],
   "source": [
    "for _ in tqdm.tqdm(range(250)):\n",
    "    batch = next(train_loader)\n",
    "    x = batch['data'].to(device)\n",
    "    _ = wrapper(x)\n",
    "detector.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in loaders['train']:\n",
    "#     x = batch['input'].to(device)\n",
    "#     _ = wrapper(x)\n",
    "# detector.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:39<00:00,  2.55it/s]\n"
     ]
    }
   ],
   "source": [
    "detector.eval()\n",
    "metrics = {domain: [] for domain in raw_data}\n",
    "metrics['true_train'] = []\n",
    "for _ in tqdm.tqdm(range(100)):\n",
    "    batch = next(train_loader)\n",
    "    x = batch['data'].to(device)\n",
    "    _ = wrapper(x)\n",
    "    metrics['true_train'].append(detector.batch_distances.squeeze().cpu())\n",
    "\n",
    "metrics['true_val'] = []\n",
    "for _ in tqdm.tqdm(range(100)):\n",
    "    batch = next(val_loader)\n",
    "    x = batch['data'].to(device)\n",
    "    _ = wrapper(x)\n",
    "    metrics['true_val'].append(detector.batch_distances.squeeze().cpu())\n",
    "\n",
    "for domain in raw_data:\n",
    "    for batch in loaders[domain]:\n",
    "        x = batch['input'].to(device)\n",
    "        _ = wrapper(x)\n",
    "        metrics[domain].append(detector.batch_distances.squeeze().cpu())\n",
    "for domain in metrics:\n",
    "    metrics[domain] = torch.cat(metrics[domain], dim=0)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:09<00:00, 10.92it/s]\n"
     ]
    }
   ],
   "source": [
    "metrics['true_val'] = []\n",
    "for _ in tqdm.tqdm(range(100)):\n",
    "    batch = next(val_loader)\n",
    "    x = batch['data'].to(device)\n",
    "    _ = wrapper(x)\n",
    "    metrics['true_val'].append(detector.batch_distances.squeeze().cpu())\n",
    "\n",
    "metrics['true_val'] = torch.cat(metrics['true_val'], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain     Distance\n",
      "train      32.568\n",
      "val        34.559\n",
      "A          44.072\n",
      "true_train 34.044\n",
      "true_val   33.816\n"
     ]
    }
   ],
   "source": [
    "print('Domain     Distance')\n",
    "for domain in metrics:\n",
    "    print(\n",
    "        f'{domain.ljust(10)} {metrics[domain].mean().item():.3f}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Norm Mahalanobis / Log Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormMahalanobisDetector(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        swivel:    str,\n",
    "        transform: bool = False,\n",
    "        device:    str  = 'cuda:0'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # init args\n",
    "        self.swivel = swivel\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    ### private methods ###\n",
    "    \n",
    "    def _reduce(\n",
    "        self, \n",
    "        x: Tensor\n",
    "    ) -> Tensor:\n",
    "        x = x.mean(dim=(2,3)).unsqueeze(1)\n",
    "\n",
    "        return x\n",
    "        \n",
    "\n",
    "    # def _distance(\n",
    "    #     self, \n",
    "    #     x: Tensor\n",
    "    # ) -> Tensor:\n",
    "    #     x_reduced        = self._reduce(x)\n",
    "    #     x_squared_euclid = (x_reduced - self.mu)**2\n",
    "    #     dist = x_squared_euclid * self.sigma_inv\n",
    "\n",
    "    #     return torch.sqrt(dist)\n",
    "\n",
    "    def _distance(\n",
    "        self, \n",
    "        x: Tensor\n",
    "    ) -> Tensor:\n",
    "        x = self._reduce(x)\n",
    "        # print(\n",
    "        #     x.shape,\n",
    "        #     self.mu.shape,\n",
    "        #     self.sigma_inv.shape\n",
    "        # )\n",
    "        # torch.Size([4, 1, 64]) torch.Size([1, 64]) torch.Size([1, 64])\n",
    "        x = (x - self.mu)**2\n",
    "        dist = (x * self.sigma_inv).sum(dim=(1,2), keepdim=True)\n",
    "\n",
    "        return torch.sqrt(dist)\n",
    "\n",
    "\n",
    "    ### public methods ###\n",
    "    @torch.no_grad()\n",
    "    def store_bn_params(\n",
    "        self,\n",
    "        params\n",
    "    ) -> None:\n",
    "        self.mu  = params.running_mean.detach().unsqueeze(0).to(self.device)\n",
    "        self.var = params.running_var.detach().unsqueeze(0).to(self.device)\n",
    "        self.sigma_inv = 1 / torch.sqrt(self.var) \n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        x: Tensor\n",
    "    ) -> Tensor:\n",
    "        if self.training:\n",
    "            pass\n",
    "        else:\n",
    "            self.batch_distances = self._distance(x).detach()\n",
    "\n",
    "        # implements identity function from a hooks perspective\n",
    "        if self.transform:\n",
    "            raise NotImplementedError('Implement transformation functionality')\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "    \n",
    "class BatchNormMahalanobisWrapper(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        adapters: nn.ModuleList,\n",
    "        copy: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model           = deepcopy(model) if copy else model\n",
    "        self.adapters        = adapters\n",
    "        self.adapter_handles = {}\n",
    "\n",
    "\n",
    "    def hook_adapters(\n",
    "        self,\n",
    "    ) -> None:\n",
    "        for adapter in self.adapters:\n",
    "            swivel = adapter.swivel\n",
    "            layer  = self.model.get_submodule(swivel)\n",
    "            adapter.store_bn_params(layer)\n",
    "            hook = self._get_hook(adapter)\n",
    "            self.adapter_handles[\n",
    "                swivel\n",
    "            ] = layer.register_forward_pre_hook(hook)\n",
    "\n",
    "\n",
    "    def _get_hook(\n",
    "        self,\n",
    "        adapter: nn.Module\n",
    "    ) -> Callable:\n",
    "        def hook_fn(\n",
    "            module: nn.Module, \n",
    "            x: Tuple[Tensor]\n",
    "        ) -> Tensor:\n",
    "            x, *_ = x # tuple, alternatively use x_in = x[0]\n",
    "            x = adapter(x)\n",
    "            return x\n",
    "        \n",
    "        return hook_fn\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        x: Tensor\n",
    "    ) -> Tensor:\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_memlab import MemReporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples and debugging\n",
    "device = 'cuda:0'\n",
    "unet.to(device)\n",
    "\n",
    "detector = BatchNormMahalanobisDetector(\n",
    "    swivel='up3.0.conv_path.0.bn',\n",
    "    device=device,\n",
    ")\n",
    "detector.train()\n",
    "wrapper = BatchNormMahalanobisWrapper(\n",
    "    model=unet,\n",
    "    adapters=nn.ModuleList([detector])\n",
    ")\n",
    "\n",
    "wrapper.hook_adapters()\n",
    "wrapper.eval()\n",
    "\n",
    "loaders = {\n",
    "    domain: DataLoader(\n",
    "        raw_data[domain], \n",
    "        batch_size=32, \n",
    "        shuffle=False, \n",
    "        drop_last=False\n",
    "    ) for domain in raw_data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:37<00:00,  2.66it/s]\n",
      "100%|██████████| 100/100 [00:06<00:00, 15.09it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "detector.eval()\n",
    "metrics = {domain: [] for domain in raw_data}\n",
    "metrics['true_train'] = []\n",
    "for _ in tqdm.tqdm(range(100)):\n",
    "    batch = next(train_loader)\n",
    "    x = batch['data']\n",
    "    _ = wrapper(x.to(device))\n",
    "    metrics['true_train'].append(detector.batch_distances.squeeze().cpu())\n",
    "\n",
    "metrics['true_val'] = []\n",
    "for _ in tqdm.tqdm(range(100)):\n",
    "    batch = next(val_loader)\n",
    "    x = batch['data'].to(device)\n",
    "    _ = wrapper(x)\n",
    "    metrics['true_val'].append(detector.batch_distances.squeeze().cpu())\n",
    "\n",
    "for domain in raw_data:\n",
    "    for batch in loaders[domain]:\n",
    "        x = batch['input'].to(device)\n",
    "        _ = wrapper(x)\n",
    "        metrics[domain].append(detector.batch_distances.squeeze().cpu())\n",
    "for domain in metrics:\n",
    "    metrics[domain] = torch.cat(metrics[domain], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain    Distance\n",
      "train      0.940\n",
      "val        0.925\n",
      "A          0.865\n",
      "true_train 0.936\n",
      "true_val   0.914\n"
     ]
    }
   ],
   "source": [
    "print('Domain    Distance')\n",
    "for domain in metrics:\n",
    "    print(\n",
    "        f'{domain.ljust(10)} {metrics[domain].mean().item():.3f}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slice Wise uncertainty Measures\n",
    "\n",
    "Accuracy-Rejection (or [E]-AURC) get it from here: https://github.com/IML-DKFZ/values/blob/main/evaluation/metrics/aurc.py\n",
    "\n",
    "AUROC \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rc_curve_stats(\n",
    "    risks: np.array, confids: np.array\n",
    ") -> Tuple[List[float], List[float], List[float]]:\n",
    "    coverages = []\n",
    "    selective_risks = []\n",
    "    assert (\n",
    "        len(risks.shape) == 1 and len(confids.shape) == 1 and len(risks) == len(confids)\n",
    "    )\n",
    "\n",
    "    n_samples = len(risks)\n",
    "    idx_sorted = np.argsort(confids)\n",
    "\n",
    "    coverage = n_samples\n",
    "    error_sum = sum(risks[idx_sorted])\n",
    "\n",
    "    coverages.append(coverage / n_samples)\n",
    "    selective_risks.append(error_sum / n_samples)\n",
    "\n",
    "    weights = []\n",
    "\n",
    "    tmp_weight = 0\n",
    "    for i in range(0, len(idx_sorted) - 1):\n",
    "        coverage = coverage - 1\n",
    "        error_sum = error_sum - risks[idx_sorted[i]]\n",
    "        tmp_weight += 1\n",
    "        if i == 0 or confids[idx_sorted[i]] != confids[idx_sorted[i - 1]]:\n",
    "            coverages.append(coverage / n_samples)\n",
    "            selective_risks.append(error_sum / (n_samples - 1 - i))\n",
    "            weights.append(tmp_weight / n_samples)\n",
    "            tmp_weight = 0\n",
    "\n",
    "    # add a well-defined final point to the RC-curve.\n",
    "    if tmp_weight > 0:\n",
    "        coverages.append(0)\n",
    "        selective_risks.append(selective_risks[-1])\n",
    "        weights.append(tmp_weight / n_samples)\n",
    "\n",
    "    return coverages, selective_risks, weights\n",
    "\n",
    "\n",
    "def aurc(risks: np.array, confids: np.array):\n",
    "    _, risks, weights = rc_curve_stats(risks, confids)\n",
    "    return sum(\n",
    "        [(risks[i] + risks[i + 1]) * 0.5 * weights[i] for i in range(len(weights))]\n",
    "    )\n",
    "\n",
    "\n",
    "def eaurc(risks: np.array, confids: np.array):\n",
    "    \"\"\"Compute normalized AURC, i.e. subtract AURC of optimal CSF (given fixed risks).\"\"\"\n",
    "    n = len(risks)\n",
    "    # optimal confidence sorts risk. Asencding here because we start from coverage 1/n\n",
    "    selective_risks = np.sort(risks).cumsum() / np.arange(1, n + 1)\n",
    "    aurc_opt = selective_risks.sum() / n\n",
    "    return aurc(risks, confids) - aurc_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import (\n",
    "    SpearmanCorrCoef, \n",
    "    AUROC,\n",
    "    ROC\n",
    ")\n",
    "from torchmetrics.utilities.compute import _auc_compute_without_check\n",
    "\n",
    "\n",
    "class SliceWiseEvaluator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ret_auroc: bool = True,\n",
    "        ret_eaurc: bool = True,\n",
    "        ret_curves: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # args\n",
    "        self.ret_auroc = ret_auroc\n",
    "        self.ret_eaurc = ret_eaurc\n",
    "        self.ret_curves = ret_curves\n",
    "        # metrics\n",
    "        self.roc_values = ROC(task='binary')\n",
    "\n",
    "\n",
    "    def auroc(\n",
    "        self,\n",
    "        risks: Tensor, \n",
    "        confids: Tensor\n",
    "    ):\n",
    "        fpr, tpr, _ = self.roc_values(\n",
    "            confids,\n",
    "            risks\n",
    "        )\n",
    "\n",
    "        ret = _auc_compute_without_check(\n",
    "            fpr,\n",
    "            tpr,\n",
    "            1.0\n",
    "        )\n",
    "\n",
    "        if self.curves:\n",
    "            return ret, fpr, tpr\n",
    "        else:\n",
    "            return ret\n",
    "        \n",
    "\n",
    "    \n",
    "    def rc_curve_stats(\n",
    "        self,\n",
    "        risks: np.array, \n",
    "        confids: np.array\n",
    "    ) -> Tuple[List[float], List[float], List[float]]:\n",
    "        coverages = []\n",
    "        selective_risks = []\n",
    "        assert (\n",
    "            len(risks.shape) == 1 and len(confids.shape) == 1 and len(risks) == len(confids)\n",
    "        )\n",
    "\n",
    "        n_samples = len(risks)\n",
    "        idx_sorted = np.argsort(confids)\n",
    "\n",
    "        coverage = n_samples\n",
    "        error_sum = sum(risks[idx_sorted])\n",
    "\n",
    "        coverages.append(coverage / n_samples)\n",
    "        selective_risks.append(error_sum / n_samples)\n",
    "\n",
    "        weights = []\n",
    "\n",
    "        tmp_weight = 0\n",
    "        for i in range(0, len(idx_sorted) - 1):\n",
    "            coverage = coverage - 1\n",
    "            error_sum = error_sum - risks[idx_sorted[i]]\n",
    "            tmp_weight += 1\n",
    "            if i == 0 or confids[idx_sorted[i]] != confids[idx_sorted[i - 1]]:\n",
    "                coverages.append(coverage / n_samples)\n",
    "                selective_risks.append(error_sum / (n_samples - 1 - i))\n",
    "                weights.append(tmp_weight / n_samples)\n",
    "                tmp_weight = 0\n",
    "\n",
    "        # add a well-defined final point to the RC-curve.\n",
    "        if tmp_weight > 0:\n",
    "            coverages.append(0)\n",
    "            selective_risks.append(selective_risks[-1])\n",
    "            weights.append(tmp_weight / n_samples)\n",
    "\n",
    "        return coverages, selective_risks, weights\n",
    "\n",
    "\n",
    "    def aurc(\n",
    "        self,\n",
    "        risks: np.array, \n",
    "        confids: np.array\n",
    "    ):\n",
    "        _, risks, weights = rc_curve_stats(risks, confids)\n",
    "\n",
    "        ret = sum(\n",
    "            [(risks[i] + risks[i + 1]) * 0.5 * weights[i] for i in range(len(weights))]\n",
    "        )\n",
    "\n",
    "        if self.curves:\n",
    "            return ret, risks, weights\n",
    "        else:\n",
    "            return ret\n",
    "\n",
    "\n",
    "    def eaurc(\n",
    "        self,\n",
    "        risks: Tensor, \n",
    "        confids: Tensor\n",
    "    ):\n",
    "        \"\"\"Compute normalized AURC, i.e. subtract AURC of optimal CSF (given fixed risks).\"\"\"\n",
    "        n = len(risks)\n",
    "        risks = risks.to_numpy()\n",
    "        confids = confids.to_numpy()\n",
    "        # optimal confidence sorts risk. Asencding here because we start from coverage 1/n\n",
    "        selective_risks = np.sort(risks).cumsum() / np.arange(1, n + 1)\n",
    "        aurc_opt = selective_risks.sum() / n\n",
    "        if self.curves:\n",
    "            ret, risks, weights = aurc(risks, confids)\n",
    "            return torch.Tensor(ret - aurc_opt), torch.Tensor(risks), torch.Tensor(weights)\n",
    "        else:\n",
    "            return torch.Tensor(aurc(risks, confids) - aurc_opt)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        risks: Tensor,\n",
    "        confids: Tensor\n",
    "    ):\n",
    "        metrics = {}\n",
    "        if self.ret_auroc:\n",
    "            metrics['auroc'] = self.auroc(risks, confids)\n",
    "        if self.ret_eaurc:\n",
    "            metrics['eaurc'] = self.eaurc(risks, confids)\n",
    "\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected argument `target` to be an int or long tensor with ground truth labels but got tensor with dtype torch.float32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m confids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m      4\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m SliceWiseEvaluator()\n\u001b[0;32m----> 6\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfids\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/default/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36mSliceWiseEvaluator.forward\u001b[0;34m(self, risks, confids)\u001b[0m\n\u001b[1;32m    131\u001b[0m metrics \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mret_auroc:\n\u001b[0;32m--> 133\u001b[0m     metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauroc\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauroc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrisks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mret_eaurc:\n\u001b[1;32m    135\u001b[0m     metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meaurc\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meaurc(risks, confids)\n",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36mSliceWiseEvaluator.auroc\u001b[0;34m(self, risks, confids)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauroc\u001b[39m(\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     27\u001b[0m     risks: Tensor, \n\u001b[1;32m     28\u001b[0m     confids: Tensor\n\u001b[1;32m     29\u001b[0m ):\n\u001b[0;32m---> 30\u001b[0m     fpr, tpr, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroc_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrisks\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     ret \u001b[38;5;241m=\u001b[39m _auc_compute_without_check(\n\u001b[1;32m     36\u001b[0m         fpr,\n\u001b[1;32m     37\u001b[0m         tpr,\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m     39\u001b[0m     )\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurves:\n",
      "File \u001b[0;32m~/anaconda3/envs/default/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/default/lib/python3.8/site-packages/torchmetrics/metric.py:298\u001b[0m, in \u001b[0;36mMetric.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_full_state_update(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_reduce_state_update\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache\n",
      "File \u001b[0;32m~/anaconda3/envs/default/lib/python3.8/site-packages/torchmetrics/metric.py:367\u001b[0m, in \u001b[0;36mMetric._forward_reduce_state_update\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# allow grads for batch computation\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;66;03m# calculate batch state and compute batch value\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m batch_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# reduce batch and global state\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/default/lib/python3.8/site-packages/torchmetrics/metric.py:460\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad):\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 460\u001b[0m         \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    462\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected all tensors to be on\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "File \u001b[0;32m~/anaconda3/envs/default/lib/python3.8/site-packages/torchmetrics/classification/precision_recall_curve.py:162\u001b[0m, in \u001b[0;36mBinaryPrecisionRecallCurve.update\u001b[0;34m(self, preds, target)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"Update metric states.\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_args:\n\u001b[0;32m--> 162\u001b[0m     \u001b[43m_binary_precision_recall_curve_tensor_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m preds, target, _ \u001b[38;5;241m=\u001b[39m _binary_precision_recall_curve_format(preds, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthresholds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index)\n\u001b[1;32m    164\u001b[0m state \u001b[38;5;241m=\u001b[39m _binary_precision_recall_curve_update(preds, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthresholds)\n",
      "File \u001b[0;32m~/anaconda3/envs/default/lib/python3.8/site-packages/torchmetrics/functional/classification/precision_recall_curve.py:138\u001b[0m, in \u001b[0;36m_binary_precision_recall_curve_tensor_validation\u001b[0;34m(preds, target, ignore_index)\u001b[0m\n\u001b[1;32m    135\u001b[0m _check_same_shape(preds, target)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39mis_floating_point():\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected argument `target` to be an int or long tensor with ground truth labels\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got tensor with dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m     )\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m preds\u001b[38;5;241m.\u001b[39mis_floating_point():\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected argument `preds` to be an floating tensor with probability/logit scores,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got tensor with dtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreds\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected argument `target` to be an int or long tensor with ground truth labels but got tensor with dtype torch.float32"
     ]
    }
   ],
   "source": [
    "risks = torch.rand(1000)\n",
    "confids = torch.rand(1000)\n",
    "##\n",
    "# TODO:\n",
    "# AUROC needs confidence + binary error PER PIXEL\n",
    "# EAURC needs confidence + Dice PER SLICE\n",
    "evaluator = SliceWiseEvaluator()\n",
    "\n",
    "metrics = evaluator(risks, confids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
