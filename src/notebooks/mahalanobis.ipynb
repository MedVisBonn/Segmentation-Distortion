{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports, settings, globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set CUDA device\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Callable, Tuple, List\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch #\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "from torch import Tensor, nn #\n",
    "import sys\n",
    "from omegaconf import OmegaConf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "sys.path.append('../')\n",
    "from data_utils import get_train_loader, get_eval_data\n",
    "from model.unet import get_unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load basic config\n",
    "cfg = OmegaConf.load('../configs/basic_config.yaml')\n",
    "OmegaConf.update(cfg, 'run.iteration', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNet Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set dataset, either brain or heart\n",
    "DATA_KEY = 'heart'\n",
    "OmegaConf.update(cfg, 'run.data_key', DATA_KEY)\n",
    "\n",
    "\n",
    "### get model\n",
    "# available models:\n",
    "#     - default-8\n",
    "#     - default-16\n",
    "#     - monai-8-4-4\n",
    "#     - monai-16-4-4\n",
    "#     - monai-16-4-8\n",
    "#     - monai-32-4-4\n",
    "#     - monai-64-4-4\n",
    "#     - swinunetr\n",
    "\n",
    "#unet_name = 'monai-64-4-4'\n",
    "unet_name = 'default-8'\n",
    "args = unet_name.split('-')\n",
    "cfg.unet[DATA_KEY].pre = unet_name\n",
    "cfg.unet[DATA_KEY].arch = args[0]\n",
    "cfg.unet[DATA_KEY].n_filters_init = None if unet_name == 'swinunetr' else int(args[1])\n",
    "if args[0] == 'monai':\n",
    "    cfg.unet[DATA_KEY].depth = int(args[2])\n",
    "    cfg.unet[DATA_KEY].num_res_units = int(args[3])\n",
    "\n",
    "unet, state_dict = get_unet(\n",
    "    cfg,\n",
    "    update_cfg_with_swivels=False,\n",
    "    return_state_dict=True)\n",
    "unet.load_state_dict(state_dict)\n",
    "_ = unet.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset\n",
      "loading all case properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset\n",
      "loading all case properties\n",
      "loading dataset\n",
      "loading all case properties\n",
      "\n",
      "Available datasets are: ['train', 'val', 'A']\n"
     ]
    }
   ],
   "source": [
    "# Set debug mode for only a small fraction of the datasets. Speeds up this cell by alot\n",
    "cfg.debug = False\n",
    "\n",
    "# update config with default values\n",
    "OmegaConf.update(\n",
    "    cfg, \n",
    "    'eval',\n",
    "    OmegaConf.load('../configs/eval/unet_config.yaml')\n",
    ")\n",
    "\n",
    "# Wether and how you want to subset in case of Brain data. WARNING:\n",
    "# After subsetting the eval below will not work with surface\n",
    "# Dice anymore, because volumes are fragmented. \n",
    "APPLY_SUBSETTING = True\n",
    "OmegaConf.update(cfg, 'eval.data.subset.apply', APPLY_SUBSETTING)\n",
    "subset_params = {\n",
    "    'n_cases': 256,  # selects at most so many cases\n",
    "    'fraction': 0.1, # selects from the 10% worst cases w.r.t to a model\n",
    "}\n",
    "OmegaConf.update(\n",
    "    cfg, \n",
    "    'eval.data.subset.params', \n",
    "    OmegaConf.create(subset_params)\n",
    ")\n",
    "\n",
    "if cfg.eval.data.subset.apply:\n",
    "    subset_dict = OmegaConf.to_container(\n",
    "        cfg.eval.data.subset.params, \n",
    "        resolve=True, \n",
    "        throw_on_missing=True\n",
    "    )\n",
    "    subset_dict['unet'] = unet\n",
    "else:\n",
    "    subset_dict = None\n",
    "\n",
    "### select the datasets within the domain\n",
    "# get training data\n",
    "cfg.eval.data.training = True\n",
    "# get validation data\n",
    "cfg.eval.data.validation = True\n",
    "# get testing data\n",
    "# Options for Brain are any subset of [1, 2, 3, 4, 5] or 'all' \n",
    "# Options for Heart are any subset of ['A', 'B', 'C', 'D'] or 'all'\n",
    "cfg.eval.data.testing = ['A']\n",
    "\n",
    "\n",
    "raw_data = get_eval_data(\n",
    "    train_set=cfg.eval.data.training,\n",
    "    val_set=cfg.eval.data.validation,\n",
    "    test_sets=cfg.eval.data.testing,    \n",
    "    cfg=cfg,\n",
    "    subset_dict=subset_dict\n",
    ")\n",
    "\n",
    "\n",
    "print(f'\\nAvailable datasets are: {list(raw_data.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset\n",
      "loading all case properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset\n",
      "loading all case properties\n"
     ]
    }
   ],
   "source": [
    "# get data\n",
    "train_loader, val_loader = get_train_loader(\n",
    "    training='unet', \n",
    "    cfg=cfg\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old Mahalanobis Detector Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Callable, Tuple, List\n",
    "\n",
    "\n",
    "import torch #\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "from torch import Tensor, nn #\n",
    "from torchmetrics import (\n",
    "    SpearmanCorrCoef, \n",
    "    AUROC,\n",
    "    ROC\n",
    ")\n",
    "from sklearn.covariance import LedoitWolf #\n",
    "\n",
    "from losses import DiceScoreCalgary, DiceScoreMMS #\n",
    "\n",
    "\n",
    "class PoolingMahalabonisDetector(nn.Module):\n",
    "    \"\"\"\n",
    "    Evaluation class for OOD and ESCE tasks based on https://arxiv.org/abs/2107.05975.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        model: nn.Module, \n",
    "        layer_ids: List[str], \n",
    "        train_loader: DataLoader, \n",
    "        valid_loader: DataLoader,\n",
    "        net_out: str,\n",
    "        criterion: nn.Module = DiceScoreCalgary(),\n",
    "        device: str = 'cuda:0'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device       = device\n",
    "        self.model        = model.to(device)\n",
    "        self.model.eval()\n",
    "        self.layer_ids    = layer_ids\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.net_out      = net_out\n",
    "        self.criterion    = criterion\n",
    "        self.pool         = nn.AvgPool3d(kernel_size=(2,2,2), stride=(2,2,2))\n",
    "        self.auroc        = AUROC(task = 'binary')\n",
    "        \n",
    "        # Init score dict for each layer:\n",
    "        self.latents   = {layer_id: [] for layer_id in self.layer_ids}\n",
    "        self.mu        = {layer_id: None for layer_id in self.layer_ids}\n",
    "        self.sigma_inv = {layer_id: None for layer_id in self.layer_ids}\n",
    "        self.dist      = {layer_id : 0 for layer_id in self.layer_ids}\n",
    "        \n",
    "        self._get_latents()\n",
    "        self._fit_gaussian_to_latents()\n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def _get_hook_fn(self, layer_id: str, mode: str = 'collect') -> Callable:\n",
    "        \n",
    "        def hook_fn(module: nn.Module, x: Tuple[Tensor]):\n",
    "            x = x[0]\n",
    "            while torch.prod(torch.tensor(x.shape[1:])) > 1e4:\n",
    "                x = self.pool(x)\n",
    "            x = self.pool(x)\n",
    "            batch_size = x.shape[0]\n",
    "\n",
    "            if mode == 'collect':\n",
    "                self.latents[layer_id].append(x.view(batch_size, -1).detach().cpu())\n",
    "            elif mode == 'single':\n",
    "                self.dist[layer_id] = x.view(batch_size, -1).to(self.device)\n",
    "                \n",
    "        return hook_fn\n",
    "    \n",
    "    \n",
    "    @torch.no_grad()        \n",
    "    def _get_latents(self) -> None:\n",
    "        handles = {}\n",
    "        for layer_id in self.layer_ids:\n",
    "            layer = self.model.get_submodule(layer_id)\n",
    "            hook  = self._get_hook_fn(layer_id, mode='collect')\n",
    "            handles[layer_id] = layer.register_forward_pre_hook(hook)\n",
    "\n",
    "        for batch in self.train_loader:\n",
    "            input_ = batch['input'].to(self.device)\n",
    "            _ = self.model(input_)\n",
    "                \n",
    "        for layer_id in handles:\n",
    "            self.latents[layer_id] = torch.cat(self.latents[layer_id], dim=0)\n",
    "            handles[layer_id].remove()\n",
    "        \n",
    "        \n",
    "    @torch.no_grad()         \n",
    "    def _fit_gaussian_to_latents(self) -> None:\n",
    "        for layer_id in self.layer_ids:\n",
    "            self.mu[layer_id] = self.latents[layer_id].mean(0, keepdims=True).to(self.device)\n",
    "            latents_centered = (self.latents[layer_id] - self.mu[layer_id].cpu()).detach().numpy()\n",
    "            sigma = torch.from_numpy(LedoitWolf().fit(latents_centered).covariance_)\n",
    "            self.sigma_inv[layer_id] = torch.linalg.inv(sigma).unsqueeze(0).to(self.device)\n",
    "            \n",
    "            \n",
    "    @torch.no_grad()\n",
    "    def testset_ood_detection(self, test_loader: DataLoader) -> Dict[str, torch.Tensor]:\n",
    "        \n",
    "        self.pred = {}\n",
    "        self.target = {}\n",
    "        \n",
    "        valid_dists = {layer_id : [] for layer_id in self.layer_ids}\n",
    "        for batch in self.valid_loader:\n",
    "            input_ = batch['input']\n",
    "            #print(input_.shape)\n",
    "            if self.net_out == 'calgary':\n",
    "                dist_volume = []\n",
    "                for input_chunk in input_:\n",
    "                    dist, _ = self.forward(input_chunk.unsqueeze(0).to(self.device))\n",
    "                    dist_volume.append(dist.copy())\n",
    "                dist = default_collate(dist_volume)\n",
    "            elif self.net_out == 'mms': \n",
    "                dist, _ = self.forward(input_.to(self.device))\n",
    "            for layer_id in self.layer_ids:\n",
    "                if self.net_out == 'calgary':\n",
    "                    valid_dists[layer_id].append(dist[layer_id].mean())\n",
    "                elif self.net_out == 'mms':\n",
    "                    valid_dists[layer_id].append(dist[layer_id])\n",
    "        self.valid_dists = valid_dists\n",
    "        self.valid_labels = {layer_id: torch.zeros(len(self.valid_dists[layer_id]), dtype=torch.uint8) \n",
    "                             for layer_id in self.layer_ids}\n",
    "        #print(len(self.valid_dists['up3']), len(self.valid_labels['up3']))\n",
    "            \n",
    "#             self.thresholds = {layer_id : 0 for layer_id in self.layer_ids}\n",
    "#             for layer_id in self.layer_ids:\n",
    "#                 if self.net_out == 'calgary':\n",
    "#                     valid_dists[layer_id] = torch.tensor(valid_dists[layer_id]).cpu()\n",
    "#                 elif self.net_out == 'mms':\n",
    "#                     valid_dists[layer_id] = torch.cat(valid_dists[layer_id], dim=0).cpu()\n",
    "#                 self.thresholds[layer_id] = torch.sort(valid_dists[layer_id])[0][len(valid_dists[layer_id]) - (len(valid_dists[layer_id]) // 20) - 1]\n",
    "                \n",
    "                    \n",
    "        test_dists = {layer_id : [] for layer_id in self.layer_ids}\n",
    "        for batch in test_loader:\n",
    "            input_ = batch['input']\n",
    "            if self.net_out == 'calgary':\n",
    "                dist_volume = []\n",
    "                for input_chunk in input_:\n",
    "                    dist, _ = self.forward(input_chunk.unsqueeze(0).to(self.device))\n",
    "                    dist_volume.append(dist.copy())\n",
    "                dist = default_collate(dist_volume)\n",
    "            elif self.net_out == 'mms': \n",
    "                dist, _ = self.forward(input_.to(self.device))\n",
    "            for layer_id in self.layer_ids:\n",
    "                if self.net_out == 'calgary':\n",
    "                    test_dists[layer_id].append(dist[layer_id].mean())\n",
    "                elif self.net_out == 'mms':    \n",
    "                    test_dists[layer_id].append(dist[layer_id])\n",
    "        \n",
    "        self.test_dists = test_dists\n",
    "        self.test_labels = {layer_id: torch.ones(len(self.test_dists[layer_id]), dtype=torch.uint8) \n",
    "                             for layer_id in self.layer_ids}\n",
    "            \n",
    "            \n",
    "        AUROC = {layer_id : 0 for layer_id in self.layer_ids}\n",
    "        for layer_id in self.layer_ids:\n",
    "            if self.net_out == 'calgary':\n",
    "                self.valid_dists[layer_id] = torch.tensor(self.valid_dists[layer_id]).cpu()\n",
    "                self.test_dists[layer_id]  = torch.tensor(self.test_dists[layer_id]).cpu()\n",
    "            elif self.net_out == 'mms':\n",
    "                self.valid_dists[layer_id] = torch.cat(self.valid_dists[layer_id], dim=0).cpu()\n",
    "                self.test_dists[layer_id]  = torch.cat(self.test_dists[layer_id], dim=0).cpu()\n",
    "            self.pred[layer_id]   = torch.cat([self.valid_dists[layer_id], self.test_dists[layer_id]]).squeeze()\n",
    "            self.target[layer_id] = torch.cat([self.valid_labels[layer_id], self.test_labels[layer_id]]).squeeze()\n",
    "            \n",
    "            print(self.pred[layer_id].shape, self.target[layer_id].shape)\n",
    "            \n",
    "            AUROC[layer_id] = self.auroc(self.pred[layer_id], self.target[layer_id])\n",
    "            #accuracy[layer_id] = ((test_dists[layer_id] > self.thresholds[layer_id]).sum() / len(test_dists[layer_id]))\n",
    "                \n",
    "        return AUROC\n",
    "    \n",
    "    \n",
    "    \n",
    "    @torch.no_grad()        \n",
    "    def testset_correlation(self, test_loader: DataLoader) -> Dict[str, torch.Tensor]:\n",
    "        corr_coeffs = {layer_id: SpearmanCorrCoef() for layer_id in self.layer_ids}\n",
    "        for batch in test_loader:\n",
    "            input_ = batch['input']\n",
    "            target = batch['target']\n",
    "            if self.net_out == 'calgary':\n",
    "                dist_volume = []\n",
    "                net_out_volume = []\n",
    "                for input_chunk in input_:\n",
    "                    dist, net_out = self.forward(input_chunk.unsqueeze(0).to(self.device))\n",
    "                    dist_volume.append(dist.copy())\n",
    "                    net_out_volume.append(net_out.cpu())\n",
    "                dist = default_collate(dist_volume)            \n",
    "                net_out = torch.cat(net_out_volume, dim=0)\n",
    "            \n",
    "            if self.net_out == 'mms':\n",
    "                target[target == -1] = 0\n",
    "                # convert to one-hot encoding\n",
    "                target = F.one_hot(target.long(), num_classes=4).squeeze(1).permute(0,3,1,2)\n",
    "                dist, net_out = self.forward(input_.to(self.device))            \n",
    "            loss = self.criterion(net_out.cpu(), target)\n",
    "\n",
    "            loss = loss.mean().float().cpu()\n",
    "            for layer_id in self.layer_ids:\n",
    "                corr_coeffs[layer_id].update(dist[layer_id].cpu().mean().view(1), 1-loss.view(1))\n",
    "\n",
    "        return corr_coeffs\n",
    "\n",
    "\n",
    "    @torch.no_grad()  \n",
    "    def forward(self, input_: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        handles = {}\n",
    "        for layer_id in self.layer_ids:\n",
    "            layer = self.model.get_submodule(layer_id)\n",
    "            hook  = self._get_hook_fn(layer_id, mode='single')\n",
    "            handles[layer_id] = layer.register_forward_pre_hook(hook)\n",
    "        \n",
    "        net_out = self.model(input_)\n",
    "        \n",
    "        for layer_id in self.layer_ids:\n",
    "            latent_centered = self.dist[layer_id].view(self.dist[layer_id].shape[0], 1, -1) - \\\n",
    "                self.mu[layer_id].unsqueeze(0)\n",
    "            self.dist[layer_id] = latent_centered @ self.sigma_inv[layer_id] @ \\\n",
    "                latent_centered.permute(0,2,1)\n",
    "            handles[layer_id].remove()\n",
    "            \n",
    "        return self.dist, net_out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactor Mahalanobis Detection  \n",
    "\n",
    "\n",
    "class Adapter:\n",
    "\n",
    "    attr:\n",
    "        swivel\n",
    "        hook_type: forward or backward\n",
    "    \n",
    "    methods:\n",
    "        \n",
    "\n",
    "\n",
    "class Transformation:\n",
    "\n",
    "\n",
    "    methods:\n",
    "        transform (forward ?)\n",
    "        score (mahalanobis or log likelihood https://stats.stackexchange.com/questions/97408/relation-of-mahalanobis-distance-to-log-likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoolingMahalanobisDetector(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        swivel:     str,\n",
    "        ledoitWolf: bool = True,\n",
    "        # hook_fn:    str  = 'pre',\n",
    "        transform:  bool = False,\n",
    "        device:     str  = 'cuda:0'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # init args\n",
    "        self.swivel = swivel\n",
    "        self.ledoitWolf = ledoitWolf\n",
    "        # self.hook_fn = self.register_forward_pre_hook if hook_fn == 'pre' else self.register_forward_hook\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "        # other attributes\n",
    "        self.training_representations = []\n",
    "        # methods\n",
    "        self._pool = nn.AvgPool3d(kernel_size=(2,2,2), stride=(2,2,2))\n",
    "\n",
    "\n",
    "    ### private methods ###\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _reduce(self, x: Tensor) -> Tensor:\n",
    "        # reduce dimensionality with 3D pooling to below 1e4 entries\n",
    "        while torch.prod(torch.tensor(x.shape[1:])) > 1e4:\n",
    "            x = self._pool(x)\n",
    "        x = self._pool(x)\n",
    "        # reshape to (batch_size, 1, n_features)\n",
    "        x = x.reshape(x.shape[0], 1, -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _collect(self, x: Tensor) -> None:\n",
    "        # reduces dimensionality, moves to cpu and stores\n",
    "        x = self._reduce(x).cpu()\n",
    "        self.training_representations.append(x)\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _merge(self) -> None:\n",
    "        # concatenate batches from training data\n",
    "        self.training_representations = torch.cat(\n",
    "            self.training_representations,\n",
    "            dim=0\n",
    "        )\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _estimate_gaussians(self) -> None:\n",
    "        self.mu = self.training_representations.mean(0, keepdims=True).detach().to(self.device)\n",
    "        if self.ledoitWolf:\n",
    "            self.sigma = torch.from_numpy(\n",
    "                LedoitWolf().fit(\n",
    "                    self.training_representations.squeeze(1)\n",
    "                ).covariance_\n",
    "            )\n",
    "        else:\n",
    "            self.sigma = torch.cov(self.training_representations.squeeze(1).T)\n",
    "        self.sigma_inv = torch.linalg.inv(self.sigma).detach().unsqueeze(0).to(self.device)\n",
    "\n",
    "\n",
    "    def _distance(self, x: Tensor) -> Tensor:\n",
    "        assert self.sigma_inv is not None, 'fit the model first'\n",
    "        # assert self.device == x.device, 'input and model device must match'\n",
    "        x_reduced  = self._reduce(x)\n",
    "        x_centered = x_reduced - self.mu\n",
    "        dist       = x_centered @ self.sigma_inv @ x_centered.permute(0,2,1)\n",
    "\n",
    "        return torch.sqrt(dist)\n",
    "\n",
    "\n",
    "    ### public methods ###\n",
    "\n",
    "    def fit(self):\n",
    "        self._merge()\n",
    "        self._estimate_gaussians()\n",
    "        del self.training_representations\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if self.training:\n",
    "            self._collect(x)\n",
    "        \n",
    "        else:\n",
    "            self.batch_distances = self._distance(x)\n",
    "\n",
    "        # implements identity function from a hooks perspective\n",
    "        if self.transform:\n",
    "            raise NotImplementedError('Implement transformation functionality')\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "    \n",
    "class PoolingMahalanobisWrapper(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        adapters: nn.ModuleList,\n",
    "        copy: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model           = deepcopy(model) if copy else model\n",
    "        self.adapters        = adapters\n",
    "        self.adapter_handles = {}\n",
    "\n",
    "\n",
    "    def hook_adapters(\n",
    "        self,\n",
    "    ) -> None:\n",
    "        for adapter in self.adapters:\n",
    "            swivel = adapter.swivel\n",
    "            layer  = self.model.get_submodule(swivel)\n",
    "            hook   = self._get_hook(adapter)\n",
    "            self.adapter_handles[\n",
    "                swivel\n",
    "            ] = layer.register_forward_pre_hook(hook)\n",
    "\n",
    "\n",
    "    def _get_hook(\n",
    "        self,\n",
    "        adapter: nn.Module\n",
    "    ) -> Callable:\n",
    "        def hook_fn(\n",
    "            module: nn.Module, \n",
    "            x: Tuple[Tensor]\n",
    "        ) -> Tensor:\n",
    "            x, *_ = x # tuple, alternatively use x_in = x[0]\n",
    "            x = adapter(x)\n",
    "            return x\n",
    "        \n",
    "        return hook_fn\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        x: Tensor\n",
    "    ) -> Tensor:\n",
    "        return self.model(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples and debugging\n",
    "device = 'cuda:0'\n",
    "\n",
    "detector = PoolingMahalanobisDetector(\n",
    "    swivel='up3.0.conv_path.0.bn',\n",
    "    device=device,\n",
    "    ledoitWolf=False\n",
    ")\n",
    "detector.train()\n",
    "wrapper = PoolingMahalanobisWrapper(\n",
    "    model=unet,\n",
    "    adapters=nn.ModuleList([detector])\n",
    ")\n",
    "\n",
    "wrapper.hook_adapters()\n",
    "\n",
    "loaders = {\n",
    "    domain: DataLoader(\n",
    "        raw_data[domain], \n",
    "        batch_size=32, \n",
    "        shuffle=False, \n",
    "        drop_last=False\n",
    "    ) for domain in raw_data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [01:42<00:00,  2.45it/s]\n"
     ]
    }
   ],
   "source": [
    "for _ in tqdm.tqdm(range(250)):\n",
    "    batch = next(train_loader)\n",
    "    x = batch['data'].to(device)\n",
    "    _ = wrapper(x)\n",
    "detector.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in loaders['train']:\n",
    "#     x = batch['input'].to(device)\n",
    "#     _ = wrapper(x)\n",
    "# detector.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:39<00:00,  2.55it/s]\n"
     ]
    }
   ],
   "source": [
    "detector.eval()\n",
    "metrics = {domain: [] for domain in raw_data}\n",
    "metrics['true_train'] = []\n",
    "for _ in tqdm.tqdm(range(100)):\n",
    "    batch = next(train_loader)\n",
    "    x = batch['data'].to(device)\n",
    "    _ = wrapper(x)\n",
    "    metrics['true_train'].append(detector.batch_distances.squeeze().cpu())\n",
    "\n",
    "metrics['true_val'] = []\n",
    "for _ in tqdm.tqdm(range(100)):\n",
    "    batch = next(val_loader)\n",
    "    x = batch['data'].to(device)\n",
    "    _ = wrapper(x)\n",
    "    metrics['true_val'].append(detector.batch_distances.squeeze().cpu())\n",
    "\n",
    "for domain in raw_data:\n",
    "    for batch in loaders[domain]:\n",
    "        x = batch['input'].to(device)\n",
    "        _ = wrapper(x)\n",
    "        metrics[domain].append(detector.batch_distances.squeeze().cpu())\n",
    "for domain in metrics:\n",
    "    metrics[domain] = torch.cat(metrics[domain], dim=0)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:09<00:00, 10.92it/s]\n"
     ]
    }
   ],
   "source": [
    "metrics['true_val'] = []\n",
    "for _ in tqdm.tqdm(range(100)):\n",
    "    batch = next(val_loader)\n",
    "    x = batch['data'].to(device)\n",
    "    _ = wrapper(x)\n",
    "    metrics['true_val'].append(detector.batch_distances.squeeze().cpu())\n",
    "\n",
    "metrics['true_val'] = torch.cat(metrics['true_val'], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain     Distance\n",
      "train      32.568\n",
      "val        34.559\n",
      "A          44.072\n",
      "true_train 34.044\n",
      "true_val   33.816\n"
     ]
    }
   ],
   "source": [
    "print('Domain     Distance')\n",
    "for domain in metrics:\n",
    "    print(\n",
    "        f'{domain.ljust(10)} {metrics[domain].mean().item():.3f}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Norm Mahalanobis / Log Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormMahalanobisDetector(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        swivel:    str,\n",
    "        transform: bool = False,\n",
    "        device:    str  = 'cuda:0'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # init args\n",
    "        self.swivel = swivel\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "\n",
    "\n",
    "    ### private methods ###\n",
    "    \n",
    "    def _reduce(\n",
    "        self, \n",
    "        x: Tensor\n",
    "    ) -> Tensor:\n",
    "        x = x.mean(dim=(2,3)).unsqueeze(1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def _distance(\n",
    "        self, \n",
    "        x: Tensor\n",
    "    ) -> Tensor:\n",
    "        x = self._reduce(x)\n",
    "        x = (x - self.mu)**2\n",
    "        dist = (x * self.sigma_inv).sum(dim=(1,2), keepdim=True)\n",
    "\n",
    "        return torch.sqrt(dist)\n",
    "\n",
    "\n",
    "    ### public methods ###\n",
    "    @torch.no_grad()\n",
    "    def store_bn_params(\n",
    "        self,\n",
    "        params\n",
    "    ) -> None:\n",
    "        self.mu  = params.running_mean.detach().unsqueeze(0).to(self.device)\n",
    "        self.var = params.running_var.detach().unsqueeze(0).to(self.device)\n",
    "        self.sigma_inv = 1 / torch.sqrt(self.var) \n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        x: Tensor\n",
    "    ) -> Tensor:\n",
    "        if self.training:\n",
    "            pass\n",
    "        else:\n",
    "            self.batch_distances = self._distance(x).detach()\n",
    "\n",
    "        # implements identity function from a hooks perspective\n",
    "        if self.transform:\n",
    "            raise NotImplementedError('Implement transformation functionality')\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "    \n",
    "class BatchNormMahalanobisWrapper(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        adapters: nn.ModuleList,\n",
    "        copy: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model           = deepcopy(model) if copy else model\n",
    "        self.adapters        = adapters\n",
    "        self.adapter_handles = {}\n",
    "\n",
    "\n",
    "    def hook_adapters(\n",
    "        self,\n",
    "    ) -> None:\n",
    "        for adapter in self.adapters:\n",
    "            swivel = adapter.swivel\n",
    "            layer  = self.model.get_submodule(swivel)\n",
    "            adapter.store_bn_params(layer)\n",
    "            hook = self._get_hook(adapter)\n",
    "            self.adapter_handles[\n",
    "                swivel\n",
    "            ] = layer.register_forward_pre_hook(hook)\n",
    "\n",
    "\n",
    "    def _get_hook(\n",
    "        self,\n",
    "        adapter: nn.Module\n",
    "    ) -> Callable:\n",
    "        def hook_fn(\n",
    "            module: nn.Module, \n",
    "            x: Tuple[Tensor]\n",
    "        ) -> Tensor:\n",
    "            x, *_ = x # tuple, alternatively use x_in = x[0]\n",
    "            x = adapter(x)\n",
    "            return x\n",
    "        \n",
    "        return hook_fn\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        x: Tensor\n",
    "    ) -> Tensor:\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_memlab import MemReporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples and debugging\n",
    "device = 'cuda:0'\n",
    "unet.to(device)\n",
    "\n",
    "detector = BatchNormMahalanobisDetector(\n",
    "    swivel='up3.0.conv_path.0.bn',\n",
    "    device=device,\n",
    ")\n",
    "detector.train()\n",
    "wrapper = BatchNormMahalanobisWrapper(\n",
    "    model=unet,\n",
    "    adapters=nn.ModuleList([detector])\n",
    ")\n",
    "\n",
    "wrapper.hook_adapters()\n",
    "wrapper.eval()\n",
    "\n",
    "loaders = {\n",
    "    domain: DataLoader(\n",
    "        raw_data[domain], \n",
    "        batch_size=32, \n",
    "        shuffle=False, \n",
    "        drop_last=False\n",
    "    ) for domain in raw_data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:37<00:00,  2.66it/s]\n",
      "100%|██████████| 100/100 [00:06<00:00, 15.09it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "detector.eval()\n",
    "metrics = {domain: [] for domain in raw_data}\n",
    "metrics['true_train'] = []\n",
    "for _ in tqdm.tqdm(range(100)):\n",
    "    batch = next(train_loader)\n",
    "    x = batch['data']\n",
    "    _ = wrapper(x.to(device))\n",
    "    metrics['true_train'].append(detector.batch_distances.squeeze().cpu())\n",
    "\n",
    "metrics['true_val'] = []\n",
    "for _ in tqdm.tqdm(range(100)):\n",
    "    batch = next(val_loader)\n",
    "    x = batch['data'].to(device)\n",
    "    _ = wrapper(x)\n",
    "    metrics['true_val'].append(detector.batch_distances.squeeze().cpu())\n",
    "\n",
    "for domain in raw_data:\n",
    "    for batch in loaders[domain]:\n",
    "        x = batch['input'].to(device)\n",
    "        _ = wrapper(x)\n",
    "        metrics[domain].append(detector.batch_distances.squeeze().cpu())\n",
    "for domain in metrics:\n",
    "    metrics[domain] = torch.cat(metrics[domain], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain    Distance\n",
      "train      0.940\n",
      "val        0.925\n",
      "A          0.865\n",
      "true_train 0.936\n",
      "true_val   0.914\n"
     ]
    }
   ],
   "source": [
    "print('Domain    Distance')\n",
    "for domain in metrics:\n",
    "    print(\n",
    "        f'{domain.ljust(10)} {metrics[domain].mean().item():.3f}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slice Wise uncertainty Measures\n",
    "\n",
    "Accuracy-Rejection (or [E]-AURC) get it from here: https://github.com/IML-DKFZ/values/blob/main/evaluation/metrics/aurc.py\n",
    "\n",
    "AUROC \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rc_curve_stats(\n",
    "    risks: np.array, confids: np.array\n",
    ") -> Tuple[List[float], List[float], List[float]]:\n",
    "    coverages = []\n",
    "    selective_risks = []\n",
    "    assert (\n",
    "        len(risks.shape) == 1 and len(confids.shape) == 1 and len(risks) == len(confids)\n",
    "    )\n",
    "\n",
    "    n_samples = len(risks)\n",
    "    idx_sorted = np.argsort(confids)\n",
    "\n",
    "    coverage = n_samples\n",
    "    error_sum = sum(risks[idx_sorted])\n",
    "\n",
    "    coverages.append(coverage / n_samples)\n",
    "    selective_risks.append(error_sum / n_samples)\n",
    "\n",
    "    weights = []\n",
    "\n",
    "    tmp_weight = 0\n",
    "    for i in range(0, len(idx_sorted) - 1):\n",
    "        coverage = coverage - 1\n",
    "        error_sum = error_sum - risks[idx_sorted[i]]\n",
    "        tmp_weight += 1\n",
    "        if i == 0 or confids[idx_sorted[i]] != confids[idx_sorted[i - 1]]:\n",
    "            coverages.append(coverage / n_samples)\n",
    "            selective_risks.append(error_sum / (n_samples - 1 - i))\n",
    "            weights.append(tmp_weight / n_samples)\n",
    "            tmp_weight = 0\n",
    "\n",
    "    # add a well-defined final point to the RC-curve.\n",
    "    if tmp_weight > 0:\n",
    "        coverages.append(0)\n",
    "        selective_risks.append(selective_risks[-1])\n",
    "        weights.append(tmp_weight / n_samples)\n",
    "\n",
    "    return coverages, selective_risks, weights\n",
    "\n",
    "\n",
    "def aurc(risks: np.array, confids: np.array):\n",
    "    _, risks, weights = rc_curve_stats(risks, confids)\n",
    "    return sum(\n",
    "        [(risks[i] + risks[i + 1]) * 0.5 * weights[i] for i in range(len(weights))]\n",
    "    )\n",
    "\n",
    "\n",
    "def eaurc(risks: np.array, confids: np.array):\n",
    "    \"\"\"Compute normalized AURC, i.e. subtract AURC of optimal CSF (given fixed risks).\"\"\"\n",
    "    n = len(risks)\n",
    "    # optimal confidence sorts risk. Asencding here because we start from coverage 1/n\n",
    "    selective_risks = np.sort(risks).cumsum() / np.arange(1, n + 1)\n",
    "    aurc_opt = selective_risks.sum() / n\n",
    "    return aurc(risks, confids) - aurc_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import (\n",
    "    SpearmanCorrCoef, \n",
    "    AUROC,\n",
    "    ROC\n",
    ")\n",
    "from torchmetrics.utilities.compute import _auc_compute_without_check\n",
    "\n",
    "class AUROC(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        ret_curves: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # args\n",
    "        self.ret_curves = ret_curves\n",
    "        # metrics\n",
    "        self.roc_values = ROC(task='binary')\n",
    "\n",
    "\n",
    "    def auroc(\n",
    "        self,\n",
    "        confids: Tensor,\n",
    "        target: Tensor\n",
    "    ):\n",
    "        fpr, tpr, _ = self.roc_values(\n",
    "            confids,\n",
    "            target\n",
    "        )\n",
    "\n",
    "        ret = _auc_compute_without_check(\n",
    "            x=fpr,\n",
    "            y=tpr,\n",
    "            direction=1.0\n",
    "        )\n",
    "\n",
    "        if self.ret_curves:\n",
    "            return ret, fpr, tpr\n",
    "        else:\n",
    "            return ret\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        confids: Tensor,\n",
    "        target: Tensor\n",
    "    ):\n",
    "        return self.auroc(confids=confids, target=target)\n",
    "\n",
    "\n",
    "\n",
    "class EAURC(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        ret_curves: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # args\n",
    "        self.ret_curves = ret_curves\n",
    "        \n",
    "    \n",
    "    def rc_curve_stats(\n",
    "        self,\n",
    "        confids: np.array, \n",
    "        risks: np.array\n",
    "    ) -> Tuple[List[float], List[float], List[float]]:\n",
    "        coverages = []\n",
    "        selective_risks = []\n",
    "        assert (\n",
    "            len(risks.shape) == 1 and len(confids.shape) == 1 and len(risks) == len(confids)\n",
    "        )\n",
    "\n",
    "        n_samples = len(risks)\n",
    "        idx_sorted = np.argsort(confids)\n",
    "\n",
    "        coverage = n_samples\n",
    "        error_sum = sum(risks[idx_sorted])\n",
    "\n",
    "        coverages.append(coverage / n_samples)\n",
    "        selective_risks.append(error_sum / n_samples)\n",
    "\n",
    "        weights = []\n",
    "\n",
    "        tmp_weight = 0\n",
    "        for i in range(0, len(idx_sorted) - 1):\n",
    "            coverage = coverage - 1\n",
    "            error_sum = error_sum - risks[idx_sorted[i]]\n",
    "            tmp_weight += 1\n",
    "            if i == 0 or confids[idx_sorted[i]] != confids[idx_sorted[i - 1]]:\n",
    "                coverages.append(coverage / n_samples)\n",
    "                selective_risks.append(error_sum / (n_samples - 1 - i))\n",
    "                weights.append(tmp_weight / n_samples)\n",
    "                tmp_weight = 0\n",
    "\n",
    "        # add a well-defined final point to the RC-curve.\n",
    "        if tmp_weight > 0:\n",
    "            coverages.append(0)\n",
    "            selective_risks.append(selective_risks[-1])\n",
    "            weights.append(tmp_weight / n_samples)\n",
    "\n",
    "        return coverages, selective_risks, weights\n",
    "\n",
    "\n",
    "    def aurc(\n",
    "        self,\n",
    "        confids: np.array, \n",
    "        risks: np.array\n",
    "    ):\n",
    "        _, risks, weights = self.rc_curve_stats(confids=confids, risks=risks)\n",
    "\n",
    "        ret = sum(\n",
    "            [(risks[i] + risks[i + 1]) * 0.5 * weights[i] for i in range(len(weights))]\n",
    "        )\n",
    "\n",
    "        if self.ret_curves:\n",
    "            return ret, risks, weights\n",
    "        else:\n",
    "            return ret\n",
    "\n",
    "\n",
    "    def eaurc(\n",
    "        self,\n",
    "        confids: Tensor, \n",
    "        risks: Tensor\n",
    "    ):\n",
    "        \"\"\"Compute normalized AURC, i.e. subtract AURC of optimal CSF (given fixed risks).\"\"\"\n",
    "        n = len(risks)\n",
    "        confids = confids.numpy()\n",
    "        risks = risks.numpy()\n",
    "        # optimal confidence sorts risk. Asencding here because we start from coverage 1/n\n",
    "        selective_risks = np.sort(risks).cumsum() / np.arange(1, n + 1)\n",
    "        aurc_opt = selective_risks.sum() / n\n",
    "        ret, risks, weights = self.aurc(confids=confids, risks=risks)\n",
    "        if self.ret_curves:\n",
    "            \n",
    "            return ret - aurc_opt, ret, torch.Tensor(risks), torch.Tensor(weights)\n",
    "        else:\n",
    "            return ret - aurc_opt, ret\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        confids: Tensor,\n",
    "        risks: Tensor\n",
    "    ):\n",
    "        return self.eaurc(confids=confids, risks=risks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "risks = torch.rand(1000)\n",
    "confids = torch.rand(1000)\n",
    "##\n",
    "# TODO:\n",
    "# AUROC needs confidence + binary error PER PIXEL\n",
    "# EAURC needs confidence + Dice PER SLICE\n",
    "eaurc = EAURC()\n",
    "auroc = AUROC()\n",
    "\n",
    "eaurc_res, aurc_res, eaurc_risks, eaurc_weights = eaurc(confids=confids, risks=risks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "eaurc_res, aurc_res, eaurc_risks, eaurc_weights = eaurc(confids=confids, risks=risks)\n",
    "auroc_res, auroc_fpr, auroc_tpr = auroc(confids=confids, target=(risks>0.8)*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4877)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auroc_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f81b47b12b0>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(auroc_fpr, auroc_tpr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
