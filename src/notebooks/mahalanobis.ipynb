{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports, settings, globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set CUDA device\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import sys\n",
    "from omegaconf import OmegaConf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List\n",
    "\n",
    "sys.path.append('../')\n",
    "from data_utils import get_eval_data\n",
    "from model.unet import get_unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load basic config\n",
    "cfg = OmegaConf.load('../configs/basic_config.yaml')\n",
    "OmegaConf.update(cfg, 'run.iteration', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNet Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set dataset, either brain or heart\n",
    "DATA_KEY = 'heart'\n",
    "OmegaConf.update(cfg, 'run.data_key', DATA_KEY)\n",
    "\n",
    "\n",
    "### get model\n",
    "# available models:\n",
    "#     - default-8\n",
    "#     - default-16\n",
    "#     - monai-8-4-4\n",
    "#     - monai-16-4-4\n",
    "#     - monai-16-4-8\n",
    "#     - monai-32-4-4\n",
    "#     - monai-64-4-4\n",
    "#     - swinunetr\n",
    "\n",
    "#unet_name = 'monai-64-4-4'\n",
    "unet_name = 'swinunetr'\n",
    "args = unet_name.split('-')\n",
    "cfg.unet[DATA_KEY].pre = unet_name\n",
    "cfg.unet[DATA_KEY].arch = args[0]\n",
    "cfg.unet[DATA_KEY].n_filters_init = None if unet_name == 'swinunetr' else int(args[1])\n",
    "if args[0] == 'monai':\n",
    "    cfg.unet[DATA_KEY].depth = int(args[2])\n",
    "    cfg.unet[DATA_KEY].num_res_units = int(args[3])\n",
    "\n",
    "unet, state_dict = get_unet(\n",
    "    cfg,\n",
    "    update_cfg_with_swivels=False,\n",
    "    return_state_dict=True)\n",
    "unet.load_state_dict(state_dict)\n",
    "_ = unet.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset\n",
      "loading all case properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset\n",
      "loading all case properties\n",
      "loading dataset\n",
      "loading all case properties\n",
      "\n",
      "Available datasets are: ['train', 'val', 'A']\n"
     ]
    }
   ],
   "source": [
    "# Set debug mode for only a small fraction of the datasets. Speeds up this cell by alot\n",
    "cfg.debug = True\n",
    "\n",
    "# update config with default values\n",
    "OmegaConf.update(\n",
    "    cfg, \n",
    "    'eval',\n",
    "    OmegaConf.load('../configs/eval/unet_config.yaml')\n",
    ")\n",
    "\n",
    "# Wether and how you want to subset in case of Brain data. WARNING:\n",
    "# After subsetting the eval below will not work with surface\n",
    "# Dice anymore, because volumes are fragmented. \n",
    "APPLY_SUBSETTING = True\n",
    "OmegaConf.update(cfg, 'eval.data.subset.apply', APPLY_SUBSETTING)\n",
    "subset_params = {\n",
    "    'n_cases': 256,  # selects at most so many cases\n",
    "    'fraction': 0.1, # selects from the 10% worst cases w.r.t to a model\n",
    "}\n",
    "OmegaConf.update(\n",
    "    cfg, \n",
    "    'eval.data.subset.params', \n",
    "    OmegaConf.create(subset_params)\n",
    ")\n",
    "\n",
    "if cfg.eval.data.subset.apply:\n",
    "    subset_dict = OmegaConf.to_container(\n",
    "        cfg.eval.data.subset.params, \n",
    "        resolve=True, \n",
    "        throw_on_missing=True\n",
    "    )\n",
    "    subset_dict['unet'] = unet\n",
    "else:\n",
    "    subset_dict = None\n",
    "\n",
    "### select the datasets within the domain\n",
    "# get training data\n",
    "cfg.eval.data.training = True\n",
    "# get validation data\n",
    "cfg.eval.data.validation = True\n",
    "# get testing data\n",
    "# Options for Brain are any subset of [1, 2, 3, 4, 5] or 'all' \n",
    "# Options for Heart are any subset of ['A', 'B', 'C', 'D'] or 'all'\n",
    "cfg.eval.data.testing = ['A']\n",
    "\n",
    "\n",
    "raw_data = get_eval_data(\n",
    "    train_set=cfg.eval.data.training,\n",
    "    val_set=cfg.eval.data.validation,\n",
    "    test_sets=cfg.eval.data.testing,    \n",
    "    cfg=cfg,\n",
    "    subset_dict=subset_dict\n",
    ")\n",
    "\n",
    "print(f'\\nAvailable datasets are: {list(raw_data.keys())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old Mahalanobis Detector Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Callable, Tuple, List\n",
    "\n",
    "\n",
    "import torch #\n",
    "from torch.utils.data import DataLoader, default_collate\n",
    "from torch import Tensor, nn #\n",
    "from torchmetrics import (\n",
    "    SpearmanCorrCoef, \n",
    "    AUROC\n",
    ")\n",
    "from sklearn.covariance import LedoitWolf #\n",
    "\n",
    "from losses import DiceScoreCalgary, DiceScoreMMS #\n",
    "\n",
    "\n",
    "class PoolingMahalabonisDetector(nn.Module):\n",
    "    \"\"\"\n",
    "    Evaluation class for OOD and ESCE tasks based on https://arxiv.org/abs/2107.05975.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        model: nn.Module, \n",
    "        layer_ids: List[str], \n",
    "        train_loader: DataLoader, \n",
    "        valid_loader: DataLoader,\n",
    "        net_out: str,\n",
    "        criterion: nn.Module = DiceScoreCalgary(),\n",
    "        device: str = 'cuda:0'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device       = device\n",
    "        self.model        = model.to(device)\n",
    "        self.model.eval()\n",
    "        self.layer_ids    = layer_ids\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.net_out      = net_out\n",
    "        self.criterion    = criterion\n",
    "        self.pool         = nn.AvgPool3d(kernel_size=(2,2,2), stride=(2,2,2))\n",
    "        self.auroc        = AUROC(task = 'binary')\n",
    "        \n",
    "        # Init score dict for each layer:\n",
    "        self.latents   = {layer_id: [] for layer_id in self.layer_ids}\n",
    "        self.mu        = {layer_id: None for layer_id in self.layer_ids}\n",
    "        self.sigma_inv = {layer_id: None for layer_id in self.layer_ids}\n",
    "        self.dist      = {layer_id : 0 for layer_id in self.layer_ids}\n",
    "        \n",
    "        self._get_latents()\n",
    "        self._fit_gaussian_to_latents()\n",
    "        \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def _get_hook_fn(self, layer_id: str, mode: str = 'collect') -> Callable:\n",
    "        \n",
    "        def hook_fn(module: nn.Module, x: Tuple[Tensor]):\n",
    "            x = x[0]\n",
    "            while torch.prod(torch.tensor(x.shape[1:])) > 1e4:\n",
    "                x = self.pool(x)\n",
    "            x = self.pool(x)\n",
    "            batch_size = x.shape[0]\n",
    "\n",
    "            if mode == 'collect':\n",
    "                self.latents[layer_id].append(x.view(batch_size, -1).detach().cpu())\n",
    "            elif mode == 'single':\n",
    "                self.dist[layer_id] = x.view(batch_size, -1).to(self.device)\n",
    "                \n",
    "        return hook_fn\n",
    "    \n",
    "    \n",
    "    @torch.no_grad()        \n",
    "    def _get_latents(self) -> None:\n",
    "        handles = {}\n",
    "        for layer_id in self.layer_ids:\n",
    "            layer = self.model.get_submodule(layer_id)\n",
    "            hook  = self._get_hook_fn(layer_id, mode='collect')\n",
    "            handles[layer_id] = layer.register_forward_pre_hook(hook)\n",
    "\n",
    "        for batch in self.train_loader:\n",
    "            input_ = batch['input'].to(self.device)\n",
    "            _ = self.model(input_)\n",
    "                \n",
    "        for layer_id in handles:\n",
    "            self.latents[layer_id] = torch.cat(self.latents[layer_id], dim=0)\n",
    "            handles[layer_id].remove()\n",
    "        \n",
    "        \n",
    "    @torch.no_grad()         \n",
    "    def _fit_gaussian_to_latents(self) -> None:\n",
    "        for layer_id in self.layer_ids:\n",
    "            self.mu[layer_id] = self.latents[layer_id].mean(0, keepdims=True).to(self.device)\n",
    "            latents_centered = (self.latents[layer_id] - self.mu[layer_id].cpu()).detach().numpy()\n",
    "            sigma = torch.from_numpy(LedoitWolf().fit(latents_centered).covariance_)\n",
    "            self.sigma_inv[layer_id] = torch.linalg.inv(sigma).unsqueeze(0).to(self.device)\n",
    "            \n",
    "            \n",
    "    @torch.no_grad()\n",
    "    def testset_ood_detection(self, test_loader: DataLoader) -> Dict[str, torch.Tensor]:\n",
    "        \n",
    "        self.pred = {}\n",
    "        self.target = {}\n",
    "        \n",
    "        valid_dists = {layer_id : [] for layer_id in self.layer_ids}\n",
    "        for batch in self.valid_loader:\n",
    "            input_ = batch['input']\n",
    "            #print(input_.shape)\n",
    "            if self.net_out == 'calgary':\n",
    "                dist_volume = []\n",
    "                for input_chunk in input_:\n",
    "                    dist, _ = self.forward(input_chunk.unsqueeze(0).to(self.device))\n",
    "                    dist_volume.append(dist.copy())\n",
    "                dist = default_collate(dist_volume)\n",
    "            elif self.net_out == 'mms': \n",
    "                dist, _ = self.forward(input_.to(self.device))\n",
    "            for layer_id in self.layer_ids:\n",
    "                if self.net_out == 'calgary':\n",
    "                    valid_dists[layer_id].append(dist[layer_id].mean())\n",
    "                elif self.net_out == 'mms':\n",
    "                    valid_dists[layer_id].append(dist[layer_id])\n",
    "        self.valid_dists = valid_dists\n",
    "        self.valid_labels = {layer_id: torch.zeros(len(self.valid_dists[layer_id]), dtype=torch.uint8) \n",
    "                             for layer_id in self.layer_ids}\n",
    "        #print(len(self.valid_dists['up3']), len(self.valid_labels['up3']))\n",
    "            \n",
    "#             self.thresholds = {layer_id : 0 for layer_id in self.layer_ids}\n",
    "#             for layer_id in self.layer_ids:\n",
    "#                 if self.net_out == 'calgary':\n",
    "#                     valid_dists[layer_id] = torch.tensor(valid_dists[layer_id]).cpu()\n",
    "#                 elif self.net_out == 'mms':\n",
    "#                     valid_dists[layer_id] = torch.cat(valid_dists[layer_id], dim=0).cpu()\n",
    "#                 self.thresholds[layer_id] = torch.sort(valid_dists[layer_id])[0][len(valid_dists[layer_id]) - (len(valid_dists[layer_id]) // 20) - 1]\n",
    "                \n",
    "                    \n",
    "        test_dists = {layer_id : [] for layer_id in self.layer_ids}\n",
    "        for batch in test_loader:\n",
    "            input_ = batch['input']\n",
    "            if self.net_out == 'calgary':\n",
    "                dist_volume = []\n",
    "                for input_chunk in input_:\n",
    "                    dist, _ = self.forward(input_chunk.unsqueeze(0).to(self.device))\n",
    "                    dist_volume.append(dist.copy())\n",
    "                dist = default_collate(dist_volume)\n",
    "            elif self.net_out == 'mms': \n",
    "                dist, _ = self.forward(input_.to(self.device))\n",
    "            for layer_id in self.layer_ids:\n",
    "                if self.net_out == 'calgary':\n",
    "                    test_dists[layer_id].append(dist[layer_id].mean())\n",
    "                elif self.net_out == 'mms':    \n",
    "                    test_dists[layer_id].append(dist[layer_id])\n",
    "        \n",
    "        self.test_dists = test_dists\n",
    "        self.test_labels = {layer_id: torch.ones(len(self.test_dists[layer_id]), dtype=torch.uint8) \n",
    "                             for layer_id in self.layer_ids}\n",
    "            \n",
    "            \n",
    "        AUROC = {layer_id : 0 for layer_id in self.layer_ids}\n",
    "        for layer_id in self.layer_ids:\n",
    "            if self.net_out == 'calgary':\n",
    "                self.valid_dists[layer_id] = torch.tensor(self.valid_dists[layer_id]).cpu()\n",
    "                self.test_dists[layer_id]  = torch.tensor(self.test_dists[layer_id]).cpu()\n",
    "            elif self.net_out == 'mms':\n",
    "                self.valid_dists[layer_id] = torch.cat(self.valid_dists[layer_id], dim=0).cpu()\n",
    "                self.test_dists[layer_id]  = torch.cat(self.test_dists[layer_id], dim=0).cpu()\n",
    "            self.pred[layer_id]   = torch.cat([self.valid_dists[layer_id], self.test_dists[layer_id]]).squeeze()\n",
    "            self.target[layer_id] = torch.cat([self.valid_labels[layer_id], self.test_labels[layer_id]]).squeeze()\n",
    "            \n",
    "            print(self.pred[layer_id].shape, self.target[layer_id].shape)\n",
    "            \n",
    "            AUROC[layer_id] = self.auroc(self.pred[layer_id], self.target[layer_id])\n",
    "            #accuracy[layer_id] = ((test_dists[layer_id] > self.thresholds[layer_id]).sum() / len(test_dists[layer_id]))\n",
    "                \n",
    "        return AUROC\n",
    "    \n",
    "    \n",
    "    \n",
    "    @torch.no_grad()        \n",
    "    def testset_correlation(self, test_loader: DataLoader) -> Dict[str, torch.Tensor]:\n",
    "        corr_coeffs = {layer_id: SpearmanCorrCoef() for layer_id in self.layer_ids}\n",
    "        for batch in test_loader:\n",
    "            input_ = batch['input']\n",
    "            target = batch['target']\n",
    "            if self.net_out == 'calgary':\n",
    "                dist_volume = []\n",
    "                net_out_volume = []\n",
    "                for input_chunk in input_:\n",
    "                    dist, net_out = self.forward(input_chunk.unsqueeze(0).to(self.device))\n",
    "                    dist_volume.append(dist.copy())\n",
    "                    net_out_volume.append(net_out.cpu())\n",
    "                dist = default_collate(dist_volume)            \n",
    "                net_out = torch.cat(net_out_volume, dim=0)\n",
    "            \n",
    "            if self.net_out == 'mms':\n",
    "                target[target == -1] = 0\n",
    "                # convert to one-hot encoding\n",
    "                target = F.one_hot(target.long(), num_classes=4).squeeze(1).permute(0,3,1,2)\n",
    "                dist, net_out = self.forward(input_.to(self.device))            \n",
    "            loss = self.criterion(net_out.cpu(), target)\n",
    "\n",
    "            loss = loss.mean().float().cpu()\n",
    "            for layer_id in self.layer_ids:\n",
    "                corr_coeffs[layer_id].update(dist[layer_id].cpu().mean().view(1), 1-loss.view(1))\n",
    "\n",
    "        return corr_coeffs\n",
    "\n",
    "\n",
    "    @torch.no_grad()  \n",
    "    def forward(self, input_: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        handles = {}\n",
    "        for layer_id in self.layer_ids:\n",
    "            layer = self.model.get_submodule(layer_id)\n",
    "            hook  = self._get_hook_fn(layer_id, mode='single')\n",
    "            handles[layer_id] = layer.register_forward_pre_hook(hook)\n",
    "        \n",
    "        net_out = self.model(input_)\n",
    "        \n",
    "        for layer_id in self.layer_ids:\n",
    "            latent_centered = self.dist[layer_id].view(self.dist[layer_id].shape[0], 1, -1) - \\\n",
    "                self.mu[layer_id].unsqueeze(0)\n",
    "            self.dist[layer_id] = latent_centered @ self.sigma_inv[layer_id] @ \\\n",
    "                latent_centered.permute(0,2,1)\n",
    "            handles[layer_id].remove()\n",
    "            \n",
    "        return self.dist, net_out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactor Mahalanobis Detection  \n",
    "\n",
    "\n",
    "class Adapter:\n",
    "\n",
    "    attr:\n",
    "        swivel\n",
    "        hook_type: forward or backward\n",
    "    \n",
    "    methods:\n",
    "        \n",
    "\n",
    "\n",
    "class Transformation:\n",
    "\n",
    "\n",
    "    methods:\n",
    "        transform (forward ?)\n",
    "        score (mahalanobis or log likelihood https://stats.stackexchange.com/questions/97408/relation-of-mahalanobis-distance-to-log-likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoolingMahalanobis(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        swivel: str,\n",
    "        hook_fn: str = 'pre',\n",
    "        transform: bool = False,\n",
    "        device: str = 'cuda:0'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # args\n",
    "        self.swivel = swivel\n",
    "        self.hook_fn = self.register_forward_pre_hook if hook_fn == 'pre' else self.register_forward_hook\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "        # attributes\n",
    "        self.training_representations = []\n",
    "        self.pool = nn.AvgPool3d(kernel_size=(2,2,2), stride=(2,2,2))\n",
    "\n",
    "    ### private methods ###\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def _reduce(self, x: Tensor) -> Tensor:\n",
    "        # reduce dimensionality with 3D pooling to below 1e4 entries\n",
    "        while torch.prod(torch.tensor(x.shape[1:])) > 1e4:\n",
    "            x = self.pool(x)\n",
    "        # reshape to (batch_size, n_features)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        return x\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _collect(self, x: Tensor) -> None:\n",
    "        x = self._reduce(x)\n",
    "        assert x.device == 'cpu', 'move data to cpu before storing it.'\n",
    "        self.training_representations.append(x)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _merge(self) -> None:\n",
    "        self.training_representations = torch.cat(self.training_representations, dim=0)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _estimate_gaussians(self, ledoitWolf=False) -> None:\n",
    "        self.mu = self.training_representations.mean(0, keepdims=True).to(self.device)\n",
    "        if ledoitWolf:\n",
    "            self.sigma = torch.from_numpy(LedoitWolf().fit(self.training_representations).covariance_)\n",
    "        else:\n",
    "            self.sigma = torch.cov(self.training_representations)\n",
    "        self.sigma_inv = torch.linalg.inv(self.sigma).unsqueeze(0).to(self.device)\n",
    "\n",
    "\n",
    "    def _distance(self, x: Tensor) -> Tensor:\n",
    "        # TODO: implement Mahalanobis distance\n",
    "        # x = self._reduce(x)\n",
    "        # x = x - self.mu\n",
    "        # x = x @ self.sigma_inv @ x.T\n",
    "        return x\n",
    "\n",
    "    ### public methods ###\n",
    "\n",
    "    def fit(self):\n",
    "        self._merge()\n",
    "        self._estimate_gaussians()\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if self.training:\n",
    "            self._collect(x)\n",
    "        \n",
    "        else:\n",
    "            self.batch_distances = self._distance(x)\n",
    "\n",
    "        # implements identity function from a hooks perspective\n",
    "        if self.transform:\n",
    "            raise NotImplementedError('Implement transformation functionality')\n",
    "        else:\n",
    "            return x\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
