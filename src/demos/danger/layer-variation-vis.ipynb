{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0188a846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.nn.functional import softmax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "from random import randrange\n",
    "from torch import nn, Tensor\n",
    "from typing import Iterable, Dict, Callable, Tuple, Union, List\n",
    "from batchgenerators.transforms.local_transforms import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89cb2524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4e9fde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Please cite the following paper when using nnUNet:\n",
      "\n",
      "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n",
      "\n",
      "\n",
      "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nnunet.training.model_restore import restore_model\n",
    "import batchgenerators\n",
    "from batchgenerators.dataloading.multi_threaded_augmenter import MultiThreadedAugmenter\n",
    "from batchgenerators.utilities.file_and_folder_operations import *\n",
    "from nnunet.paths import preprocessing_output_dir\n",
    "from nnunet.training.dataloading.dataset_loading import *\n",
    "from nnunet.training.network_training.nnUNetTrainerV2 import nnUNetTrainerV2\n",
    "from nnunet.run.load_pretrained_weights import load_pretrained_weights\n",
    "\n",
    "import os, sys\n",
    "sys.path.append('../')\n",
    "from dataset import ACDCDataset, MNMDataset\n",
    "from model.dae import resDAE, AugResDAE\n",
    "from model.unet import UNet2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8a7e91",
   "metadata": {},
   "source": [
    "# General definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "041114b1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Frankenstein(nn.Module):\n",
    "    \"\"\"Wrapper class for segmentation models and feature transformations.\n",
    "\n",
    "    Wraps (a copy of) the segmentation model and attaches feature\n",
    "    trasformations to it via hooks (at potentially various positions\n",
    "    simultaneously). Additionally, it provides control utilities for the\n",
    "    hooks as well as different types for inference and training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        seg_model: nn.Module,\n",
    "        transformations: nn.ModuleDict,\n",
    "        disabled_ids: list = [],\n",
    "        copy: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.seg_model = deepcopy(seg_model) if copy else seg_model\n",
    "\n",
    "        self.transformations = transformations\n",
    "        self.disabled_ids = disabled_ids\n",
    "        self.transformation_handles = {}\n",
    "        self.train_transformation_handles = {}\n",
    "        self.inspect_transformation_handles = {}\n",
    "        self.training_data = {}\n",
    "        self.inspect_data = {}\n",
    "\n",
    "    def hook_train_transformations(self, transformations: Dict[str, nn.Module]) -> None:\n",
    "        for layer_id in transformations:\n",
    "            layer = self.seg_model.get_submodule(layer_id)\n",
    "            hook = self._get_train_transformation_hook(\n",
    "                transformations[layer_id], layer_id\n",
    "            )\n",
    "            self.train_transformation_handles[\n",
    "                layer_id\n",
    "            ] = layer.register_forward_pre_hook(hook)\n",
    "\n",
    "    def hook_transformations(\n",
    "        self, transformations: Dict[str, nn.Module], n_samples: int\n",
    "    ) -> None:\n",
    "        for layer_id in transformations:\n",
    "            layer = self.seg_model.get_submodule(layer_id)\n",
    "            hook = self._get_transformation_hook(transformations[layer_id], n_samples)\n",
    "            self.transformation_handles[layer_id] = layer.register_forward_pre_hook(\n",
    "                hook\n",
    "            )\n",
    "            \n",
    "    def hook_inspect_transformation(\n",
    "        self, \n",
    "        transformations: Dict[str, nn.Module], \n",
    "        n_samples: int,\n",
    "        arch: str = 'ae'\n",
    "    ) -> None:\n",
    "        for layer_id in transformations:\n",
    "            if layer_id not in self.disabled_ids:\n",
    "                layer = self.seg_model.get_submodule(layer_id)\n",
    "                hook  = self._get_inspect_transformation_hook(transformations[layer_id], layer_id, n_samples, arch)\n",
    "                self.inspect_transformation_handles[layer_id] = layer.register_forward_pre_hook(hook)\n",
    "            \n",
    "\n",
    "    def _get_train_transformation_hook(\n",
    "        self, transformation: nn.Module, layer_id: str\n",
    "    ) -> Callable:\n",
    "        def hook(module: nn.Module, x: Tuple[Tensor]) -> Tensor:\n",
    "            x_in, *_ = x  # tuple, alternatively use x_in = x[0]\n",
    "            x_orig = x_in[:1]\n",
    "            #x_views = x_in[1:]\n",
    "            x_in_denoised = transformation(x_in)\n",
    "            if layer_id not in self.disabled_ids:\n",
    "                mse = nn.functional.mse_loss(x_in_denoised, x_orig.detach(), reduction=\"mean\")\n",
    "\n",
    "                training_data = {\n",
    "                    \"mse\": mse,\n",
    "                }\n",
    "\n",
    "                self.training_data[layer_id] = training_data\n",
    "\n",
    "            return x_in_denoised\n",
    "\n",
    "        return hook\n",
    "    \n",
    "\n",
    "    def _get_transformation_hook(\n",
    "        self, transformation: nn.Module, n_samples: int = 1\n",
    "    ) -> Callable:\n",
    "        def hook(module: nn.Module, x: Tuple[Tensor]) -> Tensor:\n",
    "            x_in, *_ = x  # weird tuple, can use x_in = x[0]\n",
    "            if n_samples == 0:\n",
    "                return x\n",
    "            elif n_samples == -1:\n",
    "                x_in_new = transformation(x_in)\n",
    "                return x_in_new\n",
    "            else:\n",
    "                x_in_new = x_in.unsqueeze(1).repeat(1, n_samples, 1, 1, 1).flatten(0, 1)\n",
    "                x_in_new = transformation(x_in_new)\n",
    "                return torch.cat([x_in, x_in_new], dim=0)\n",
    "\n",
    "        return hook\n",
    "            \n",
    "        \n",
    "    def _get_inspect_transformation_hook(\n",
    "            self, \n",
    "            transformation: nn.Module, \n",
    "            layer_id: str, \n",
    "            n_samples: int,\n",
    "            arch: str = 'ae',\n",
    "        ) -> Callable:\n",
    "        \n",
    "        @torch.no_grad()\n",
    "        def hook(module: nn.Module, x: Tuple[Tensor]) -> Tensor:\n",
    "            x_in, *_ = x  # weird tuple, can use x_in = x[0]\n",
    "            if n_samples == 0:\n",
    "                return x\n",
    "            elif n_samples == -1:\n",
    "                mu, log_var, x_in_new = transformation(x_in)\n",
    "            else:\n",
    "                x_in_new = x_in.unsqueeze(1).repeat(1, n_samples, 1, 1, 1).flatten(0, 1)\n",
    "                if arch == 'ae':\n",
    "                    x_in_new = transformation(x_in_new)\n",
    "                elif arch == 'res_ae':\n",
    "                    x_in_new, prior, residual = transformation(x_in_new)\n",
    "                x_in_new = torch.cat([x_in, x_in_new], dim=0)\n",
    "                \n",
    "            if layer_id not in self.disabled_ids:\n",
    "                training_data = {\n",
    "                    'input'  : x_in_new[ :1],\n",
    "                    'recon'  : x_in_new[1: ],\n",
    "                }\n",
    "                \n",
    "                if arch == 'res_ae':\n",
    "                    training_data['prior'] = prior\n",
    "                    training_data['residual'] = residual\n",
    "                \n",
    "                self.inspect_data[layer_id] = training_data\n",
    "            \n",
    "            return x_in_new\n",
    "        \n",
    "        return hook\n",
    "   \n",
    "    \n",
    "\n",
    "    def remove_train_transformation_hook(self, layer_id: str) -> None:\n",
    "        self.train_transformation_handles[layer_id].remove()\n",
    "\n",
    "    def remove_transformation_hook(self, layer_id: str) -> None:\n",
    "        self.transformation_handles[layer_id].remove()\n",
    "        \n",
    "    def remove_inspect_transformation_hook(self, layer_id: str) -> None:\n",
    "        self.inspect_transformation_handles[layer_id].remove()\n",
    "\n",
    "    def remove_all_hooks(self):\n",
    "        if hasattr(self, \"train_transformation_handles\"):\n",
    "            for handle in self.train_transformation_handles:\n",
    "                self.train_transformation_handles[handle].remove()\n",
    "            self.train_transformation_handles = {}\n",
    "\n",
    "        if hasattr(self, \"transformation_handles\"):\n",
    "            for handle in self.transformation_handles:\n",
    "                self.transformation_handles[handle].remove()\n",
    "            self.transformation_handles = {}\n",
    "            \n",
    "        if hasattr(self, 'inspect_transformation_handles'):\n",
    "            for handle in self.inspect_transformation_handles:\n",
    "                self.inspect_transformation_handles[handle].remove()\n",
    "            self.inspect_transformation_handles = {}\n",
    "        \n",
    "\n",
    "    def freeze_seg_model(self):\n",
    "        self.seg_model.eval()\n",
    "        for param in self.seg_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def set_number_of_samples_to(self, n_samples: int):\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "    def disable(self, layer_ids: list) -> None:\n",
    "        for layer_id in layer_ids:\n",
    "            self.transformations[layer_id].turn_off()\n",
    "\n",
    "    def enable(self, layer_ids: list) -> None:\n",
    "        for layer_id in layer_ids:\n",
    "            self.transformations[layer_id].turn_on()\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return self.seg_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04d941cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# define single image dataloader from batchgenerator example here:\n",
    "# https://github.com/MIC-DKFZ/batchgenerators/blob/master/batchgenerators/examples/example_ipynb.ipynb\n",
    "class SingleImageMultiViewDataLoader(batchgenerators.dataloading.data_loader.SlimDataLoaderBase):\n",
    "    def __init__(self, data: ACDCDataset, batch_size: int = 2, return_orig: str = False):\n",
    "        super(SingleImageMultiViewDataLoader, self).__init__(data, batch_size)\n",
    "        # data is now stored in self._data.\n",
    "        self.return_orig = return_orig\n",
    "    \n",
    "    def generate_train_batch(self):\n",
    "        \n",
    "        data = self._data[randrange(len(self._data))]\n",
    "        img = data['input'].numpy().astype(np.float32)\n",
    "        tar = data['target'][0].numpy().astype(np.float32)\n",
    "        \n",
    "        img_batched = np.tile(img, (self.batch_size, 1, 1, 1))\n",
    "        tar_batched = np.tile(tar, (self.batch_size, 1, 1, 1))\n",
    "        # now construct the dictionary and return it. np.float32 cast because most networks take float\n",
    "        out = {'data': img_batched, \n",
    "               'seg':  tar_batched}\n",
    "        \n",
    "        # if the original data is also needed, activate this flag to store it where augmentations\n",
    "        # cant find it.\n",
    "        if self.return_orig:\n",
    "            out['data_orig']   = data['input'].unsqueeze(0)\n",
    "            out['target_orig'] = data['target'].unsqueeze(0)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "class LayerAugDiff(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        measure: str = 'std',\n",
    "        copy: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model   = deepcopy(model) if copy else model\n",
    "        self.measure = measure\n",
    "        # init container for data and hook handles\n",
    "        self.layer_ids = []\n",
    "        self.handles   = {}\n",
    "        self.diffs     = {}\n",
    "        # filter modules for layers that are particular interesting\n",
    "        # hint: modify get_layer_ids for alternative filter\n",
    "        self.get_layer_ids()\n",
    "        # init forward (after) hooks to collect data later. Hooks\n",
    "        # are manageable via self.handles\n",
    "        self.hook_hooks()\n",
    "        \n",
    "        \n",
    "    def get_layer_ids(self) -> None:\n",
    "        # Get all BN layers\n",
    "        for name, module in self.model.named_modules():\n",
    "            #if '.bn' in name:\n",
    "            #if 'down1.2' in name:\n",
    "            if name.count('.') == 1:\n",
    "                self.layer_ids.append(name)      \n",
    "                \n",
    "        \n",
    "    def hook_hooks(self) -> None:\n",
    "        for layer_id in self.layer_ids:\n",
    "            # get module for layer id to hook onto\n",
    "            layer = self.model.get_submodule(layer_id)\n",
    "            # build hook for this layer with function above\n",
    "            hook = self._get_hook(layer_id)\n",
    "            # hook hook and save handle to interact with it later\n",
    "            self.handles[layer_id] = layer.register_forward_hook(hook)\n",
    "            \n",
    "            \n",
    "    @torch.no_grad()\n",
    "    def _get_hook(self, layer_id: str) -> Callable:\n",
    "        def hook(module: nn.Module, x_in: Tuple[Tensor], x_out: Tuple[Tensor]) -> Tensor:\n",
    "            #print(x_out.shape)\n",
    "            #x, *_ = x_out  # tuple, alternatively use x_in = x[0]\n",
    "            #print(x_in[0].shape, x_out.shape)\n",
    "            \n",
    "            # calculates mean std across inputs with different augmentations\n",
    "            if self.measure == 'std':\n",
    "                diff = x_out.std(0).mean().item()\n",
    "            # calculate activation normalized MSE\n",
    "            elif self.measure == 'mse':\n",
    "                mse = torch.pow(\n",
    "                    x_out[:1] - x_out[1:],\n",
    "                    2\n",
    "                ).mean()\n",
    "                diff = mse / torch.abs(x_out[:1]).mean()\n",
    "                diff = torch.clamp(diff, 0, 2)\n",
    "            # appends diff to list for each layer. Inits list if doesn't exist\n",
    "            self.diffs.setdefault(layer_id, []).append(diff.detach().cpu().item())\n",
    "            return x_out\n",
    "        return hook\n",
    "    \n",
    "    \n",
    "    def forward(self, x: Tensor):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cab8ed4",
   "metadata": {},
   "source": [
    "# Plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bab7f8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_layer_diff(\n",
    "    scanner: str, \n",
    "    augmentations: str, \n",
    "    scaled: bool, \n",
    "    n_views: int = 32, \n",
    "    n_imgs: int = 250, \n",
    "    unet_id: str = 0\n",
    "):\n",
    "    \n",
    "#     \n",
    "#     augmentations = 'output_invariant'  \n",
    "#     scaled = False\n",
    "#     n_views = 4\n",
    "#     n_imgs = 100\n",
    "#     unet_id = 0\n",
    "\n",
    "    ### - build augmentator\n",
    "    # where data is saved\n",
    "    nnUnet_prefix = '../../../nnUNet/'\n",
    "    # Initialize trainer to get data loaders with data augmentations from training\n",
    "    pkl_file          = nnUnet_prefix + 'data/nnUNet_preprocessed/Task500_ACDC/nnUNetPlansv2.1_plans_2D.pkl'\n",
    "    fold              = 0\n",
    "    output_folder     = nnUnet_prefix + 'results/nnUnet/nnUNet/2d/Task027_ACDC/nnUNetTrainerV2__nnUNetPlansv2.1/'\n",
    "    dataset_directory = nnUnet_prefix + 'data/nnUNet_preprocessed/Task500_ACDC'\n",
    "    trainer = nnUNetTrainerV2(pkl_file, 0, output_folder, dataset_directory)\n",
    "    trainer.initialize()\n",
    "    train_loader = trainer.tr_gen\n",
    "    # filter augmentations w.r.t. given keyword and init augmentor\n",
    "    if augmentations == 'all':\n",
    "        transforms = [t for t in train_loader.transform.transforms]\n",
    "    elif augmentations == 'output_invariant':\n",
    "        data_only_transforms = (\n",
    "            batchgenerators.transforms.resample_transforms.SimulateLowResolutionTransform,\n",
    "            batchgenerators.transforms.noise_transforms.GaussianNoiseTransform,\n",
    "            batchgenerators.transforms.noise_transforms.GaussianBlurTransform,\n",
    "            batchgenerators.transforms.color_transforms.BrightnessMultiplicativeTransform,\n",
    "            batchgenerators.transforms.color_transforms.ContrastAugmentationTransform,\n",
    "            batchgenerators.transforms.color_transforms.GammaTransform,\n",
    "            batchgenerators.transforms.utility_transforms.RemoveLabelTransform,\n",
    "            batchgenerators.transforms.utility_transforms.RenameTransform,\n",
    "            batchgenerators.transforms.utility_transforms.NumpyToTensor)\n",
    "\n",
    "        transforms = [t for t in train_loader.transform.transforms if isinstance(t, data_only_transforms)]\n",
    "    augmentor  = batchgenerators.transforms.abstract_transforms.Compose(transforms)\n",
    "\n",
    "    ### - Load dataset and init batch generator\n",
    "    if scanner in ['train', 'val']:\n",
    "        data = ACDCDataset(data=scanner, debug=False)\n",
    "    elif scanner in ['A', 'B', 'C', 'D']:\n",
    "        data = MNMDataset(vendor=scanner, debug=False)\n",
    "    gen = SingleImageMultiViewDataLoader(data, batch_size=n_views, return_orig=True)\n",
    "    multithreaded_gen = MultiThreadedAugmenter(gen, augmentor, 1, 1, seeds=None)\n",
    "\n",
    "    ### - Init Model and hook feature extractors\n",
    "    root = '../../'\n",
    "    middle = 'unet8_'\n",
    "    pre = 'acdc'\n",
    "    name = f'{pre}_{middle}{unet_id}'\n",
    "\n",
    "    model_path = f'{root}pre-trained-tmp/trained_UNets/{name}_best.pt'\n",
    "    state_dict = torch.load(model_path)['model_state_dict']\n",
    "    n_chans_out = 4\n",
    "    unet = UNet2D(n_chans_in=1, \n",
    "                  n_chans_out=n_chans_out, \n",
    "                  n_filters_init=8, \n",
    "                  dropout=False)\n",
    "    unet.load_state_dict(state_dict)\n",
    "    model = LayerAugDiff(unet, measure='mse', copy=True)\n",
    "    model.eval()\n",
    "    model.to(0)\n",
    "\n",
    "    ### - gather diffs across all inputs with their respective views\n",
    "    for i in range(n_imgs):\n",
    "        batch = multithreaded_gen.next()\n",
    "        input_ = torch.cat([batch['data_orig'], batch['data']], dim=0)\n",
    "        input_mse = torch.pow(\n",
    "                        input_[:1] - input_[1:],\n",
    "                        2\n",
    "                    ).mean()\n",
    "        input_mse /=  torch.abs(input_[:1]).mean()\n",
    "        model.diffs.setdefault('Augmented Input', []).append(input_mse.item())\n",
    "        output = softmax(model(input_.cuda()), dim=1).detach().cpu()\n",
    "        output_mse = torch.pow(\n",
    "                        output[:1] - output[1:],\n",
    "                        2\n",
    "                    ).mean()\n",
    "        output_mse = output_mse / torch.abs(output[:1]).mean()\n",
    "        model.diffs.setdefault('Softmax Output', []).append(output_mse.item())\n",
    "\n",
    "    ### - build dataframe for plotting\n",
    "    df_diffs = pd.DataFrame(columns=['layer', 'group', 'diff'])\n",
    "    for i, key in enumerate(model.diffs):\n",
    "        entries  = [[key, key.partition('.')[0], diff] for diff in model.diffs[key]]\n",
    "        df_diffs = pd.concat([df_diffs, pd.DataFrame(entries, columns=['layer', 'group', 'diff'])])\n",
    "\n",
    "    ### - build plot\n",
    "    sns.set(rc={'figure.figsize':(30,15)})\n",
    "    g = sns.boxplot(\n",
    "        data=df_diffs, \n",
    "        x='layer', \n",
    "        y='diff', \n",
    "    )\n",
    "    \n",
    "    if scaled:\n",
    "        g.set(ylim=(-0.1, 1.3))\n",
    "    sns.despine(offset=10, trim=True)\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    if augmentations == 'all':\n",
    "        aug_str = 'all'\n",
    "    elif augmentations == 'output_invariant':\n",
    "        aug_str = 'output-invariant'\n",
    "\n",
    "    title = f\"Normalized MSE to Original per Layer for identical Inputs with {aug_str} Augmentations - Data {scanner.upper()}\"\n",
    "    plt.title(title, fontsize = 30)\n",
    "    plt.ylabel(\"Avrg standard deviation over Pixel and Channel\", fontsize=20)\n",
    "    plt.xlabel(\"Layer\", fontsize=20)\n",
    "    save_path = f\"../../results-tmp/results/eval/variations-across-layers_MSE_{aug_str}_{scanner}_{'scaled' if scaled else 'unscaled'}_{unet_id}_filtertest.jpg\"\n",
    "    print(save_path)\n",
    "    plt.savefig(save_path, bbox_inches='tight', dpi=400)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541653ae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plot_layer_diff(scanner='train', augmentations='output_invariant', scaled=False, unet_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a091804d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_layer_diff(\n",
    "    scanner: str, \n",
    "    augmentations: str, \n",
    "    scaled: bool, \n",
    "    n_views: int = 32, \n",
    "    n_imgs: int = 250, \n",
    "    unet_id: str = 0\n",
    "):\n",
    "    \n",
    "#     \n",
    "#     augmentations = 'output_invariant'  \n",
    "#     scaled = False\n",
    "#     n_views = 4\n",
    "#     n_imgs = 100\n",
    "#     unet_id = 0\n",
    "    scanner = 'train'\n",
    "    ### - build augmentator\n",
    "    # where data is saved\n",
    "    nnUnet_prefix = '../../../nnUNet/'\n",
    "    # Initialize trainer to get data loaders with data augmentations from training\n",
    "    pkl_file          = nnUnet_prefix + 'data/nnUNet_preprocessed/Task500_ACDC/nnUNetPlansv2.1_plans_2D.pkl'\n",
    "    fold              = 0\n",
    "    output_folder     = nnUnet_prefix + 'results/nnUnet/nnUNet/2d/Task027_ACDC/nnUNetTrainerV2__nnUNetPlansv2.1/'\n",
    "    dataset_directory = nnUnet_prefix + 'data/nnUNet_preprocessed/Task500_ACDC'\n",
    "    trainer = nnUNetTrainerV2(pkl_file, 0, output_folder, dataset_directory)\n",
    "    trainer.initialize()\n",
    "    train_loader = trainer.tr_gen\n",
    "    # filter augmentations w.r.t. given keyword and init augmentor\n",
    "    if augmentations == 'all':\n",
    "        transforms = [t for t in train_loader.transform.transforms]\n",
    "    elif augmentations == 'output_invariant':\n",
    "        data_only_transforms = (\n",
    "            batchgenerators.transforms.noise_transforms.GaussianNoiseTransform,\n",
    "            batchgenerators.transforms.noise_transforms.GaussianBlurTransform,\n",
    "            batchgenerators.transforms.color_transforms.BrightnessMultiplicativeTransform,\n",
    "            batchgenerators.transforms.color_transforms.ContrastAugmentationTransform,\n",
    "            batchgenerators.transforms.color_transforms.GammaTransform,\n",
    "            batchgenerators.transforms.utility_transforms.RemoveLabelTransform,\n",
    "            batchgenerators.transforms.utility_transforms.RenameTransform,\n",
    "            batchgenerators.transforms.utility_transforms.NumpyToTensor)\n",
    "\n",
    "        transforms = [t for t in train_loader.transform.transforms if isinstance(t, data_only_transforms)]\n",
    "    augmentor  = batchgenerators.transforms.abstract_transforms.Compose(transforms)\n",
    "\n",
    "    ### - Load dataset and init batch generator\n",
    "    if scanner in ['train', 'val']:\n",
    "        data = ACDCDataset(data=scanner, debug=False)\n",
    "    elif scanner in ['A', 'B', 'C', 'D']:\n",
    "        data = MNMDataset(vendor=scanner, debug=False)\n",
    "    gen = SingleImageMultiViewDataLoader(data, batch_size=n_views, return_orig=True)\n",
    "    multithreaded_gen = MultiThreadedAugmenter(gen, augmentor, 1, 1, seeds=None)\n",
    "\n",
    "    ### - Init Model and hook feature extractors\n",
    "    root = '../../'\n",
    "    middle = 'unet8_'\n",
    "    pre = 'acdc'\n",
    "    name = f'{pre}_{middle}{unet_id}'\n",
    "\n",
    "    model_path = f'{root}pre-trained-tmp/trained_UNets/{name}_best.pt'\n",
    "    state_dict = torch.load(model_path)['model_state_dict']\n",
    "    n_chans_out = 4\n",
    "    unet = UNet2D(n_chans_in=1, \n",
    "                  n_chans_out=n_chans_out, \n",
    "                  n_filters_init=8, \n",
    "                  dropout=False)\n",
    "    # unet.load_state_dict(state_dict)\n",
    "    model = LayerAugDiff(unet, measure='mse', copy=True)\n",
    "    model.eval()\n",
    "    model.to(0)\n",
    "\n",
    "    ### - gather diffs across all inputs with their respective views\n",
    "    for i in range(n_imgs):\n",
    "        batch = multithreaded_gen.next()\n",
    "        input_ = torch.cat([batch['data_orig'], batch['data']], dim=0)\n",
    "        input_mse = torch.pow(\n",
    "                        input_[:1] - input_[1:],\n",
    "                        2\n",
    "                    ).mean()\n",
    "        input_mse /=  torch.abs(input_[:1]).mean()\n",
    "        model.diffs.setdefault('Augmented Input', []).append(input_mse.item())\n",
    "        output = softmax(model(input_.cuda()), dim=1).detach().cpu()\n",
    "        output_mse = torch.pow(\n",
    "                        output[:1] - output[1:],\n",
    "                        2\n",
    "                    ).mean()\n",
    "        output_mse = output_mse / torch.abs(output[:1]).mean()\n",
    "        model.diffs.setdefault('Softmax Output', []).append(output_mse.item())\n",
    "\n",
    "    ### - build dataframe for plotting\n",
    "    df_diffs = pd.DataFrame(columns=['layer', 'group', 'diff'])\n",
    "    for i, key in enumerate(model.diffs):\n",
    "        entries  = [[key, key.partition('.')[0], diff] for diff in model.diffs[key]]\n",
    "        df_diffs = pd.concat([df_diffs, pd.DataFrame(entries, columns=['layer', 'group', 'diff'])])\n",
    "\n",
    "    ### - build plot\n",
    "    sns.set(rc={'figure.figsize':(30,15)})\n",
    "    g = sns.boxplot(\n",
    "        data=df_diffs, \n",
    "        x='layer', \n",
    "        y='diff', \n",
    "        showfliers=False, \n",
    "        showcaps=False, \n",
    "        showbox=False,\n",
    "        whis=0,\n",
    "        medianprops=dict(color='blue')\n",
    "    )\n",
    "    \n",
    "    if scaled:\n",
    "        g.set(ylim=(-0.1, 1.3))\n",
    "    sns.despine(offset=10, trim=True)\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    if augmentations == 'all':\n",
    "        aug_str = 'all'\n",
    "    elif augmentations == 'output_invariant':\n",
    "        aug_str = 'output-invariant'\n",
    "\n",
    "    title = f\"Normalized MSE to Original per Layer for identical Inputs with {aug_str} Augmentations - Data {scanner.upper()}\"\n",
    "    plt.title(title, fontsize = 30)\n",
    "    plt.ylabel(\"Avrg standard deviation over Pixel and Channel\", fontsize=20)\n",
    "    plt.xlabel(\"Layer\", fontsize=20)\n",
    "    save_path = f\"../../results-tmp/results/eval/variations-across-layers_MSE_{aug_str}_{scanner}_{'scaled' if scaled else 'unscaled'}_{unet_id}.jpg\"\n",
    "    print(save_path)\n",
    "    #plt.savefig(save_path, bbox_inches='tight', dpi=400)\n",
    "    #plt.show()\n",
    "\n",
    "    scanner = 'A'\n",
    "    ### - build augmentator\n",
    "    # where data is saved\n",
    "    nnUnet_prefix = '../../../nnUNet/'\n",
    "    # Initialize trainer to get data loaders with data augmentations from training\n",
    "    pkl_file          = nnUnet_prefix + 'data/nnUNet_preprocessed/Task500_ACDC/nnUNetPlansv2.1_plans_2D.pkl'\n",
    "    fold              = 0\n",
    "    output_folder     = nnUnet_prefix + 'results/nnUnet/nnUNet/2d/Task027_ACDC/nnUNetTrainerV2__nnUNetPlansv2.1/'\n",
    "    dataset_directory = nnUnet_prefix + 'data/nnUNet_preprocessed/Task500_ACDC'\n",
    "    trainer = nnUNetTrainerV2(pkl_file, 0, output_folder, dataset_directory)\n",
    "    trainer.initialize()\n",
    "    train_loader = trainer.tr_gen\n",
    "    # filter augmentations w.r.t. given keyword and init augmentor\n",
    "    if augmentations == 'all':\n",
    "        transforms = [t for t in train_loader.transform.transforms]\n",
    "    elif augmentations == 'output_invariant':\n",
    "        data_only_transforms = (\n",
    "            batchgenerators.transforms.noise_transforms.GaussianNoiseTransform,\n",
    "            batchgenerators.transforms.noise_transforms.GaussianBlurTransform,\n",
    "            batchgenerators.transforms.color_transforms.BrightnessMultiplicativeTransform,\n",
    "            batchgenerators.transforms.color_transforms.ContrastAugmentationTransform,\n",
    "            batchgenerators.transforms.color_transforms.GammaTransform,\n",
    "            batchgenerators.transforms.utility_transforms.RemoveLabelTransform,\n",
    "            batchgenerators.transforms.utility_transforms.RenameTransform,\n",
    "            batchgenerators.transforms.utility_transforms.NumpyToTensor)\n",
    "\n",
    "        transforms = [t for t in train_loader.transform.transforms if isinstance(t, data_only_transforms)]\n",
    "    augmentor  = batchgenerators.transforms.abstract_transforms.Compose(transforms)\n",
    "\n",
    "    ### - Load dataset and init batch generator\n",
    "    if scanner in ['train', 'val']:\n",
    "        data = ACDCDataset(data=scanner, debug=False)\n",
    "    elif scanner in ['A', 'B', 'C', 'D']:\n",
    "        data = MNMDataset(vendor=scanner, debug=False)\n",
    "    gen = SingleImageMultiViewDataLoader(data, batch_size=n_views, return_orig=True)\n",
    "    multithreaded_gen = MultiThreadedAugmenter(gen, augmentor, 1, 1, seeds=None)\n",
    "\n",
    "    ### - Init Model and hook feature extractors\n",
    "    root = '../../'\n",
    "    middle = 'unet8_'\n",
    "    pre = 'acdc'\n",
    "    name = f'{pre}_{middle}{unet_id}'\n",
    "\n",
    "    model_path = f'{root}pre-trained-tmp/trained_UNets/{name}_best.pt'\n",
    "    state_dict = torch.load(model_path)['model_state_dict']\n",
    "    n_chans_out = 4\n",
    "    unet = UNet2D(n_chans_in=1, \n",
    "                  n_chans_out=n_chans_out, \n",
    "                  n_filters_init=8, \n",
    "                  dropout=False)\n",
    "    #unet.load_state_dict(state_dict)\n",
    "    model = LayerAugDiff(unet, measure='mse', copy=True)\n",
    "    model.eval()\n",
    "    model.to(0)\n",
    "\n",
    "    ### - gather diffs across all inputs with their respective views\n",
    "    for i in range(n_imgs):\n",
    "        batch = multithreaded_gen.next()\n",
    "        input_ = torch.cat([batch['data_orig'], batch['data']], dim=0)\n",
    "        input_mse = torch.pow(\n",
    "                        input_[:1] - input_[1:],\n",
    "                        2\n",
    "                    ).mean()\n",
    "        input_mse /=  torch.abs(input_[:1]).mean()\n",
    "        model.diffs.setdefault('Augmented Input', []).append(input_mse.item())\n",
    "        output = softmax(model(input_.cuda()), dim=1).detach().cpu()\n",
    "        output_mse = torch.pow(\n",
    "                        output[:1] - output[1:],\n",
    "                        2\n",
    "                    ).mean()\n",
    "        output_mse = output_mse / torch.abs(output[:1]).mean()\n",
    "        model.diffs.setdefault('Softmax Output', []).append(output_mse.item())\n",
    "\n",
    "    ### - build dataframe for plotting\n",
    "    df_diffs = pd.DataFrame(columns=['layer', 'group', 'diff'])\n",
    "    for i, key in enumerate(model.diffs):\n",
    "        entries  = [[key, key.partition('.')[0], diff] for diff in model.diffs[key]]\n",
    "        df_diffs = pd.concat([df_diffs, pd.DataFrame(entries, columns=['layer', 'group', 'diff'])])\n",
    "\n",
    "    ### - build plot\n",
    "    sns.set(rc={'figure.figsize':(30,15)})\n",
    "    g = sns.boxplot(\n",
    "        data=df_diffs, \n",
    "        x='layer', \n",
    "        y='diff', \n",
    "        showfliers=False, \n",
    "        showcaps=False, \n",
    "        showbox=False,\n",
    "        whis=0,\n",
    "        medianprops=dict(color='red')\n",
    "    )\n",
    "    \n",
    "    if scaled:\n",
    "        g.set(ylim=(-0.1, 1.3))\n",
    "    sns.despine(offset=10, trim=True)\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    if augmentations == 'all':\n",
    "        aug_str = 'all'\n",
    "    elif augmentations == 'output_invariant':\n",
    "        aug_str = 'output-invariant'\n",
    "\n",
    "    title = f\"Normalized MSE to Original per Layer for identical Inputs with {aug_str} Augmentations - Comparison Train vs Test\"\n",
    "    plt.title(title, fontsize = 30)\n",
    "    plt.ylabel(\"Avrg standard deviation over Pixel and Channel\", fontsize=20)\n",
    "    plt.xlabel(\"Layer\", fontsize=20)\n",
    "    save_path = f\"../../results-tmp/results/eval/variations-across-layers_MSE_{aug_str}_{scanner}_{'scaled' if scaled else 'unscaled'}_{unet_id}_untrained.jpg\"\n",
    "    print(save_path)\n",
    "    plt.savefig(save_path, bbox_inches='tight', dpi=400)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9872bb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset\n",
      "loading all case properties\n",
      "2023-08-22 15:40:14.053583: Using splits from existing split file: ../../../nnUNet/data/nnUNet_preprocessed/Task500_ACDC/splits_final.pkl\n",
      "2023-08-22 15:40:14.054868: The split file contains 5 splits.\n",
      "2023-08-22 15:40:14.054999: Desired fold for training: 0\n",
      "2023-08-22 15:40:14.055536: This split has 160 training and 40 validation cases.\n",
      "unpacking dataset\n",
      "done\n",
      "loading dataset\n",
      "loading all case properties\n",
      "../../results-tmp/results/eval/variations-across-layers_MSE_output-invariant_train_unscaled_0.jpg\n",
      "loading dataset\n",
      "loading all case properties\n",
      "2023-08-22 15:40:53.650719: Using splits from existing split file: ../../../nnUNet/data/nnUNet_preprocessed/Task500_ACDC/splits_final.pkl\n",
      "2023-08-22 15:40:53.651926: The split file contains 5 splits.\n",
      "2023-08-22 15:40:53.652286: Desired fold for training: 0\n",
      "2023-08-22 15:40:53.652864: This split has 160 training and 40 validation cases.\n",
      "unpacking dataset\n",
      "done\n",
      "loading dataset\n",
      "loading all case properties\n",
      "../../results-tmp/results/eval/variations-across-layers_MSE_output-invariant_A_unscaled_0_untrained.jpg\n"
     ]
    }
   ],
   "source": [
    "plot_layer_diff(scanner='A', augmentations='output_invariant', scaled=False, unet_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4276cf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_layer_diff(\n",
    "    scanner: str, \n",
    "    augmentations: str, \n",
    "    scaled: bool, \n",
    "    n_views: int = 32, \n",
    "    n_imgs: int = 100, \n",
    "    unet_id: str = 0\n",
    "):\n",
    "    \n",
    "#     \n",
    "#     augmentations = 'output_invariant'  \n",
    "#     scaled = False\n",
    "#     n_views = 4\n",
    "#     n_imgs = 100\n",
    "#     unet_id = 0\n",
    "\n",
    "\n",
    "\n",
    "    scanner = 'train'\n",
    "    ### - build augmentator\n",
    "    # where data is saved\n",
    "    nnUnet_prefix = '../../../nnUNet/'\n",
    "    # Initialize trainer to get data loaders with data augmentations from training\n",
    "    pkl_file          = nnUnet_prefix + 'data/nnUNet_preprocessed/Task500_ACDC/nnUNetPlansv2.1_plans_2D.pkl'\n",
    "    fold              = 0\n",
    "    output_folder     = nnUnet_prefix + 'results/nnUnet/nnUNet/2d/Task027_ACDC/nnUNetTrainerV2__nnUNetPlansv2.1/'\n",
    "    dataset_directory = nnUnet_prefix + 'data/nnUNet_preprocessed/Task500_ACDC'\n",
    "    trainer = nnUNetTrainerV2(pkl_file, 0, output_folder, dataset_directory)\n",
    "    trainer.initialize()\n",
    "    train_loader = trainer.tr_gen\n",
    "    # filter augmentations w.r.t. given keyword and init augmentor\n",
    "    if augmentations == 'all':\n",
    "        transforms = [t for t in train_loader.transform.transforms]\n",
    "    elif augmentations == 'output_invariant':\n",
    "        data_only_transforms = (\n",
    "            batchgenerators.transforms.noise_transforms.GaussianNoiseTransform,\n",
    "            batchgenerators.transforms.noise_transforms.GaussianBlurTransform,\n",
    "            batchgenerators.transforms.color_transforms.BrightnessMultiplicativeTransform,\n",
    "            batchgenerators.transforms.color_transforms.ContrastAugmentationTransform,\n",
    "            batchgenerators.transforms.color_transforms.GammaTransform,\n",
    "            batchgenerators.transforms.utility_transforms.RemoveLabelTransform,\n",
    "            batchgenerators.transforms.utility_transforms.RenameTransform,\n",
    "            batchgenerators.transforms.utility_transforms.NumpyToTensor)\n",
    "\n",
    "        transforms = [t for t in train_loader.transform.transforms if isinstance(t, data_only_transforms)]\n",
    "    augmentor  = batchgenerators.transforms.abstract_transforms.Compose(transforms)\n",
    "\n",
    "    ### - Load dataset and init batch generator\n",
    "    if scanner in ['train', 'val']:\n",
    "        data = ACDCDataset(data=scanner, debug=False)\n",
    "    elif scanner in ['A', 'B', 'C', 'D']:\n",
    "        data = MNMDataset(vendor=scanner, debug=False)\n",
    "    gen = SingleImageMultiViewDataLoader(data, batch_size=n_views, return_orig=True)\n",
    "    multithreaded_gen = MultiThreadedAugmenter(gen, augmentor, 1, 1, seeds=None)\n",
    "\n",
    "    ### - Init Model and hook feature extractors\n",
    "    root = '../../'\n",
    "    middle = 'unet8_'\n",
    "    pre = 'acdc'\n",
    "    name = f'{pre}_{middle}{unet_id}'\n",
    "\n",
    "    model_path = f'{root}pre-trained-tmp/trained_UNets/{name}_best.pt'\n",
    "    state_dict = torch.load(model_path)['model_state_dict']\n",
    "    n_chans_out = 4\n",
    "    unet = UNet2D(n_chans_in=1, \n",
    "                  n_chans_out=n_chans_out, \n",
    "                  n_filters_init=8, \n",
    "                  dropout=False)\n",
    "    #unet.load_state_dict(state_dict)\n",
    "    model = LayerAugDiff(unet, measure='mse', copy=True)\n",
    "    model.eval()\n",
    "    model.to(0)\n",
    "\n",
    "    ### - gather diffs across all inputs with their respective views\n",
    "    for i in range(n_imgs):\n",
    "        batch = multithreaded_gen.next()\n",
    "        input_ = torch.cat([batch['data_orig'], batch['data']], dim=0)\n",
    "        input_mse = torch.pow(\n",
    "                        input_[:1] - input_[1:],\n",
    "                        2\n",
    "                    ).mean()\n",
    "        input_mse /=  torch.abs(input_[:1]).mean()\n",
    "        model.diffs.setdefault('Augmented Input', []).append(input_mse.item())\n",
    "        output = softmax(model(input_.cuda()), dim=1).detach().cpu()\n",
    "        output_mse = torch.pow(\n",
    "                        output[:1] - output[1:],\n",
    "                        2\n",
    "                    ).mean()\n",
    "        output_mse = output_mse / torch.abs(output[:1]).mean()\n",
    "        model.diffs.setdefault('Softmax Output', []).append(output_mse.item())\n",
    "\n",
    "    ### - build dataframe for plotting\n",
    "    df_diffs = pd.DataFrame(columns=['layer', 'group', 'diff'])\n",
    "    for i, key in enumerate(model.diffs):\n",
    "        entries  = [[key, key.partition('.')[0], diff] for diff in model.diffs[key]]\n",
    "        df_diffs = pd.concat([df_diffs, pd.DataFrame(entries, columns=['layer', 'group', 'diff'])])\n",
    "\n",
    "    ### - build plot\n",
    "    sns.set(rc={'figure.figsize':(30,15)})\n",
    "    g = sns.boxplot(\n",
    "        data=df_diffs, \n",
    "        x='layer', \n",
    "        y='diff', \n",
    "        showfliers=False, \n",
    "        showcaps=False, \n",
    "        showbox=False,\n",
    "        whis=0,\n",
    "        medianprops=dict(color='blue')\n",
    "    )\n",
    "    \n",
    "    if scaled:\n",
    "        g.set(ylim=(-0.1, 1.3))\n",
    "    sns.despine(offset=10, trim=True)\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    if augmentations == 'all':\n",
    "        aug_str = 'all'\n",
    "    elif augmentations == 'output_invariant':\n",
    "        aug_str = 'output-invariant'\n",
    "\n",
    "    title = f\"Normalized MSE to Original per Layer for identical Inputs with {aug_str} Augmentations - Data {scanner.upper()}\"\n",
    "    plt.title(title, fontsize = 30)\n",
    "    plt.ylabel(\"Avrg standard deviation over Pixel and Channel\", fontsize=20)\n",
    "    plt.xlabel(\"Layer\", fontsize=20)\n",
    "    save_path = f\"../../results-tmp/results/eval/variations-across-layers_MSE_{aug_str}_{scanner}_{'scaled' if scaled else 'unscaled'}_{unet_id}.jpg\"\n",
    "    print(save_path)\n",
    "    #plt.savefig(save_path, bbox_inches='tight', dpi=400)\n",
    "    #plt.show()\n",
    "\n",
    "    scanner = 'C'\n",
    "    ### - build augmentator\n",
    "    # where data is saved\n",
    "    nnUnet_prefix = '../../../nnUNet/'\n",
    "    # Initialize trainer to get data loaders with data augmentations from training\n",
    "    pkl_file          = nnUnet_prefix + 'data/nnUNet_preprocessed/Task500_ACDC/nnUNetPlansv2.1_plans_2D.pkl'\n",
    "    fold              = 0\n",
    "    output_folder     = nnUnet_prefix + 'results/nnUnet/nnUNet/2d/Task027_ACDC/nnUNetTrainerV2__nnUNetPlansv2.1/'\n",
    "    dataset_directory = nnUnet_prefix + 'data/nnUNet_preprocessed/Task500_ACDC'\n",
    "    trainer = nnUNetTrainerV2(pkl_file, 0, output_folder, dataset_directory)\n",
    "    trainer.initialize()\n",
    "    train_loader = trainer.tr_gen\n",
    "    # filter augmentations w.r.t. given keyword and init augmentor\n",
    "    if augmentations == 'all':\n",
    "        transforms = [t for t in train_loader.transform.transforms]\n",
    "    elif augmentations == 'output_invariant':\n",
    "        data_only_transforms = (\n",
    "            batchgenerators.transforms.noise_transforms.GaussianNoiseTransform,\n",
    "            batchgenerators.transforms.noise_transforms.GaussianBlurTransform,\n",
    "            batchgenerators.transforms.color_transforms.BrightnessMultiplicativeTransform,\n",
    "            batchgenerators.transforms.color_transforms.ContrastAugmentationTransform,\n",
    "            batchgenerators.transforms.color_transforms.GammaTransform,\n",
    "            batchgenerators.transforms.utility_transforms.RemoveLabelTransform,\n",
    "            batchgenerators.transforms.utility_transforms.RenameTransform,\n",
    "            batchgenerators.transforms.utility_transforms.NumpyToTensor)\n",
    "\n",
    "        transforms = [t for t in train_loader.transform.transforms if isinstance(t, data_only_transforms)]\n",
    "    augmentor  = batchgenerators.transforms.abstract_transforms.Compose(transforms)\n",
    "\n",
    "    ### - Load dataset and init batch generator\n",
    "    if scanner in ['train', 'val']:\n",
    "        data = ACDCDataset(data=scanner, debug=False)\n",
    "    elif scanner in ['A', 'B', 'C', 'D']:\n",
    "        data = MNMDataset(vendor=scanner, debug=False)\n",
    "    gen = SingleImageMultiViewDataLoader(data, batch_size=n_views, return_orig=True)\n",
    "    multithreaded_gen = MultiThreadedAugmenter(gen, augmentor, 1, 1, seeds=None)\n",
    "\n",
    "    ### - Init Model and hook feature extractors\n",
    "    root = '../../'\n",
    "    middle = 'unet8_'\n",
    "    pre = 'acdc'\n",
    "    name = f'{pre}_{middle}{unet_id}'\n",
    "\n",
    "    model_path = f'{root}pre-trained-tmp/trained_UNets/{name}_best.pt'\n",
    "    state_dict = torch.load(model_path)['model_state_dict']\n",
    "    n_chans_out = 4\n",
    "    unet = UNet2D(n_chans_in=1, \n",
    "                  n_chans_out=n_chans_out, \n",
    "                  n_filters_init=8, \n",
    "                  dropout=False)\n",
    "    #unet.load_state_dict(state_dict)\n",
    "    model_aug_hooks = LayerAugDiff(unet, measure='mse', copy=True)\n",
    "    \n",
    "    disabled_ids = ['shortcut0', 'shortcut1', 'shortcut2']\n",
    "    DAEs = nn.ModuleDict({'up3': AugResDAE(in_channels = 64, \n",
    "                                        in_dim      = 32,\n",
    "                                        latent_dim  = 128,\n",
    "                                        depth       = 3,\n",
    "                                        block_size  = 1)})\n",
    "\n",
    "\n",
    "    for layer_id in disabled_ids:\n",
    "        DAEs[layer_id] = nn.Identity()\n",
    "\n",
    "    transformer = Frankenstein(seg_model=unet,\n",
    "                         transformations=DAEs,\n",
    "                         disabled_ids=disabled_ids,\n",
    "                         copy=True)\n",
    "\n",
    "    model_path = f'{root}pre-trained-tmp/trained_AEs/acdc_AugResDAE{unet_id}_base_best.pt'    \n",
    "    state_dict = torch.load(model_path)['model_state_dict']\n",
    "    transformer.load_state_dict(state_dict)\n",
    "    \n",
    "    #model.seg_model = model_aug_hooks\n",
    "    transformer.hook_transformations(transformer.transformations, n_samples=-1)\n",
    "    transformer.seg_model.load_state_dict(unet.state_dict())\n",
    "    model = LayerAugDiff(transformer.seg_model, measure='mse', copy=False)\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(0)\n",
    "    \n",
    "    transformer.eval()\n",
    "    transformer.to(0)\n",
    "    ### - gather diffs across all inputs with their respective views\n",
    "    for i in range(n_imgs):\n",
    "        batch = multithreaded_gen.next()\n",
    "        input_ = torch.cat([batch['data_orig'], batch['data']], dim=0)\n",
    "        input_mse = torch.pow(\n",
    "                        input_[:1] - input_[1:],\n",
    "                        2\n",
    "                    ).mean()\n",
    "        input_mse /=  torch.abs(input_[:1]).mean()\n",
    "        model.diffs.setdefault('Augmented Input', []).append(input_mse.item())\n",
    "        output = softmax(model(input_.cuda()), dim=1).detach().cpu()\n",
    "        output_mse = torch.pow(\n",
    "                        output[:1] - output[1:],\n",
    "                        2\n",
    "                    ).mean()\n",
    "        output_mse = output_mse / torch.abs(output[:1]).mean()\n",
    "        model.diffs.setdefault('Softmax Output', []).append(output_mse.item())\n",
    "\n",
    "    ### - build dataframe for plotting\n",
    "    df_diffs = pd.DataFrame(columns=['layer', 'group', 'diff'])\n",
    "    for i, key in enumerate(model.diffs):\n",
    "        entries  = [[key, key.partition('.')[0], diff] for diff in model.diffs[key]]\n",
    "        df_diffs = pd.concat([df_diffs, pd.DataFrame(entries, columns=['layer', 'group', 'diff'])])\n",
    "\n",
    "    ### - build plot\n",
    "    sns.set(rc={'figure.figsize':(30,15)})\n",
    "    g = sns.boxplot(\n",
    "        data=df_diffs, \n",
    "        x='layer', \n",
    "        y='diff', \n",
    "        showfliers=False, \n",
    "        showcaps=False, \n",
    "        showbox=False,\n",
    "        whis=0,\n",
    "        medianprops=dict(color='red')\n",
    "    )\n",
    "    \n",
    "    if scaled:\n",
    "        g.set(ylim=(-0.1, 1.3))\n",
    "    sns.despine(offset=10, trim=True)\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    if augmentations == 'all':\n",
    "        aug_str = 'all'\n",
    "    elif augmentations == 'output_invariant':\n",
    "        aug_str = 'output-invariant'\n",
    "\n",
    "    title = f\"Normalized MSE to Original per Layer for identical Inputs with {aug_str} Augmentations - Data {scanner.upper()}\"\n",
    "    plt.title(title, fontsize = 30)\n",
    "    plt.ylabel(\"Avrg standard deviation over Pixel and Channel\", fontsize=20)\n",
    "    plt.xlabel(\"Layer\", fontsize=20)\n",
    "    save_path = f\"../../results-tmp/results/eval/variations-across-layers_MSE_{aug_str}_{scanner}_{'scaled' if scaled else 'unscaled'}_{unet_id}_untrained.jpg\"\n",
    "    print(save_path)\n",
    "    plt.savefig(save_path, bbox_inches='tight', dpi=400)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f61e6915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset\n",
      "loading all case properties\n",
      "2023-08-22 15:38:02.746119: Using splits from existing split file: ../../../nnUNet/data/nnUNet_preprocessed/Task500_ACDC/splits_final.pkl\n",
      "2023-08-22 15:38:02.747222: The split file contains 5 splits.\n",
      "2023-08-22 15:38:02.747377: Desired fold for training: 0\n",
      "2023-08-22 15:38:02.747958: This split has 160 training and 40 validation cases.\n",
      "unpacking dataset\n",
      "done\n",
      "loading dataset\n",
      "loading all case properties\n",
      "../../results-tmp/results/eval/variations-across-layers_MSE_output-invariant_train_unscaled_0.jpg\n",
      "loading dataset\n",
      "loading all case properties\n",
      "2023-08-22 15:38:21.807901: Using splits from existing split file: ../../../nnUNet/data/nnUNet_preprocessed/Task500_ACDC/splits_final.pkl\n",
      "2023-08-22 15:38:21.808832: The split file contains 5 splits.\n",
      "2023-08-22 15:38:21.809078: Desired fold for training: 0\n",
      "2023-08-22 15:38:21.809476: This split has 160 training and 40 validation cases.\n",
      "unpacking dataset\n",
      "done\n",
      "loading dataset\n",
      "loading all case properties\n",
      "../../results-tmp/results/eval/variations-across-layers_MSE_output-invariant_C_unscaled_0_untrained.jpg\n"
     ]
    }
   ],
   "source": [
    "plot_layer_diff(scanner='C', augmentations='output_invariant', scaled=False, unet_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885061c0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_layer_diff(\n",
    "    scanner: str, \n",
    "    augmentations: str, \n",
    "    scaled: bool, \n",
    "    n_views: int = 32, \n",
    "    n_imgs: int = 100, \n",
    "    unet_id: str = 0\n",
    "):\n",
    "    ### - build augmentator\n",
    "    # where data is saved\n",
    "    nnUnet_prefix = '../../../nnUNet/'\n",
    "    # Initialize trainer to get data loaders with data augmentations from training\n",
    "    pkl_file          = nnUnet_prefix + 'data/nnUNet_preprocessed/Task500_ACDC/nnUNetPlansv2.1_plans_2D.pkl'\n",
    "    fold              = 0\n",
    "    output_folder     = nnUnet_prefix + 'results/nnUnet/nnUNet/2d/Task027_ACDC/nnUNetTrainerV2__nnUNetPlansv2.1/'\n",
    "    dataset_directory = nnUnet_prefix + 'data/nnUNet_preprocessed/Task500_ACDC'\n",
    "    trainer = nnUNetTrainerV2(pkl_file, 0, output_folder, dataset_directory)\n",
    "    trainer.initialize()\n",
    "    train_loader = trainer.tr_gen\n",
    "    # filter augmentations w.r.t. given keyword and init augmentor\n",
    "    if augmentations == 'all':\n",
    "        transforms = [t for t in train_loader.transform.transforms]\n",
    "    elif augmentations == 'output_invariant':\n",
    "        data_only_transforms = (\n",
    "            batchgenerators.transforms.noise_transforms.GaussianNoiseTransform,\n",
    "            batchgenerators.transforms.noise_transforms.GaussianBlurTransform,\n",
    "            batchgenerators.transforms.color_transforms.BrightnessMultiplicativeTransform,\n",
    "            batchgenerators.transforms.color_transforms.ContrastAugmentationTransform,\n",
    "            batchgenerators.transforms.color_transforms.GammaTransform,\n",
    "            batchgenerators.transforms.utility_transforms.RemoveLabelTransform,\n",
    "            batchgenerators.transforms.utility_transforms.RenameTransform,\n",
    "            batchgenerators.transforms.utility_transforms.NumpyToTensor,\n",
    "            batchgenerators.transforms.utility_transforms.NumpyToTensor)\n",
    "        \n",
    "        transforms = [t for t in train_loader.transform.transforms if isinstance(t, data_only_transforms)]\n",
    "    augmentor  = batchgenerators.transforms.abstract_transforms.Compose(transforms)\n",
    "    \n",
    "    ### - Load dataset and init batch generator\n",
    "    if scanner in ['train', 'val']:\n",
    "        data = ACDCDataset(data=scanner, debug=False)\n",
    "    elif scanner in ['A', 'B', 'C', 'D']:\n",
    "        data = MNMDataset(vendor=scanner, debug=False)\n",
    "    gen = SingleImageMultiViewDataLoader(data, batch_size=n_views)\n",
    "    multithreaded_gen = MultiThreadedAugmenter(gen, augmentor, 4, 2, seeds=None)\n",
    "    \n",
    "    ### - Init Model and hook feature extractors\n",
    "    root = '../../'\n",
    "    middle = 'unet8_'\n",
    "    pre = 'acdc'\n",
    "    name = f'{pre}_{middle}{unet_id}'\n",
    "\n",
    "    model_path = f'{root}pre-trained-tmp/trained_UNets/{name}_best.pt'\n",
    "    state_dict = torch.load(model_path)['model_state_dict']\n",
    "    n_chans_out = 4\n",
    "    unet = UNet2D(n_chans_in=1, \n",
    "                  n_chans_out=n_chans_out, \n",
    "                  n_filters_init=8, \n",
    "                  dropout=False)\n",
    "    unet.load_state_dict(state_dict)\n",
    "    model = LayerAugDiff(unet, copy=True)\n",
    "    model.eval()\n",
    "    model.to(0)\n",
    "    \n",
    "    ### - gather diffs across all inputs with their respective views\n",
    "    for i in range(n_imgs):\n",
    "        batch = multithreaded_gen.next()\n",
    "        model.diffs.setdefault('Augmented Input', []).append(batch['data'].std(0).mean().item())\n",
    "        out = model(batch['data'].cuda())\n",
    "        model.diffs.setdefault('Softmax Output', []).append(softmax(out, dim=1).std(0).mean().item())\n",
    "    \n",
    "    ### - build dataframe for plotting\n",
    "    df_diffs = pd.DataFrame(columns=['layer', 'group', 'diff'])\n",
    "    for i, key in enumerate(model.diffs):\n",
    "        entries  = [[key, key.partition('.')[0], diff] for diff in model.diffs[key]]\n",
    "        df_diffs = pd.concat([df_diffs, pd.DataFrame(entries, columns=['layer', 'group', 'diff'])])\n",
    "    \n",
    "    ### - build plot\n",
    "    sns.set(rc={'figure.figsize':(30,15)})\n",
    "    g = sns.boxplot(data=df_diffs, x='layer', y='diff')\n",
    "    if scaled:\n",
    "        g.set(ylim=(-0.1, 1.3))\n",
    "    sns.despine(offset=10, trim=True)\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    if augmentations == 'all':\n",
    "        aug_str = 'all'\n",
    "    elif augmentations == 'output_invariant':\n",
    "        aug_str = 'output-invariant'\n",
    "        \n",
    "    title = f\"Variation per BN-Layer for identical Inputs with {aug_str} Augmentations - Data {scanner.upper()}\"\n",
    "    plt.title(title, fontsize = 30)\n",
    "    plt.ylabel(\"Avrg standard deviation over Pixel and Channel\", fontsize=20)\n",
    "    plt.xlabel(\"Layer\", fontsize=20)\n",
    "    save_path = f\"../../results-tmp/results/eval/variations-across-layers_{aug_str}_{scanner}_{'scaled' if scaled else 'unscaled'}_{unet_id}.jpg\"\n",
    "    print(save_path)\n",
    "    #plt.savefig(save_path, bbox_inches='tight', dpi=400)\n",
    "    plt.show()\n",
    "    #plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e7ee86",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plot_layer_diff(scanner='train', augmentations='output_invariant', scaled=False, unet_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd1bb75",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for scanner in ['train', 'val', 'A', 'B', 'C', 'D']:\n",
    "    for augmentations in ['all', 'output_invariant']:\n",
    "        for scaled in [True, False]:\n",
    "            for unet_id in [0, 1]:\n",
    "                plot_layer_diff(scanner=scanner, augmentations=augmentations, scaled=scaled, unet_id=unet_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9422398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "src/demos/OT.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9311e6c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a23fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89174d32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1cda7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe4874f8",
   "metadata": {},
   "source": [
    "# Drafting Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5543a1",
   "metadata": {},
   "source": [
    "## Load existing trainer to reproduce augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f704c38",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## where data is saved\n",
    "nnUnet_prefix = '../../../nnUNet/'\n",
    "\n",
    "## Initialize trainer to get data loaders with data augmentations from training\n",
    "pkl_file          = nnUnet_prefix + 'data/nnUNet_preprocessed/Task500_ACDC/nnUNetPlansv2.1_plans_2D.pkl'\n",
    "fold              = 0\n",
    "output_folder     = nnUnet_prefix + 'results/nnUnet/nnUNet/2d/Task027_ACDC/nnUNetTrainerV2__nnUNetPlansv2.1/'\n",
    "dataset_directory = nnUnet_prefix + 'data/nnUNet_preprocessed/Task500_ACDC'\n",
    "\n",
    "trainer = nnUNetTrainerV2(pkl_file, 0, output_folder, dataset_directory)\n",
    "trainer.initialize()\n",
    "\n",
    "train_loader = trainer.tr_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e217f1d",
   "metadata": {},
   "source": [
    "## Extract augmentations and re-build augmentater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8885efe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_loader.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cf73af",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Collect all transforms that only impact the input, not the segmentation mask\n",
    "data_only_transforms = (\n",
    "    batchgenerators.transforms.noise_transforms.GaussianNoiseTransform,\n",
    "    batchgenerators.transforms.noise_transforms.GaussianBlurTransform,\n",
    "    batchgenerators.transforms.color_transforms.BrightnessMultiplicativeTransform,\n",
    "    batchgenerators.transforms.color_transforms.ContrastAugmentationTransform,\n",
    "    batchgenerators.transforms.color_transforms.GammaTransform,\n",
    "    batchgenerators.transforms.utility_transforms.NumpyToTensor\n",
    "    )\n",
    "\n",
    "# Filter transforms for input-only augmentations and build an augmenter\n",
    "#transforms = [t for t in train_loader.transform.transforms if isinstance(t, data_only_transforms)]\n",
    "transforms = [t for t in train_loader.transform.transforms]\n",
    "augmenter  = batchgenerators.transforms.abstract_transforms.Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e6f3a2",
   "metadata": {},
   "source": [
    "## Custom Dataloader for single-image multi-view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334ec7ca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# define single image dataloader from batchgenerator example here:\n",
    "# https://github.com/MIC-DKFZ/batchgenerators/blob/master/batchgenerators/examples/example_ipynb.ipynb\n",
    "class DataLoader(batchgenerators.dataloading.data_loader.SlimDataLoaderBase):\n",
    "    def __init__(self, data, batch_size=2):\n",
    "        super(DataLoader, self).__init__(data, batch_size) \n",
    "        # data is now stored in self._data.\n",
    "    \n",
    "    def generate_train_batch(self):\n",
    "        # usually you would now select random instances of your data. We only have one therefore we skip this\n",
    "        \n",
    "        data = self._data[randrange(len(self._data))]\n",
    "        img = data['input'].numpy()\n",
    "        tar = data['target'].numpy()\n",
    "        \n",
    "        \n",
    "        # The camera image has only one channel. Our batch layout must be (b, c, x, y). Let's fix that\n",
    "        img = np.tile(img, (self.batch_size, 1, 1, 1))\n",
    "        tar = np.tile(tar, (self.batch_size, 1, 1, 1))\n",
    "        # now construct the dictionary and return it. np.float32 cast because most networks take float\n",
    "        return {'data':img.astype(np.float32), \n",
    "                'target': tar.astype(np.float32), \n",
    "                'seg': tar.astype(np.float32), 'some_other_key':'some other value'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f40816e",
   "metadata": {},
   "source": [
    "## Load data and init generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d2cecc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#test_set = ACDCDataset(data='train', debug=True)\n",
    "test_set = MNMDataset(vendor='D', debug=False)\n",
    "#acdc_train_loader = DataLoader(acdc_train, batch_size=32, shuffle=False, drop_last=False)\n",
    "#loader['acdc_train'] = acdc_train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d05ab6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "batchgen = DataLoader(test_set, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d49c23f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "multithreaded_generator = batchgenerators.dataloading.multi_threaded_augmenter.MultiThreadedAugmenter(batchgen, augmenter, 4, 2, seeds=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472a4d81",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfd36ae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_batch(batch):\n",
    "    batch_size = batch['data'].shape[0]\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    for i in range(batch_size):\n",
    "        plt.subplot(1, batch_size, i+1)\n",
    "        plt.imshow(batch['data'][i, 0], cmap=\"gray\") # only grayscale image here\n",
    "    plt.show()\n",
    "plot_batch(multithreaded_generator.next())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33d0840",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "batch = multithreaded_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66d83b0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(batch['data'].std((0,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b257dc",
   "metadata": {},
   "source": [
    "## U-Net and hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d09352",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "root = '../../'\n",
    "middle = 'unet8_'\n",
    "pre = 'acdc'\n",
    "name = f'{pre}_{middle}0'\n",
    "\n",
    "model_path = f'{root}pre-trained-tmp/trained_UNets/{name}_best.pt'\n",
    "state_dict = torch.load(model_path)['model_state_dict']\n",
    "n_chans_out = 4\n",
    "unet = UNet2D(n_chans_in=1, \n",
    "              n_chans_out=n_chans_out, \n",
    "              n_filters_init=8, \n",
    "              dropout=False)\n",
    "unet.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b0ea32",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Find layer names to attach hooks to. We aim to attach after\n",
    "## each BN layer. This way, aggregate metrics are comparable\n",
    "## up until normalization issues\n",
    "\n",
    "### Does it make sense? Do BN Layers delete any signal?\n",
    "for name, module in unet.named_modules():\n",
    "    if '.bn' in name:\n",
    "        print(name)\n",
    "    if name == 'init_path.4.conv_path.1.bn':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b375a566",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model = LayerAugDiff(unet, copy=True)\n",
    "model.eval()\n",
    "model.to(0)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee583f5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "batchgen = DataLoader(test_set, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c90ba64",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "multithreaded_generator = batchgenerators.dataloading.multi_threaded_augmenter.MultiThreadedAugmenter(batchgen, augmenter, 4, 2, seeds=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271d929b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    batch = multithreaded_generator.next()\n",
    "    model.diffs.setdefault('Augmented Input', []).append(batch['data'].std(0).mean().item())\n",
    "    out = model(batch['data'].cuda())\n",
    "    model.diffs.setdefault('Softmax Output', []).append(softmax(out, dim=1).std(0).mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868925da",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_diffs = pd.DataFrame(columns=['layer', 'group', 'diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416577ce",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for i, key in enumerate(model.diffs):\n",
    "    entries  = [[key, key.partition('.')[0], diff] for diff in model.diffs[key]]\n",
    "    df_diffs = pd.concat([df_diffs, pd.DataFrame(entries, columns=['layer', 'group', 'diff'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82488018",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(30,15)})\n",
    "g = sns.boxplot(data=df_diffs, x='layer', y='diff')\n",
    "g.set(ylim=(-0.1, 1.3))\n",
    "sns.despine(offset=10, trim=True)\n",
    "plt.xticks(rotation=90)\n",
    "#plt.title(\"Variation per BN-Layer for identical Inputs with 'output-invariant' Augmentations - Testset D\", fontsize = 30)\n",
    "plt.title(\"Variation per BN-Layer for identical Inputs with ALL Augmentations - Train Data\", fontsize = 30)\n",
    "plt.ylabel(\"Avrg standard deviation over Pixel and Channel\", fontsize=20)\n",
    "plt.xlabel(\"Layer\", fontsize=20)\n",
    "#plt.ylim(-0.1, 1.3)\n",
    "plt.show()\n",
    "#plt.savefig(\"../../results-tmp/results/eval/variations-across-layers_output-invariance_testset-D_not-scaled.jpg\", bbox_inches='tight', dpi=400)\n",
    "plt.savefig(\"../../results-tmp/results/eval/variations-across-layers_no-output-invariance_train_scaled.jpg\", bbox_inches='tight', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f460faae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eed66a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd85905e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
