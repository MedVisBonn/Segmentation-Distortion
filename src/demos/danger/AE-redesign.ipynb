{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a573b0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "import os, sys\n",
    "import time\n",
    "import numpy as np\n",
    "import collections\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torchvision.transforms import Resize, CenterCrop\n",
    "from typing import Iterable, Dict, Callable, Tuple\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randrange\n",
    "from copy import deepcopy\n",
    "\n",
    "from nnunet.training.model_restore import restore_model\n",
    "import batchgenerators\n",
    "from batchgenerators.transforms.local_transforms import *\n",
    "from batchgenerators.dataloading.single_threaded_augmenter import SingleThreadedAugmenter\n",
    "from batchgenerators.dataloading.multi_threaded_augmenter import MultiThreadedAugmenter\n",
    "from batchgenerators.utilities.file_and_folder_operations import *\n",
    "from nnunet.paths import preprocessing_output_dir\n",
    "from nnunet.training.dataloading.dataset_loading import *\n",
    "from nnunet.training.network_training.nnUNetTrainerV2 import nnUNetTrainerV2\n",
    "from nnunet.run.load_pretrained_weights import load_pretrained_weights\n",
    "\n",
    "sys.path.append('..')\n",
    "from dataset import CalgaryCampinasDataset, ACDCDataset, MNMDataset\n",
    "from utils import EarlyStopping, epoch_average, average_metrics\n",
    "from model.layers import ConvBlock\n",
    "from model.dae import AugResDAE\n",
    "from model.unet import UNet2D\n",
    "from model.wrapper import Frankenstein\n",
    "from losses import MNMCriterionAE, SampleDice, UnetDice\n",
    "from trainer.ae_trainer import AETrainerACDC\n",
    "\n",
    "nnUnet_prefix = '../../../nnUNet/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5030147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "from typing import Iterable, Dict, Callable, Tuple, Union\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Conv block for the AE model.\n",
    "\n",
    "    Dynamic conv block that supports both down and up\n",
    "    convolutions as well as different block sizes. It's\n",
    "    the main building block for the auto encoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        in_dim: int,\n",
    "        kernel_size: int = 3,\n",
    "        stride: int = 2,\n",
    "        padding: int = 1,\n",
    "        block_size: int = 1,\n",
    "        reverse: bool = False,\n",
    "        residual: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.residual = residual\n",
    "\n",
    "        in_channels = int(in_channels)\n",
    "        out_channels = int(out_channels)\n",
    "        in_dim = int(in_dim)\n",
    "\n",
    "        if not reverse:\n",
    "            if block_size > 1:\n",
    "                self.block = nn.Sequential(\n",
    "                    *chain.from_iterable(\n",
    "                        [\n",
    "                            [\n",
    "                                nn.Conv2d(\n",
    "                                    in_channels,\n",
    "                                    in_channels,\n",
    "                                    kernel_size=kernel_size,\n",
    "                                    stride=1,\n",
    "                                    padding=padding,\n",
    "                                ),\n",
    "                                nn.LayerNorm(\n",
    "                                    torch.Size([in_channels, in_dim, in_dim])\n",
    "                                ),\n",
    "                                nn.LeakyReLU(),\n",
    "                            ]\n",
    "                            for _ in range(block_size - 1)\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            self.sample = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=stride,\n",
    "                    padding=padding,\n",
    "                ),\n",
    "                nn.LayerNorm(torch.Size([out_channels, in_dim, in_dim])),\n",
    "                nn.LeakyReLU(),\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            if padding == 0:\n",
    "                output_padding = 0\n",
    "            else:\n",
    "                output_padding = 1\n",
    "\n",
    "            if block_size > 1:\n",
    "                self.block = nn.Sequential(\n",
    "                    *chain.from_iterable(\n",
    "                        [\n",
    "                            [\n",
    "                                nn.ConvTranspose2d(\n",
    "                                    in_channels,\n",
    "                                    in_channels,\n",
    "                                    kernel_size=kernel_size,\n",
    "                                    stride=1,\n",
    "                                    padding=padding,\n",
    "                                    output_padding=0,\n",
    "                                ),\n",
    "                                nn.LayerNorm(\n",
    "                                    torch.Size([in_channels, in_dim, in_dim])\n",
    "                                ),\n",
    "                                nn.LeakyReLU(),\n",
    "                            ]\n",
    "                            for _ in range(block_size - 1)\n",
    "                        ]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            self.sample = nn.Sequential(\n",
    "                nn.ConvTranspose2d(\n",
    "                    in_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=stride,\n",
    "                    padding=padding,\n",
    "                    output_padding=output_padding,\n",
    "                ),\n",
    "                nn.LayerNorm(torch.Size([out_channels, in_dim, in_dim])),\n",
    "                nn.LeakyReLU(),\n",
    "            )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if self.block_size > 1:\n",
    "            x = x + self.block(x) if self.residual else self.block(x)\n",
    "        return self.sample(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "551762ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_in = torch.ones((2,16,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2316a456",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = ConvBlock(\n",
    "    in_channels=16,\n",
    "    out_channels=64,\n",
    "    in_dim=32,\n",
    "    kernel_size=3,\n",
    "    stride=1,\n",
    "    padding=1,\n",
    "    block_size=2,\n",
    "    reverse=False,\n",
    "    residual=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2de822df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = layer(x_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50416961",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = nn.ModuleList(nn.Linear(1,1) for i in range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aa238101",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAE(nn.Module):\n",
    "    \"\"\"Autoencoder (AE) class to transform U-Net feature maps.\n",
    "\n",
    "    Module that dynamically builds an AE. It expects a certain\n",
    "    input shape (due to the fc layer). It uses conv blocks\n",
    "    (see layers.py) and supports different depths, block sizes\n",
    "    and latent dimensions. Currently, it only supports a dense\n",
    "    bottleneck.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        in_dim: int,\n",
    "        latent_dim: int = 128,\n",
    "        depth: int = 3,\n",
    "        block_size: int = 1,\n",
    "        residual: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.on = True\n",
    "        self.in_channels = in_channels\n",
    "        self.in_dim = in_dim\n",
    "        self.depth = depth\n",
    "        self.latent_dim = latent_dim\n",
    "        self.block_size = block_size\n",
    "        self.residual = residual\n",
    "        \n",
    "        \n",
    "        self.init = ConvBlock(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.in_channels,\n",
    "            in_dim=self.in_dim,\n",
    "            block_size=self.block_size,\n",
    "            residual=self.residual,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,)\n",
    "        self.encoder = self._build_encoder()\n",
    "        self.decoder = self._build_decoder()\n",
    "        self.out     = nn.Conv2d(in_channels, in_channels, 1)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def _build_encoder(self):\n",
    "        encoder_list = nn.ModuleList(\n",
    "            ConvBlock(\n",
    "                in_channels=self.in_channels // 4**i,\n",
    "                out_channels=self.in_channels // 4**(i+1),\n",
    "                in_dim=self.in_dim,\n",
    "                block_size=self.block_size,\n",
    "                residual=self.residual,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ) for i in range(self.depth)\n",
    "        )\n",
    "        return encoder_list \n",
    "\n",
    "    \n",
    "    def _build_decoder(self):\n",
    "        decoder_list = nn.ModuleList(\n",
    "            ConvBlock(\n",
    "                in_channels=self.in_channels // 4**(i+1),\n",
    "                out_channels=self.in_channels // 4**i,\n",
    "                in_dim=self.in_dim,\n",
    "                block_size=self.block_size,\n",
    "                residual=self.residual,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "            ) for i in reversed(range(self.depth))\n",
    "        )\n",
    "        return decoder_list\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoder_outputs = []\n",
    "        \n",
    "        # Encoding\n",
    "        x = self.init(x)\n",
    "        encoder_outputs.append(x)\n",
    "        for enc_layer in self.encoder:\n",
    "            x = enc_layer(x)\n",
    "            encoder_outputs.append(x)\n",
    "\n",
    "        # Decoding\n",
    "        for i, dec_layer in enumerate(self.decoder):\n",
    "            if i > 0:\n",
    "                x = dec_layer(x) + encoder_outputs[-(i + 2)]  # Skip connection\n",
    "            else:\n",
    "                x = dec_layer(x) # no skip connection if bottleneck\n",
    "        # Output layer\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4ef40cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 32, 32])\n",
      "torch.Size([1, 4, 32, 32])\n",
      "torch.Size([1, 1, 32, 32])\n",
      "torch.Size([1, 4, 32, 32]) torch.Size([1, 4, 32, 32])\n",
      "torch.Size([1, 16, 32, 32]) torch.Size([1, 16, 32, 32])\n",
      "torch.Size([1, 64, 32, 32]) torch.Size([1, 64, 32, 32])\n",
      "Output shape: torch.Size([1, 64, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Your ChannelAE class definition here\n",
    "\n",
    "# Instantiate the model\n",
    "model = ChannelAE(in_channels=64, in_dim=32)\n",
    "\n",
    "# Generate a dummy input\n",
    "dummy_input = torch.randn(1, 64, 32, 32)  # Batch size of 1, 64 channels, 32x32 spatial dimension\n",
    "\n",
    "# Run forward pass\n",
    "output = model(dummy_input)\n",
    "\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ede3b89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in reversed(range(model.depth)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acfa76a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
