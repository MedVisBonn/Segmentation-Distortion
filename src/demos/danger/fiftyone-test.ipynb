{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e87c3b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "661e54f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Please cite the following paper when using nnUNet:\n",
      "\n",
      "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n",
      "\n",
      "\n",
      "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "from typing import Iterable, Dict, List, Callable, Tuple, Union, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('../')\n",
    "from dataset import ACDCDataset, MNMDataset\n",
    "from model.unet import UNet2D\n",
    "from model.ae import AE\n",
    "from model.dae import resDAE, AugResDAE\n",
    "from model.wrapper import Frankenstein, ModelAdapter\n",
    "from losses import DiceScoreMMS\n",
    "from utils import  epoch_average, UMapGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f85eed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset\n",
      "loading all case properties\n"
     ]
    }
   ],
   "source": [
    "acdc_train = ACDCDataset(data='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a58210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = acdc_train[0*10]\n",
    "img = data['input']\n",
    "mask = data['target']y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fff80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0285538",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img.squeeze() + mask.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c1c25a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset\n",
      "loading all case properties\n"
     ]
    }
   ],
   "source": [
    "### - datasets\n",
    "#acdc_train = ACDCDataset(data='val')\n",
    "\n",
    "\n",
    "\n",
    "debug = False\n",
    "loader = {}\n",
    "vendor = 'A'\n",
    "\n",
    "mnm_a = MNMDataset(vendor=vendor, debug=debug)\n",
    "mnm_a_loader = DataLoader(mnm_a, batch_size=1, shuffle=False, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cace125",
   "metadata": {},
   "outputs": [],
   "source": [
    "### - init unets\n",
    "# U-Nets\n",
    "ROOT = '../../'\n",
    "middle = 'unet8_'\n",
    "pre = 'acdc'\n",
    "unet_names = [f'{pre}_{middle}{i}' for i in range(10)]\n",
    "unets = []\n",
    "for name in unet_names:\n",
    "    model_path = f'{ROOT}pre-trained-tmp/trained_UNets/{name}_best.pt'\n",
    "    state_dict = torch.load(model_path)['model_state_dict']\n",
    "    n_chans_out = 4\n",
    "    unet = UNet2D(n_chans_in=1, \n",
    "                  n_chans_out=n_chans_out, \n",
    "                  n_filters_init=8, \n",
    "                  dropout=False)\n",
    "    unet.load_state_dict(state_dict)\n",
    "    unets.append(unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b4acf722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init models\n",
    "post = 'localAug_multiImgSingleView_res_balanced_same'\n",
    "post = 'localAug_multiImgSingleView_recon_balanced_same'\n",
    "disabled_ids = ['shortcut0', 'shortcut1', 'shortcut2']\n",
    "models = []\n",
    "for i, unet in enumerate(unets):\n",
    "    DAEs = nn.ModuleDict({'up3': AugResDAE(in_channels = 64, \n",
    "                                        in_dim      = 32,\n",
    "                                        latent_dim  = 256,\n",
    "                                        depth       = 3,\n",
    "                                        block_size  = 4,\n",
    "                                        residual    = False),\n",
    "                         })\n",
    "\n",
    "\n",
    "    for layer_id in disabled_ids:\n",
    "        DAEs[layer_id] = nn.Identity()\n",
    "    \n",
    "    model = Frankenstein(seg_model=unet,\n",
    "                         transformations=DAEs,\n",
    "                         disabled_ids=disabled_ids,\n",
    "                         copy=True)\n",
    "    model_path = f'{ROOT}pre-trained-tmp/trained_AEs/acdc_AugResDAE{i}_{post}_best.pt'\n",
    "    #model_path = f'{ROOT}pre-trained-tmp/trained_AEs/{pre}_resDAE{i}_{post}_best.pt'\n",
    "    #model_path = f'{ROOT}pre-trained-tmp/trained_AEs/acdc_epinet_CE-only_prior-1_best.pt'localAug_multiImgSingleView_res\n",
    "    #model_path = f'{ROOT}pre-trained-tmp/trained_AEs/acdc_resDAE0_venus_best.pt'\n",
    "    state_dict = torch.load(model_path)['model_state_dict']\n",
    "    model.load_state_dict(state_dict)\n",
    "    # Remove trainiung hooks, add evaluation hooks\n",
    "    model.remove_all_hooks()        \n",
    "    model.hook_inference_transformations(model.transformations,\n",
    "                               n_samples=1)\n",
    "    # Put model in evaluation state\n",
    "    model.eval()\n",
    "    model.freeze_seg_model()\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b205f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Init two models for UNet 0, Reconstruction and Residual\n",
    "\n",
    "# init models\n",
    "posts = ['localAug_multiImgSingleView_res_balanced_same', 'localAug_multiImgSingleView_recon_balanced_same']\n",
    "residuals = [True, False]\n",
    "disabled_ids = ['shortcut0', 'shortcut1', 'shortcut2']\n",
    "models = []\n",
    "for i, (post, residual) in enumerate(zip(posts, residuals)):\n",
    "    DAEs = nn.ModuleDict({'up3': AugResDAE(in_channels = 64, \n",
    "                                        in_dim      = 32,\n",
    "                                        latent_dim  = 256,\n",
    "                                        depth       = 3,\n",
    "                                        block_size  = 4,\n",
    "                                        residual    = residual),\n",
    "                         })\n",
    "\n",
    "\n",
    "    for layer_id in disabled_ids:\n",
    "        DAEs[layer_id] = nn.Identity()\n",
    "    \n",
    "    model = Frankenstein(seg_model=unet,\n",
    "                         transformations=DAEs,\n",
    "                         disabled_ids=disabled_ids,\n",
    "                         copy=True)\n",
    "    model_path = f'{ROOT}pre-trained-tmp/trained_AEs/acdc_AugResDAE1_{post}_best.pt'\n",
    "    #model_path = f'{ROOT}pre-trained-tmp/trained_AEs/{pre}_resDAE{i}_{post}_best.pt'\n",
    "    #model_path = f'{ROOT}pre-trained-tmp/trained_AEs/acdc_epinet_CE-only_prior-1_best.pt'localAug_multiImgSingleView_res\n",
    "    #model_path = f'{ROOT}pre-trained-tmp/trained_AEs/acdc_resDAE0_venus_best.pt'\n",
    "    state_dict = torch.load(model_path)['model_state_dict']\n",
    "    model.load_state_dict(state_dict)\n",
    "    # Remove trainiung hooks, add evaluation hooks\n",
    "    model.remove_all_hooks()        \n",
    "    model.hook_inference_transformations(model.transformations,\n",
    "                               n_samples=1)\n",
    "    # Put model in evaluation state\n",
    "    model.eval()\n",
    "    model.freeze_seg_model()\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ec8a29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "import fiftyone as fo\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from torchmetrics import Dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7f17b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     dataset.delete()\n",
    "# except:\n",
    "#     print(\"dataset name already available or dataset didnt exist\")\n",
    "\n",
    "\n",
    "# # build first dataset\n",
    "# # init dataset\n",
    "# dataset = fo.Dataset(name=\"segmentation_dataset_test\")\n",
    "# # add group and all Groups we need\n",
    "# dataset.add_group_field(\"group\", default=\"image\")\n",
    "\n",
    "# # set mask targets\n",
    "# ## ground truth targets\n",
    "# dataset.mask_targets = {\n",
    "#     \"ground_truth\": {0: \"background\",\n",
    "#                      1: \"LV\",\n",
    "#                      2: \"MYO\",\n",
    "#                      3: \"RV\"}\n",
    "# }\n",
    "# # error map labels\n",
    "# for i in range(n_unets):\n",
    "#     dataset.mask_targets[f'errormap_it:{i}'] = {1: 'error'}\n",
    "    \n",
    "# # make temporary dir for data handling\n",
    "# os.makedirs('tmp', exist_ok=True)\n",
    "# path = 'tmp/'\n",
    "# # init sample list. We save each sample here and add it to the\n",
    "# # dataset in the end\n",
    "# samples = []\n",
    "# n_unets = 1\n",
    "# # init dice score class\n",
    "# dcs = Dice(num_classes=4, ignore_index=0)\n",
    "# # init umap generator\n",
    "# umap_generator_entropy = UMapGenerator(method='entropy', net_out='mms')\n",
    "# umap_generator_AE = UMapGenerator(method='ae', net_out='mms')\n",
    "\n",
    "# # itertatively make samples\n",
    "# for i in range(10):\n",
    "#     # get data\n",
    "#     data = mnm_a[i*10]\n",
    "#     img = data['input']\n",
    "#     mask = data['target']\n",
    "#     mask[mask < 0] = 0\n",
    "    \n",
    "#     # save image to disk\n",
    "#     img_path  = path + f'test_{i}.png'\n",
    "#     img_norm  = img - img.min()\n",
    "#     img_norm /= img_norm.max()\n",
    "\n",
    "#     save_image(img_norm, img_path)\n",
    "#     # make sample\n",
    "#     group = fo.Group()\n",
    "#     sample_image = fo.Sample(\n",
    "#         filepath=img_path,\n",
    "#         group=group.element('image'),\n",
    "#         ground_truth=fo.Segmentation(\n",
    "#             mask=mask.squeeze().numpy()\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "#     sample_activation = fo.Sample(\n",
    "#         filepath=img_path,\n",
    "#         group=group.element('test'),\n",
    "#         ground_truth=fo.Segmentation(\n",
    "#             mask=mask.squeeze().numpy()\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "#     # predictions, error maps, umaps and foreground dice\n",
    "#     DSC = torch.zeros((n_unets))\n",
    "#     for i in range(n_unets):\n",
    "#         # U-Net - Predictions and errormaps\n",
    "#         unet_output = unets[i](img.unsqueeze(0))\n",
    "#         pred = torch.argmax(unet_output, dim=1, keepdims=True)\n",
    "#         err_map = (pred != mask)\n",
    "#         sample_image[f'pred_unet_it:{i}'] = fo.Segmentation(mask=pred.squeeze().numpy())\n",
    "#         sample_image[f'error_unet_it:{i}']  = fo.Segmentation(mask=err_map.squeeze().numpy())\n",
    "#         # umaps\n",
    "#         ## entropy\n",
    "#         umap_entropy = umap_generator_entropy(unet_output)\n",
    "#         sample_image[f'umap_entropy_it:{i}'] = fo.Heatmap(map=umap_entropy.squeeze().numpy())\n",
    "#         ## segmentation distortion - predictions with AE and UQ map\n",
    "#         model_output  = models[i](img.unsqueeze(0))\n",
    "#         model_pred    = torch.argmax(model_output[1], dim=0, keepdims=False)\n",
    "#         model_err_map = (model_pred != mask.squeeze())\n",
    "#         umap_ae = umap_generator_AE(model_output)\n",
    "#         sample_image[f'pred_ae_it:{i}'] = fo.Segmentation(mask=model_pred.squeeze().numpy())\n",
    "#         sample_image[f'error_ae_it:{i}']  = fo.Segmentation(mask=model_err_map.squeeze().numpy())\n",
    "#         sample_image[f'umap_ae_it:{i}'] = fo.Heatmap(map=umap_ae.squeeze().numpy())\n",
    "        \n",
    "#         # dice score\n",
    "#         DSC[i] = dcs(pred.squeeze(), mask.squeeze().int())        \n",
    "    \n",
    "#     # get dice related tags per sample\n",
    "#     DSC = DSC.mean()\n",
    "#     thresholds = [0.3, 0.5, 0.8]\n",
    "#     for threshold in thresholds:\n",
    "#         if DSC < threshold:\n",
    "#             sample_image.tags.append(f'DSC_below_{int((threshold * 100))}')\n",
    "        \n",
    "#     # add sample to sample list\n",
    "#     samples += [sample_image, sample_activation]\n",
    "    \n",
    "# # add samples to dataset\n",
    "# dataset.add_samples(samples)\n",
    "\n",
    "\n",
    "# # validate dataset\n",
    "# print(dataset)\n",
    "\n",
    "# # export dataset\n",
    "# export_dir = os.path.expanduser('~') + '/fiftyone/test-dataset'\n",
    "# dataset.export(export_dir=export_dir, dataset_type=fo.types.FiftyOneDataset)\n",
    "\n",
    "# # clean up. remove tmp\n",
    "# shutil.rmtree('tmp')\n",
    "# #dataset.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053e1c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dataset.delete()\n",
    "except:\n",
    "    print(\"dataset name already available or dataset didnt exist\")\n",
    "\n",
    "\n",
    "# build first dataset\n",
    "n_unets = 2\n",
    "export_dir = os.path.expanduser('~') + '/fiftyone/test-dataset'\n",
    "# init dataset\n",
    "dataset = fo.Dataset(name=\"test\")\n",
    "# add group and all Groups we need\n",
    "dataset.add_group_field(\"group\", default=\"it_0\")\n",
    "\n",
    "# set mask targets\n",
    "## ground truth targets\n",
    "dataset.mask_targets = {\n",
    "    \"ground_truth\": {0: \"background\",\n",
    "                     1: \"LV\",\n",
    "                     2: \"MYO\",\n",
    "                     3: \"RV\"}\n",
    "}\n",
    "# error map labels\n",
    "for i in range(n_unets):\n",
    "    dataset.mask_targets[f'errormap_it:{i}'] = {1: 'error'}\n",
    "    \n",
    "# make temporary dir for data handling\n",
    "os.makedirs('tmp', exist_ok=True)\n",
    "path = 'tmp/'\n",
    "# init sample list. We save each sample here and add it to the\n",
    "# dataset in the end\n",
    "samples = []\n",
    "# init dice score class\n",
    "dcs = Dice(num_classes=4, ignore_index=0)\n",
    "# init umap generator\n",
    "umap_generator_entropy = UMapGenerator(method='entropy', net_out='mms')\n",
    "umap_generator_AE = UMapGenerator(method='ae', net_out='mms')\n",
    "\n",
    "# itertatively make samples\n",
    "for i in range(20, 30):\n",
    "    # get data\n",
    "    data = acdc_train[i*10]\n",
    "    img = data['input']\n",
    "    mask = data['target']\n",
    "    mask[mask < 0] = 0\n",
    "    \n",
    "    # save image to disk\n",
    "    img_path  = path + f'img_{i}.png'\n",
    "    #print(img_path)\n",
    "    img_norm  = img - img.min()\n",
    "    img_norm /= img_norm.max()\n",
    "\n",
    "    save_image(img_norm, img_path)\n",
    "    torch.save(img, path + f'img_{i}.pt')\n",
    "    \n",
    "#     sample_activation = fo.Sample(\n",
    "#         filepath=img_path,\n",
    "#         group=group.element('test'),\n",
    "#         ground_truth=fo.Segmentation(\n",
    "#             mask=mask.squeeze().numpy()\n",
    "#         )\n",
    "#     )\n",
    "    # init group for slice\n",
    "    group = fo.Group()\n",
    "    # predictions, error maps, umaps and foreground dice\n",
    "    DSC = torch.zeros((n_unets))\n",
    "    for j in range(n_unets):\n",
    "        # make sample\n",
    "        #print(img_path)\n",
    "        sample_image = fo.Sample(\n",
    "            filepath=img_path,\n",
    "            group=group.element(f'it_{j}'),\n",
    "            ground_truth=fo.Segmentation(\n",
    "                mask=mask.squeeze().numpy()\n",
    "            )\n",
    "        )\n",
    "        # save additional info to link id to image and all its masks\n",
    "        sample_info = {\n",
    "            'vendor': 'val',\n",
    "            'slice': i,\n",
    "            'unet': j\n",
    "        }\n",
    "        sample_image['sample_info'] = sample_info\n",
    "#         if j == 0:\n",
    "#             sample_image['input'] = img\n",
    "        \n",
    "        # U-Net - Predictions and errormaps\n",
    "        unet_output = unets[j](img.unsqueeze(0))\n",
    "        pred = torch.argmax(unet_output, dim=1, keepdims=True)\n",
    "        err_map = (pred != mask)\n",
    "        sample_image[f'pred_unet'] = fo.Segmentation(mask=pred.squeeze().numpy())\n",
    "        sample_image[f'error_unet'] = fo.Segmentation(mask=err_map.squeeze().numpy())\n",
    "        # umaps\n",
    "        ## entropy\n",
    "        umap_entropy = umap_generator_entropy(unet_output)\n",
    "        sample_image[f'umap_entropy'] = fo.Heatmap(map=umap_entropy.squeeze().numpy())\n",
    "        ## segmentation distortion - predictions with AE and UQ map\n",
    "        model_output  = models[j](img.unsqueeze(0))\n",
    "        model_pred    = torch.argmax(model_output[1], dim=0, keepdims=False)\n",
    "        model_err_map = (model_pred != mask.squeeze())\n",
    "        umap_ae = umap_generator_AE(model_output)\n",
    "        sample_image[f'pred_ae']  = fo.Segmentation(mask=model_pred.squeeze().numpy())\n",
    "        sample_image[f'error_ae'] = fo.Segmentation(mask=model_err_map.squeeze().numpy())\n",
    "        sample_image[f'umap_ae']  = fo.Heatmap(map=umap_ae.squeeze().numpy())\n",
    "        \n",
    "        # dice scores\n",
    "        sample_image['unet_dsc'] = dcs(pred.squeeze(), mask.squeeze().int()).numpy()\n",
    "        sample_image['model_dsc'] = dcs(model_pred.squeeze(), mask.squeeze().int()).numpy()\n",
    "        DSC[j] = dcs(pred.squeeze(), mask.squeeze().int())\n",
    "        \n",
    "        # add sample to sample list\n",
    "        samples += [sample_image]\n",
    "        \n",
    "      \n",
    "    \n",
    "    # get dice related tags per sample\n",
    "    DSC = DSC.mean()\n",
    "    thresholds = [0.3, 0.5, 0.8]\n",
    "    for threshold in thresholds:\n",
    "        if DSC < threshold:\n",
    "            sample_image.tags.append(f'DSC_below_{int((threshold * 100))}')\n",
    "        \n",
    "#     # add sample to sample list\n",
    "#     samples += [sample_image, sample_activation]\n",
    "    \n",
    "# add samples to dataset\n",
    "dataset.add_samples(samples)\n",
    "print()\n",
    "\n",
    "# # validate dataset\n",
    "# print(dataset)\n",
    "\n",
    "# # export dataset\n",
    "# export_dir = os.path.expanduser('~') + '/fiftyone/test-dataset'\n",
    "# dataset.export(export_dir=export_dir, dataset_type=fo.types.FiftyOneDataset)\n",
    "\n",
    "# # clean up. remove tmp\n",
    "# shutil.rmtree('tmp')\n",
    "# #dataset.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc4de2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree('tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7f745c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f8e72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce3ce11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_dir = \"/home/lennartz/fiftyone/test-dataset\"\n",
    "#dataset_type = fo.types.FiftyOneDataset\n",
    "#dataset_show = fo.Dataset.from_dir(dataset_dir, dataset_type=dataset_type)\n",
    "\n",
    "session = fo.launch_app(dataset, port=8100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f861310",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fo.close_app()\n",
    "#session.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd0f339",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset['651e795cf009e13d026dd8ec']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa6bc6e",
   "metadata": {},
   "source": [
    "# Visualizing the effect of our models on activations from certain images, specified by ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "620ef857",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load model\n",
    "def load_model(post, residual, i):\n",
    "#     post = 'localAug_multiImgSingleView_res_balanced_same'\n",
    "    disabled_ids = ['shortcut0', 'shortcut1', 'shortcut2']\n",
    "#     models = []\n",
    "    DAEs = nn.ModuleDict({'up3': AugResDAE(in_channels = 64, \n",
    "                                           in_dim      = 32,\n",
    "                                           latent_dim  = 256,\n",
    "                                           depth       = 3,\n",
    "                                           block_size  = 4,\n",
    "                                           residual    = residual)})\n",
    "\n",
    "    for layer_id in disabled_ids:\n",
    "        DAEs[layer_id] = nn.Identity()\n",
    "\n",
    "    model = ModelAdapter(seg_model=unet,\n",
    "                         transformations=DAEs,\n",
    "                         disabled_ids=disabled_ids,\n",
    "                         copy=True)\n",
    "    model_path = f'../../pre-trained-tmp/trained_AEs/acdc_AugResDAE{i}_{post}_best.pt'\n",
    "    #model_path = f'{ROOT}pre-trained-tmp/trained_AEs/{pre}_resDAE{i}_{post}_best.pt'\n",
    "    #model_path = f'{ROOT}pre-trained-tmp/trained_AEs/acdc_epinet_CE-only_prior-1_best.pt'localAug_multiImgSingleView_res\n",
    "    #model_path = f'{ROOT}pre-trained-tmp/trained_AEs/acdc_resDAE0_venus_best.pt'\n",
    "    state_dict = torch.load(model_path)['model_state_dict']\n",
    "    model.load_state_dict(state_dict)\n",
    "    # Remove trainiung hooks, add evaluation hooks\n",
    "    model.remove_all_hooks()        \n",
    "    model.hook_inspect_transformation(model.transformations)\n",
    "    # Put model in evaluation state\n",
    "    model.eval()\n",
    "    model.freeze_seg_model()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "### test\n",
    "model = load_model('localAug_multiImgSingleView_res_balanced_same', True, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c14e1129",
   "metadata": {},
   "outputs": [],
   "source": [
    "### handle data from dataset before forward pass (i.e. extract correct slice and augment)\n",
    "from nnunet.training.network_training.nnUNetTrainerV2 import nnUNetTrainerV2\n",
    "import batchgenerators\n",
    "from batchgenerators.transforms.local_transforms import *\n",
    "from batchgenerators.dataloading.single_threaded_augmenter import SingleThreadedAugmenter\n",
    "### get augmentor from training\n",
    "def get_local_augmentor_from_nnUnetplan():\n",
    "    nnUnet_prefix = '../../../nnUNet/'\n",
    "    pkl_file          = nnUnet_prefix + 'data/nnUNet_preprocessed/Task500_ACDC/nnUNetPlansv2.1_plans_2D.pkl'\n",
    "    fold              = 0\n",
    "    output_folder     = nnUnet_prefix + 'results/nnUnet/nnUNet/2d/Task027_ACDC/nnUNetTrainerV2__nnUNetPlansv2.1/'\n",
    "    dataset_directory = nnUnet_prefix + 'data/nnUNet_preprocessed/Task500_ACDC'\n",
    "\n",
    "    trainer = nnUNetTrainerV2(pkl_file, 0, output_folder, dataset_directory)\n",
    "    trainer.initialize()\n",
    "\n",
    "    train_loader = trainer.tr_gen\n",
    "\n",
    "    original_transforms = (\n",
    "        batchgenerators.transforms.resample_transforms.SimulateLowResolutionTransform,\n",
    "        batchgenerators.transforms.noise_transforms.GaussianNoiseTransform,\n",
    "        batchgenerators.transforms.utility_transforms.RemoveLabelTransform,\n",
    "        batchgenerators.transforms.utility_transforms.RenameTransform,\n",
    "        batchgenerators.transforms.utility_transforms.NumpyToTensor\n",
    "    )\n",
    "\n",
    "    scale = 200.\n",
    "    local_transforms = [\n",
    "        BrightnessGradientAdditiveTransform(scale=scale, max_strength=4, p_per_sample=0.2, p_per_channel=1),\n",
    "        LocalGammaTransform(scale=scale, gamma=(2, 5), p_per_sample=0.2, p_per_channel=1),\n",
    "        LocalSmoothingTransform(scale=scale, smoothing_strength=(0.5, 1), p_per_sample=0.2, p_per_channel=1),\n",
    "        LocalContrastTransform(scale=scale, new_contrast=(1, 3), p_per_sample=0.2, p_per_channel=1),\n",
    "    ]\n",
    "\n",
    "    transforms = local_transforms + [t for t in train_loader.transform.transforms if isinstance(t, original_transforms)]\n",
    "    augmentor = batchgenerators.transforms.abstract_transforms.Compose(transforms)\n",
    "    \n",
    "    return augmentor\n",
    "\n",
    "### built latent vis dataloader from SingleImageMultiView\n",
    "\n",
    "class SingleImageMultiViewDataLoader(batchgenerators.dataloading.data_loader.SlimDataLoaderBase):\n",
    "    def __init__(self, data: dict, batch_size: int):\n",
    "        super(SingleImageMultiViewDataLoader, self).__init__(data, batch_size)\n",
    "        # data is now stored in self._data.\n",
    "    \n",
    "    def generate_train_batch(self):\n",
    "        #data = self._data[randrange(len(self._data))]\n",
    "        img = self._data.numpy().astype(np.float32)\n",
    "        #tar = np.zeros_like(img, dtype=np.float32)\n",
    "        \n",
    "        img_batched = np.tile(img, (self.batch_size, 1, 1, 1))\n",
    "        tar_batched = np.zeros_like(img_batched, dtype=np.float32)\n",
    "        # now construct the dictionary and return it. np.float32 cast because most networks take float\n",
    "        out = {'data': img_batched, \n",
    "               'seg':  tar_batched}\n",
    "        \n",
    "        # if the original data is also needed, activate this flag to store it where augmentations\n",
    "        # cant find it.\n",
    "\n",
    "        out['data_orig']   = self._data.unsqueeze(0)\n",
    "        #out['target_orig'] = data['target'].unsqueeze(0)\n",
    "        \n",
    "        return out\n",
    "\n",
    "def get_dataloader_for_dataset_and_id(dataset, sample_id, batch_size=1):\n",
    "    #sample_id = '65313dd6718acf6628df3bc4'\n",
    "    sample = dataset[sample_id]\n",
    "    # sample information\n",
    "    sample_info = sample['sample_info']\n",
    "    # data sample\n",
    "    img_path = sample['filepath'].replace('.png', '.pt')\n",
    "    x_in = torch.load(img_path)\n",
    "\n",
    "    dataloader = SingleImageMultiViewDataLoader(x_in, batch_size)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "### test\n",
    "# augmentor  = get_local_augmentor_from_nnUnetplan()\n",
    "# dataloader = get_dataloader_for_dataset_and_id(dataset_show, '65313dd6718acf6628df3bc4', batch_size=1)\n",
    "# generator  = SingleThreadedAugmenter(dataloader, augmentor)\n",
    "# batch      = next(generator)\n",
    "# print(batch['data'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8da299af",
   "metadata": {},
   "outputs": [],
   "source": [
    "### get activations\n",
    "def get_activations(\n",
    "    dataset, \n",
    "    sample_id, \n",
    "    model='localAug_multiImgSingleView_res_balanced_same', \n",
    "    residual=True, \n",
    "    iteration=0\n",
    "): \n",
    "\n",
    "    model = load_model(model, residual, iteration)\n",
    "\n",
    "    augmentor  = get_local_augmentor_from_nnUnetplan()\n",
    "    dataloader = get_dataloader_for_dataset_and_id(dataset, sample_id, batch_size=1)\n",
    "    generator  = SingleThreadedAugmenter(dataloader, augmentor)\n",
    "    batch      = next(generator)\n",
    "\n",
    "    tmp = model(batch['data'])\n",
    "    activations_augmented = model.inspect_data['up3']\n",
    "    original_act_augmented = activations_augmented['input']\n",
    "    denoised_act_augmented = activations_augmented['denoised']\n",
    "\n",
    "    tmp = model(batch['data_orig'])\n",
    "    activations = model.inspect_data['up3']\n",
    "    original_act = activations['input']\n",
    "    denoised_act = activations['denoised']\n",
    "\n",
    "    # original_act_augmented, original_act, dist(original_act, denoised_act), dist(original_act, denoised_act_augmented)\n",
    "    recon_residual_original = (original_act - denoised_act) ** 2\n",
    "    recon_residual_original_augmented = (original_act - denoised_act_augmented) ** 2\n",
    "    change_act = (original_act - original_act_augmented) ** 2\n",
    "    residual_original   = activations['residuals']\n",
    "    residual_original_augmented = activations_augmented['residuals']\n",
    "    \n",
    "    \n",
    "    data = torch.cat([original_act, \n",
    "                      original_act_augmented, \n",
    "                      residual_original, \n",
    "                      residual_original_augmented, \n",
    "                      change_act,\n",
    "                      recon_residual_original, \n",
    "                      recon_residual_original_augmented], dim=0)\n",
    "\n",
    "    scaling_factor = data.max() - data.min()\n",
    "    data -= torch.flatten(data, start_dim=1, end_dim=3).min(1).values.view(-1, 1, 1, 1)\n",
    "    data /= scaling_factor\n",
    "    \n",
    "    data[2:4] *= 10\n",
    "    data[4:] *= 100\n",
    "    data = data.clamp(0,1)\n",
    "    \n",
    "    return data.transpose(0,1)\n",
    "\n",
    "def temp_save_activation_data(data, path, img_names):\n",
    "    for i, channel in enumerate(data):\n",
    "        for name, img in zip(img_names, channel):\n",
    "            save_image(img, path + f'{name}_{i}_test.png')\n",
    "    save_image(torch.ones(data.shape[-2:]), path + f'background.png')\n",
    "## testing\n",
    "#data = get_activations(dataset, '65315041bc3ffd5e948dbabb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1f30c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make dataset for fiftyone\n",
    "try:\n",
    "    act_dataset.delete()\n",
    "except:\n",
    "    print(\"dataset name already available or dataset didnt exist\")\n",
    "# init dataset\n",
    "act_dataset = fo.Dataset(name=\"activation_dataset_test\")\n",
    "# add group and all Groups we need\n",
    "act_dataset.add_group_field(\"group\", default=\"activation\")\n",
    "# make temporary dir for data handling\n",
    "os.makedirs('tmp', exist_ok=True)\n",
    "path = 'tmp/'\n",
    "# init samples\n",
    "samples = []\n",
    "\n",
    "data = get_activations(dataset, '65326e3811ebb0636b055ed3')\n",
    "img_names = [\n",
    "    'activation',\n",
    "    'augmented_activation',\n",
    "    'residual_activation',\n",
    "    'residual_augmented_activation',\n",
    "    'activation_difference',\n",
    "    'reconstruction_residual_activation',\n",
    "    'reconstruction_residual_augmented_activation'\n",
    "]\n",
    "path = 'tmp/'\n",
    "temp_save_activation_data(path, img_names)\n",
    "\n",
    "for i, channel in enumerate(data):\n",
    "    # make group for channel\n",
    "    group = fo.Group()\n",
    "    \n",
    "    for name, img in zip(img_names, channel):\n",
    "        \n",
    "        img_path = path + f'{name}_{i}_test.png'\n",
    "        \n",
    "        sample_image = fo.Sample(\n",
    "            filepath=img_path,\n",
    "            group=group.element(f'{name}'),\n",
    "        )\n",
    "        \n",
    "        samples += [sample_image]\n",
    "        \n",
    "act_dataset.add_samples(samples)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25a63dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.list_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f589f379",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(act_dataset, port=8100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e0055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dataset.delete()\n",
    "except:\n",
    "    print(\"dataset name already available or dataset didnt exist\")\n",
    "    \n",
    "    \n",
    "### Make dataset for fiftyone\n",
    "try:\n",
    "    act_dataset.delete()\n",
    "except:\n",
    "    print(\"dataset name already available or dataset didnt exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da0ee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tmp in [original_act, original_act_augmented, residual_original, residual_original_augmented]:\n",
    "    print(tmp.min(), tmp.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48db1314",
   "metadata": {},
   "source": [
    "# Visualizing the effect of local augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5efa154e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset\n",
      "loading all case properties\n",
      "2023-10-24 15:29:55.485117: Using splits from existing split file: ../../../nnUNet/data/nnUNet_preprocessed/Task500_ACDC/splits_final.pkl\n",
      "2023-10-24 15:29:55.486391: The split file contains 5 splits.\n",
      "2023-10-24 15:29:55.486510: Desired fold for training: 0\n",
      "2023-10-24 15:29:55.487473: This split has 160 training and 40 validation cases.\n",
      "unpacking dataset\n",
      "done\n",
      "loading dataset\n",
      "loading all case properties\n"
     ]
    }
   ],
   "source": [
    "## Initialize trainer to get data loaders with data augmentations from training\n",
    "from batchgenerators.dataloading.multi_threaded_augmenter import MultiThreadedAugmenter\n",
    "from augment import MultiImageSingleViewDataLoader\n",
    "\n",
    "nnUnet_prefix = '../../../nnUNet/'\n",
    "pkl_file          = nnUnet_prefix + 'data/nnUNet_preprocessed/Task500_ACDC/nnUNetPlansv2.1_plans_2D.pkl'\n",
    "fold              = 0\n",
    "output_folder     = nnUnet_prefix + 'results/nnUnet/nnUNet/2d/Task027_ACDC/nnUNetTrainerV2__nnUNetPlansv2.1/'\n",
    "dataset_directory = nnUnet_prefix + 'data/nnUNet_preprocessed/Task500_ACDC'\n",
    "\n",
    "trainer = nnUNetTrainerV2(pkl_file, 0, output_folder, dataset_directory)\n",
    "trainer.initialize()\n",
    "\n",
    "train_loader = trainer.tr_gen\n",
    "\n",
    "\n",
    "from batchgenerators.transforms.local_transforms import *\n",
    "from batchgenerators.dataloading.single_threaded_augmenter import SingleThreadedAugmenter\n",
    "\n",
    "original_transforms = (\n",
    "    batchgenerators.transforms.resample_transforms.SimulateLowResolutionTransform,\n",
    "    batchgenerators.transforms.noise_transforms.GaussianNoiseTransform,\n",
    "    batchgenerators.transforms.utility_transforms.RemoveLabelTransform,\n",
    "    batchgenerators.transforms.utility_transforms.RenameTransform,\n",
    "    batchgenerators.transforms.utility_transforms.NumpyToTensor\n",
    ")\n",
    "\n",
    "scale = 200.\n",
    "local_transforms = [\n",
    "    BrightnessGradientAdditiveTransform(scale=scale, max_strength=4, p_per_sample=0.2, p_per_channel=1),\n",
    "    LocalGammaTransform(scale=scale, gamma=(2, 5), p_per_sample=0.2, p_per_channel=1),\n",
    "    LocalSmoothingTransform(scale=scale, smoothing_strength=(0.5, 1), p_per_sample=0.2, p_per_channel=1),\n",
    "    LocalContrastTransform(scale=scale, new_contrast=(1, 3), p_per_sample=0.2, p_per_channel=1),\n",
    "]\n",
    "\n",
    "train_transforms = local_transforms + [t for t in train_loader.transform.transforms if isinstance(t, original_transforms)]\n",
    "\n",
    "train_augmentor = batchgenerators.transforms.abstract_transforms.Compose(train_transforms)\n",
    "### - Load dataset and init batch generator\n",
    "train_data = ACDCDataset(data='train', debug=False)\n",
    "train_gen = MultiImageSingleViewDataLoader(train_data, batch_size=1, return_orig=True)\n",
    "train_gen = MultiThreadedAugmenter(train_gen, train_augmentor, 1, 1, seeds=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5a79cd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.delete_non_persistent_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "deb0d9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 100/100 [1.6s elapsed, 0s remaining, 63.6 samples/s]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:eta.core.utils: 100% |█████████████████| 100/100 [1.6s elapsed, 0s remaining, 63.6 samples/s]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# what we need\n",
    "# model (Unet, single is alright for this)\n",
    "# train data loader from DAEs. Half original, half augmented\n",
    "# visualize segmentation, ground by original and augmented.\n",
    "# Maybe include augmentation pipeline here to be able to change it if necessary (e.g. increase magnitude)\n",
    "\n",
    "# first fiftyone stuff\n",
    "\n",
    "try:\n",
    "    local_aug_dataset.delete()\n",
    "except:\n",
    "    print(\"local_aug_dataset name already available or dataset didnt exist\")\n",
    "\n",
    "\n",
    "# build dataset\n",
    "samples = []\n",
    "n_unets = 1\n",
    "export_dir = os.path.expanduser('~') + '/fiftyone/test-dataset'\n",
    "# init dataset\n",
    "local_aug_dataset = fo.Dataset(name=\"local_aug_0\")\n",
    "# add group and all Groups we need\n",
    "local_aug_dataset.add_group_field(\"group\", default=\"original\")\n",
    "\n",
    "# set mask targets\n",
    "## ground truth targets\n",
    "local_aug_dataset.mask_targets = {\n",
    "    \"ground_truth\": {0: \"background\",\n",
    "                     1: \"LV\",\n",
    "                     2: \"MYO\",\n",
    "                     3: \"RV\"}\n",
    "}\n",
    "\n",
    "\n",
    "# make temporary dir for data handling\n",
    "os.makedirs('tmp', exist_ok=True)\n",
    "path = 'tmp/'\n",
    "\n",
    "\n",
    "# itertatively make samples\n",
    "for i in range(0, 50):\n",
    "    # get data\n",
    "    data = next(train_gen)\n",
    "    img_original = data['data_orig']\n",
    "    img_augmented = data['data']\n",
    "    mask = data['target_orig']\n",
    "    mask[mask < 0] = 0\n",
    "    \n",
    "    # save images to disk\n",
    "    img_path  = path + f'img_original_{i}.png'\n",
    "    img_norm  = img_original - img_original.min()\n",
    "    img_norm /= img_norm.max()\n",
    "    save_image(img_norm, img_path)\n",
    "    torch.save(img_original, path + f'img_original_{i}.pt')\n",
    "    \n",
    "    img_augmented_path  = path + f'img_augmented_{i}.png'\n",
    "    img_augmented_norm  = img_augmented - img_augmented.min()\n",
    "    img_augmented_norm /= img_augmented_norm.max()\n",
    "    save_image(img_augmented_norm, img_augmented_path)\n",
    "    torch.save(img_augmented, path + f'img_augmented_{i}.pt')\n",
    "    \n",
    "    # init group for slice\n",
    "    group = fo.Group()\n",
    "    \n",
    "    sample_original = fo.Sample(\n",
    "        filepath=img_path,\n",
    "        group=group.element(f'original'),\n",
    "        ground_truth=fo.Segmentation(\n",
    "            mask=mask.squeeze().numpy()\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    sample_augmented = fo.Sample(\n",
    "        filepath=img_augmented_path,\n",
    "        group=group.element(f'augmented'),\n",
    "        ground_truth=fo.Segmentation(\n",
    "            mask=mask.squeeze().numpy()\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # UNet predictions\n",
    "    input_batch = torch.cat([data['data_orig'], data['data']], dim=0)\n",
    "    unet_output = unets[0](input_batch)\n",
    "    pred = torch.argmax(unet_output, dim=1, keepdims=True)\n",
    "    err_map = (pred != mask)\n",
    "    sample_original[f'pred_unet'] = fo.Segmentation(mask=pred[0].squeeze().numpy())\n",
    "    sample_augmented[f'pred_unet'] = fo.Segmentation(mask=pred[1].squeeze().numpy())\n",
    "    \n",
    "    # UNet predictions with feature resampling\n",
    "    res_model_output_original       = models[0](data['data_orig'])\n",
    "    res_model_pred_original         = torch.argmax(res_model_output_original[1], dim=0, keepdims=False)\n",
    "    sample_original[f'res_pred_ae'] = fo.Segmentation(mask=res_model_pred_original.squeeze().numpy())\n",
    "    \n",
    "    res_model_output_augmented       = models[0](data['data'])\n",
    "    res_model_pred_augmented         = torch.argmax(res_model_output_augmented[1], dim=0, keepdims=False)\n",
    "    sample_augmented[f'res_pred_ae'] = fo.Segmentation(mask=res_model_pred_augmented.squeeze().numpy())\n",
    "    \n",
    "    rec_model_output_original       = models[1](data['data_orig'])\n",
    "    rec_model_pred_original         = torch.argmax(rec_model_output_original[1], dim=0, keepdims=False)\n",
    "    sample_original[f'rec_pred_ae'] = fo.Segmentation(mask=rec_model_pred_original.squeeze().numpy())\n",
    "    \n",
    "    rec_model_output_augmented       = models[1](data['data'])\n",
    "    rec_model_pred_augmented         = torch.argmax(rec_model_output_augmented[1], dim=0, keepdims=False)\n",
    "    sample_augmented[f'rec_pred_ae'] = fo.Segmentation(mask=rec_model_pred_augmented.squeeze().numpy())\n",
    "    # add samples to sample list\n",
    "    samples += [sample_original, sample_augmented]\n",
    "    \n",
    "    \n",
    "# add samples to dataset\n",
    "local_aug_dataset.add_samples(samples)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f3c81c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:8100/?notebook=True&subscription=74871b9d-3dcd-433f-87b5-e82be8887049\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fed94df2760>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "session = fo.launch_app(local_aug_dataset, port=8100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b0f7bc",
   "metadata": {},
   "source": [
    "# visualize activations for a pair of augmented / non augmented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "830e118c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_dataset_and_ids(dataset, sample_ids):\n",
    "    #sample_id = '65313dd6718acf6628df3bc4'\n",
    "    \n",
    "    samples_out = []\n",
    "    \n",
    "    for sample_id in sample_ids:\n",
    "        sample = dataset[sample_id]\n",
    "        img_path = sample['filepath'].replace('.png', '.pt')\n",
    "        samples_out.append(torch.load(img_path))\n",
    "        \n",
    "    return samples_out\n",
    "\n",
    "\n",
    "# def get_data_from_dataset_and_ids(dataset, sample_id, batch_size=1):\n",
    "#     sample = dataset[sample_id]\n",
    "#     img_path = sample['filepath'].replace('.png', '.pt')\n",
    "#     return torch.load(img_path)\n",
    "        \n",
    "\n",
    "### get activations\n",
    "def get_activations_without_additional_augmentations(\n",
    "    dataset, \n",
    "    sample_ids, \n",
    "    model='localAug_multiImgSingleView_res_balanced_same', \n",
    "    residual=True, \n",
    "    iteration=0\n",
    "): \n",
    "\n",
    "    model = load_model(model, residual, iteration)\n",
    "    samples = get_data_from_dataset_and_ids(dataset, sample_ids)\n",
    "    print(samples[0].shape)\n",
    "\n",
    "    tmp = model(samples[1])\n",
    "    activations_augmented = model.inspect_data['up3']\n",
    "    original_act_augmented = activations_augmented['input']\n",
    "    denoised_act_augmented = activations_augmented['denoised']\n",
    "\n",
    "    tmp = model(samples[0])\n",
    "    activations = model.inspect_data['up3']\n",
    "    original_act = activations['input']\n",
    "    denoised_act = activations['denoised']\n",
    "\n",
    "    # original_act_augmented, original_act, dist(original_act, denoised_act), dist(original_act, denoised_act_augmented)\n",
    "    recon_residual_original = (original_act - denoised_act)\n",
    "    recon_residual_original_augmented = (original_act - denoised_act_augmented)\n",
    "    change_act = (original_act - original_act_augmented)\n",
    "    residual_original = activations['residuals']\n",
    "    residual_original_augmented = activations_augmented['residuals']\n",
    "    \n",
    "    \n",
    "    data = torch.cat([original_act, \n",
    "                      original_act_augmented,\n",
    "                      change_act,\n",
    "                      residual_original, \n",
    "                      residual_original_augmented, \n",
    "                      #recon_residual_original, \n",
    "                      recon_residual_original_augmented], dim=0)\n",
    "    \n",
    "    for t in data:\n",
    "        print(t.shape, t.min(), t.max())\n",
    "    \n",
    "    scaling_factor = data.max() - data.min()\n",
    "    #data -= torch.flatten(data, start_dim=1, end_dim=3).min(1).values.view(-1, 1, 1, 1)\n",
    "    #data /= scaling_factor\n",
    "    \n",
    "    #data[2:4] *= 10\n",
    "    #data[4:] *= 100\n",
    "    data = data.clamp(-10,10)\n",
    "    \n",
    "    return data.transpose(0,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e298b3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 256, 256])\n",
      "torch.Size([64, 32, 32]) tensor(-6.6202) tensor(10.5962)\n",
      "torch.Size([64, 32, 32]) tensor(-6.2948) tensor(9.2680)\n",
      "torch.Size([64, 32, 32]) tensor(-5.4037) tensor(6.6280)\n",
      "torch.Size([64, 32, 32]) tensor(-0.2675) tensor(0.3243)\n",
      "torch.Size([64, 32, 32]) tensor(-4.7259) tensor(6.6208)\n",
      "torch.Size([64, 32, 32]) tensor(-3.6312) tensor(4.2636)\n",
      "torch.Size([1, 1, 256, 256])\n",
      "torch.Size([64, 32, 32]) tensor(-6.6202) tensor(10.5962)\n",
      "torch.Size([64, 32, 32]) tensor(-6.2948) tensor(9.2680)\n",
      "torch.Size([64, 32, 32]) tensor(-5.4037) tensor(6.6280)\n",
      "torch.Size([64, 32, 32]) tensor(-1.0151) tensor(0.7926)\n",
      "torch.Size([64, 32, 32]) tensor(-4.7242) tensor(5.6957)\n",
      "torch.Size([64, 32, 32]) tensor(-3.5219) tensor(3.7527)\n",
      " 100% |█████████████████| 384/384 [1.2s elapsed, 0s remaining, 322.2 samples/s]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:eta.core.utils: 100% |█████████████████| 384/384 [1.2s elapsed, 0s remaining, 322.2 samples/s]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Make dataset for fiftyone\n",
    "try:\n",
    "    act_dataset2.delete()\n",
    "except:\n",
    "    print(\"dataset name already available or dataset didnt exist\")\n",
    "# init dataset\n",
    "act_dataset2 = fo.Dataset(name=\"activation_dataset_test\")\n",
    "# add group and all Groups we need\n",
    "act_dataset2.add_group_field(\"group\", default=\"activation\")\n",
    "# make temporary dir for data handling\n",
    "os.makedirs('tmp', exist_ok=True)\n",
    "path = 'tmp/'\n",
    "# init samples\n",
    "samples = []\n",
    "\n",
    "ids = [\n",
    "    '6538e8c4324eda9bed423a06',\n",
    "    '6538e8c4324eda9bed423a07'\n",
    "]\n",
    "\n",
    "data_res = get_activations_without_additional_augmentations(\n",
    "    local_aug_dataset, ids, \n",
    "    model='localAug_multiImgSingleView_res_balanced_same', \n",
    "    residual=True, iteration=0)\n",
    "data_rec = get_activations_without_additional_augmentations(\n",
    "    local_aug_dataset, ids, \n",
    "    model='localAug_multiImgSingleView_recon_balanced_same', \n",
    "    residual=False, iteration=0)\n",
    "data = torch.stack([data_res, data_rec], dim=2)\n",
    "\n",
    "img_names = [\n",
    "    'activation',\n",
    "    'augmented_activation',\n",
    "    'activation_difference',\n",
    "    'residual_activation',\n",
    "    'residual_augmented_activation',\n",
    "    #'reconstruction_residual_activation',\n",
    "    'reconstruction_residual_augmented_activation'\n",
    "]\n",
    "path = 'tmp/'\n",
    "temp_save_activation_data(data, path, img_names)\n",
    "path_background = path + 'background.png'\n",
    "_max = 0\n",
    "_min = 0\n",
    "for i, channel in enumerate(data):\n",
    "    # make group for channel\n",
    "    group = fo.Group()\n",
    "    \n",
    "    for name, img in zip(img_names, channel):\n",
    "        \n",
    "        img_path = path + f'{name}_{i}_test.png'\n",
    "        \n",
    "        sample_image = fo.Sample(\n",
    "            filepath=path_background,\n",
    "            group=group.element(f'{name}'),\n",
    "        )\n",
    "        \n",
    "        sample_image['rec_heatmap_pos'] = fo.Heatmap(\n",
    "            map=img[0].numpy(),\n",
    "            #range=[-5, 5]\n",
    "        )\n",
    "        \n",
    "        sample_image['rec_heatmap_neg'] = fo.Heatmap(\n",
    "            map=-img[0].numpy(),\n",
    "            #range=[-5, 5]\n",
    "        )\n",
    "        \n",
    "        sample_image['res_heatmap_pos'] = fo.Heatmap(\n",
    "            map=img[1].numpy(),\n",
    "            #range=[-5, 5]\n",
    "        )\n",
    "        \n",
    "        sample_image['res_heatmap_neg'] = fo.Heatmap(\n",
    "            map=-img[1].numpy(),\n",
    "            #range=[-5, 5]\n",
    "        )\n",
    "        \n",
    "        samples += [sample_image]\n",
    "        _max = max(_max, img.max())\n",
    "        _min = min(_min, img.min())\n",
    "\n",
    "        \n",
    "act_dataset2.add_samples(samples)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "73ca4de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:8099/?notebook=True&subscription=a7b3ea71-3699-453c-9da0-ad965f8c2f0a\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fef780daa60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "session = fo.launch_app(act_dataset2, port=8099)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7de6a502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Could not connect session, trying again in 10 seconds\n",
      "\n",
      "\n",
      "Could not connect session, trying again in 10 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fo.close_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "56b0c8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████████| 50/50 [1.7s elapsed, 0s remaining, 30.2 samples/s]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:eta.core.utils: 100% |███████████████████| 50/50 [1.7s elapsed, 0s remaining, 30.2 samples/s]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# what we need\n",
    "# model (Unet, single is alright for this)\n",
    "# train data loader from DAEs. Half original, half augmented\n",
    "# visualize segmentation, ground by original and augmented.\n",
    "# Maybe include augmentation pipeline here to be able to change it if necessary (e.g. increase magnitude)\n",
    "\n",
    "# first fiftyone stuff\n",
    "\n",
    "try:\n",
    "    test_dataset.delete()\n",
    "except:\n",
    "    print(\"test_dataset name already available or dataset didnt exist\")\n",
    "\n",
    "\n",
    "# build dataset\n",
    "samples = []\n",
    "n_unets = 1\n",
    "export_dir = os.path.expanduser('~') + '/fiftyone/test-dataset'\n",
    "# init dataset\n",
    "test_dataset = fo.Dataset(name=\"test_set_A\")\n",
    "# add group and all Groups we need\n",
    "test_dataset.add_group_field(\"group\", default=\"original\")\n",
    "\n",
    "# set mask targets\n",
    "## ground truth targets\n",
    "test_dataset.mask_targets = {\n",
    "    \"ground_truth\": {0: \"background\",\n",
    "                     1: \"LV\",\n",
    "                     2: \"MYO\",\n",
    "                     3: \"RV\"}\n",
    "}\n",
    "\n",
    "\n",
    "# make temporary dir for data handling\n",
    "os.makedirs('tmp', exist_ok=True)\n",
    "path = 'tmp/'\n",
    "\n",
    "# init \n",
    "umap_generator_AE = UMapGenerator(method='ae', net_out='mms')\n",
    "\n",
    "# itertatively make samples\n",
    "for i in range(0, 50):\n",
    "    # get data\n",
    "    data = mnm_a[i * 10]\n",
    "    img_original = data['input'].unsqueeze(0)\n",
    "#     img_augmented = data['data']\n",
    "    mask = data['target'].unsqueeze(0)\n",
    "    mask[mask < 0] = 0\n",
    "    \n",
    "    # save images to disk\n",
    "    img_path  = path + f'img_testA_{i}.png'\n",
    "    img_norm  = img_original - img_original.min()\n",
    "    img_norm /= img_norm.max()\n",
    "    save_image(img_norm, img_path)\n",
    "    torch.save(img_original, path + f'img_testA_{i}.pt')\n",
    "    \n",
    "#     img_augmented_path  = path + f'img_augmented_{i}.png'\n",
    "#     img_augmented_norm  = img_augmented - img_augmented.min()\n",
    "#     img_augmented_norm /= img_augmented_norm.max()\n",
    "#     save_image(img_augmented_norm, img_augmented_path)\n",
    "#     torch.save(img_augmented, path + f'img_augmented_{i}.pt')\n",
    "    \n",
    "    # init group for slice\n",
    "    group = fo.Group()\n",
    "    \n",
    "    sample_original = fo.Sample(\n",
    "        filepath=img_path,\n",
    "        group=group.element(f'original'),\n",
    "        ground_truth=fo.Segmentation(\n",
    "            mask=mask.squeeze().numpy()\n",
    "        )\n",
    "    )\n",
    "\n",
    "#     sample_augmented = fo.Sample(\n",
    "#         filepath=img_augmented_path,\n",
    "#         group=group.element(f'augmented'),\n",
    "#         ground_truth=fo.Segmentation(\n",
    "#             mask=mask.squeeze().numpy()\n",
    "#         )\n",
    "#     )\n",
    "    \n",
    "    # UNet predictions\n",
    "#     input_batch = torch.cat([data['data_orig'], data['data']], dim=0)\n",
    "    unet_output = unets[1](img_original)\n",
    "    pred = torch.argmax(unet_output, dim=1, keepdims=True)\n",
    "    err_map = (pred != mask)\n",
    "    sample_original[f'pred_unet'] = fo.Segmentation(mask=pred[0].squeeze().numpy())\n",
    "    sample_original[f'error_unet'] = fo.Segmentation(mask=err_map.squeeze().numpy())\n",
    "#     sample_augmented[f'pred_unet'] = fo.Segmentation(mask=pred[1].squeeze().numpy())\n",
    "    \n",
    "    # UNet predictions with feature resampling\n",
    "    res_model_output_original       = models[0](img_original)\n",
    "    res_model_pred_original         = torch.argmax(res_model_output_original[1], dim=0, keepdims=False)\n",
    "    sample_original[f'res_pred_ae'] = fo.Segmentation(mask=res_model_pred_original.squeeze().numpy())\n",
    "    res_model_umap                  = umap_generator_AE(res_model_output_original)\n",
    "    sample_original[f'res_umap_ae']    = fo.Heatmap(map=res_model_umap.squeeze().numpy())\n",
    "    \n",
    "#     res_model_output_augmented       = models[0](data['data'])\n",
    "#     res_model_pred_augmented         = torch.argmax(res_model_output_augmented[1], dim=0, keepdims=False)\n",
    "#     sample_augmented[f'res_pred_ae'] = fo.Segmentation(mask=res_model_pred_augmented.squeeze().numpy())\n",
    "    \n",
    "    rec_model_output_original       = models[1](img_original)\n",
    "    rec_model_pred_original         = torch.argmax(rec_model_output_original[1], dim=0, keepdims=False)\n",
    "    sample_original[f'rec_pred_ae'] = fo.Segmentation(mask=rec_model_pred_original.squeeze().numpy())\n",
    "    rec_model_umap                  = umap_generator_AE(rec_model_output_original)\n",
    "    sample_original[f'rec_umap_ae']    = fo.Heatmap(map=rec_model_umap.squeeze().numpy())\n",
    "#     rec_model_output_augmented       = models[1](data['data'])\n",
    "#     rec_model_pred_augmented         = torch.argmax(rec_model_output_augmented[1], dim=0, keepdims=False)\n",
    "#     sample_augmented[f'rec_pred_ae'] = fo.Segmentation(mask=rec_model_pred_augmented.squeeze().numpy())\n",
    "    # add samples to sample list\n",
    "#     samples += [sample_original, sample_augmented]\n",
    "    samples += [sample_original]\n",
    "    \n",
    "# add samples to dataset\n",
    "test_dataset.add_samples(samples)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "295999e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.delete_non_persistent_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "06cda169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:8098/?notebook=True&subscription=3bac6a00-fd73-4b9f-bf17-530b80a16cd4\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fed50264d00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Could not connect session, trying again in 10 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session = fo.launch_app(test_dataset, port=8098)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f97c95c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 256])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['input'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157ee397",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check input batch dim\n",
    "data = next(train_gen)\n",
    "img = data['data_orig']\n",
    "img_augmented = data['data']\n",
    "mask = data['target_orig']\n",
    "mask[mask < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29bdaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = torch.cat([data['data_orig'], data['data']], dim=0)\n",
    "unet_output = unets[0](input_batch)\n",
    "pred = torch.argmax(unet_output, dim=1, keepdims=True)\n",
    "err_map = (pred != mask)\n",
    "input_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9453e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac08698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_transforms = (\n",
    "#     batchgenerators.transforms.resample_transforms.SimulateLowResolutionTransform,\n",
    "#     batchgenerators.transforms.noise_transforms.GaussianNoiseTransform,\n",
    "#     batchgenerators.transforms.utility_transforms.RemoveLabelTransform,\n",
    "#     batchgenerators.transforms.utility_transforms.RenameTransform,\n",
    "#     batchgenerators.transforms.utility_transforms.NumpyToTensor\n",
    "# )\n",
    "\n",
    "# scale = 200.\n",
    "# local_transforms = [\n",
    "#     BrightnessGradientAdditiveTransform(scale=scale, max_strength=4, p_per_sample=0.2, p_per_channel=1),\n",
    "#     LocalGammaTransform(scale=scale, gamma=(2, 5), p_per_sample=0.2, p_per_channel=1),\n",
    "#     LocalSmoothingTransform(scale=scale, smoothing_strength=(0.5, 1), p_per_sample=0.2, p_per_channel=1),\n",
    "#     LocalContrastTransform(scale=scale, new_contrast=(1, 3), p_per_sample=0.2, p_per_channel=1),\n",
    "# ]\n",
    "\n",
    "# transforms = local_transforms + [t for t in train_loader.transform.transforms if isinstance(t, original_transforms)]\n",
    "# augmentor = batchgenerators.transforms.abstract_transforms.Compose(train_transforms)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ### - Load dataset and init batch generator\n",
    "# train_data = ACDCDataset(data='train', debug=False)\n",
    "# valid_data = ACDCDataset(data='val', debug=False)\n",
    "\n",
    "# train_gen = MultiImageSingleViewDataLoader(train_data, batch_size=cfg['batch_size'], return_orig=True)\n",
    "# #train_gen = SingleThreadedAugmenter(train_gen, train_augmentor)\n",
    "# train_gen = MultiThreadedAugmenter(train_gen, train_augmentor, 4, 2, seeds=None)\n",
    "# valid_gen = MultiImageSingleViewDataLoader(valid_data, batch_size=cfg['batch_size'], return_orig=True)\n",
    "# #valid_gen = SingleThreadedAugmenter(valid_gen, valid_augmentor)\n",
    "# valid_gen = MultiThreadedAugmenter(valid_gen, valid_augmentor, 4, 2, seeds=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797b68d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import batchgenerators\n",
    "\n",
    "# # define single image dataloader from batchgenerator example here:\n",
    "# # https://github.com/MIC-DKFZ/batchgenerators/blob/master/batchgenerators/examples/example_ipynb.ipynb\n",
    "# class SingleImageMultiViewDataLoader(batchgenerators.dataloading.data_loader.SlimDataLoaderBase):\n",
    "#     def __init__(self, data: ACDCDataset, batch_size: int = 1, return_orig: str = False):\n",
    "#         super(SingleImageMultiViewDataLoader, self).__init__(data, batch_size)\n",
    "#         # data is now stored in self._data.\n",
    "#         self.return_orig = return_orig\n",
    "    \n",
    "#     def generate_train_batch(self):\n",
    "#         #data = self._data[randrange(len(self._data))]\n",
    "#         img = data['input'].numpy().astype(np.float32)\n",
    "#         tar = data['target'][0].numpy().astype(np.float32)\n",
    "        \n",
    "#         #img_batched = np.tile(img, (self.batch_size, 1, 1, 1))\n",
    "#         #tar_batched = np.tile(tar, (self.batch_size, 1, 1, 1))\n",
    "#         # now construct the dictionary and return it. np.float32 cast because most networks take float\n",
    "#         out = {'data': img, \n",
    "#                'seg':  img}\n",
    "        \n",
    "#         # if the original data is also needed, activate this flag to store it where augmentations\n",
    "#         # cant find it.\n",
    "#         if self.return_orig:\n",
    "#             out['data_orig']   = data['input'].unsqueeze(0)\n",
    "#             out['target_orig'] = data['target'].unsqueeze(0)\n",
    "        \n",
    "#         return out\n",
    "\n",
    "# def build_dataloader():\n",
    "#     pass\n",
    "\n",
    "\n",
    "\n",
    "# def get_data_from_id_for_actvis(sample_id, dataset, debug):\n",
    "#     sample = dataset[sample_id]\n",
    "#     # sample information\n",
    "#     sample_info = sample['sample_info']\n",
    "#     # data sample\n",
    "#     img_path = sample['filepath'].replace('.png', '.pt')\n",
    "#     x_in = torch.load(img_path)\n",
    "#     # load models\n",
    "#     model = load_model(\n",
    "#         post = 'localAug_multiImgSingleView_res_balanced_same',\n",
    "#         residual = True,\n",
    "#         i = sample_info['unet']\n",
    "#     )\n",
    "#     # get activations\n",
    "    \n",
    "#     activation = None\n",
    "    \n",
    "    \n",
    "# def build_51dataset_from_data():\n",
    "#     pass\n",
    "    \n",
    "# def start_51_activation_vis():\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2922ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model('localAug_multiImgSingleView_res_balanced_same', True, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e7b9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# post = 'localAug_multiImgSingleView_res_balanced_same'\n",
    "# disabled_ids = ['shortcut0', 'shortcut1', 'shortcut2']\n",
    "# models = []\n",
    "# DAEs = nn.ModuleDict({'up3': AugResDAE(in_channels = 64, \n",
    "#                                     in_dim      = 32,\n",
    "#                                     latent_dim  = 256,\n",
    "#                                     depth       = 3,\n",
    "#                                     block_size  = 4)})\n",
    "\n",
    "\n",
    "# for layer_id in disabled_ids:\n",
    "#     DAEs[layer_id] = nn.Identity()\n",
    "\n",
    "# model = Frankenstein(seg_model=unet,\n",
    "#                      transformations=DAEs,\n",
    "#                      disabled_ids=disabled_ids,\n",
    "#                      copy=True)\n",
    "# model_path = f'{ROOT}pre-trained-tmp/trained_AEs/acdc_AugResDAE{i}_{post}_best.pt'\n",
    "# #model_path = f'{ROOT}pre-trained-tmp/trained_AEs/{pre}_resDAE{i}_{post}_best.pt'\n",
    "# #model_path = f'{ROOT}pre-trained-tmp/trained_AEs/acdc_epinet_CE-only_prior-1_best.pt'localAug_multiImgSingleView_res\n",
    "# #model_path = f'{ROOT}pre-trained-tmp/trained_AEs/acdc_resDAE0_venus_best.pt'\n",
    "# state_dict = torch.load(model_path)['model_state_dict']\n",
    "# model.load_state_dict(state_dict)\n",
    "# # Remove trainiung hooks, add evaluation hooks\n",
    "# model.remove_all_hooks()        \n",
    "# model.hook_inference_transformations(model.transformations,\n",
    "#                            n_samples=1)\n",
    "# # Put model in evaluation state\n",
    "# model.eval()\n",
    "# model.freeze_seg_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3db9435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # try:\n",
    "# #     dataset.delete()\n",
    "# # except:\n",
    "# #     print(\"dataset name already available or dataset didnt exist\")\n",
    "\n",
    "\n",
    "# # build first dataset\n",
    "# # init dataset\n",
    "# dataset = fo.Dataset(name=\"activation_dataset_test\")\n",
    "# # add group and all Groups we need\n",
    "# dataset.add_group_field(\"group\", default=\"it_0\")\n",
    "\n",
    "# # set mask targets\n",
    "# ## ground truth targets\n",
    "# # dataset.mask_targets = {\n",
    "# #     \"ground_truth\": {0: \"background\",\n",
    "# #                      1: \"LV\",\n",
    "# #                      2: \"MYO\",\n",
    "# #                      3: \"RV\"}\n",
    "# # }\n",
    "# # error map labels\n",
    "# # for i in range(n_unets):\n",
    "# #     dataset.mask_targets[f'errormap_it:{i}'] = {1: 'error'}\n",
    "    \n",
    "# # make temporary dir for data handling\n",
    "# os.makedirs('tmp', exist_ok=True)\n",
    "# path = 'tmp/'\n",
    "# # init sample list. We save each sample here and add it to the\n",
    "# # dataset in the end\n",
    "# samples = []\n",
    "# n_unets = 2\n",
    "# # init dice score class\n",
    "# dcs = Dice(num_classes=4, ignore_index=0)\n",
    "# # init umap generator\n",
    "# umap_generator_entropy = UMapGenerator(method='entropy', net_out='mms')\n",
    "# umap_generator_AE = UMapGenerator(method='ae', net_out='mms')\n",
    "\n",
    "# # itertatively make samples\n",
    "# for i in range(10):\n",
    "    # get data\n",
    "#     data = mnm_a[i*10]\n",
    "#     img = data['input']\n",
    "#     mask = data['target']\n",
    "#     mask[mask < 0] = 0\n",
    "    \n",
    "#     # save image to disk\n",
    "#     img_path  = path + f'test_{i}.png'\n",
    "#     img_norm  = img - img.min()\n",
    "#     img_norm /= img_norm.max()\n",
    "\n",
    "#     save_image(img_norm, img_path)\n",
    "\n",
    "    \n",
    "# #     sample_activation = fo.Sample(\n",
    "# #         filepath=img_path,\n",
    "# #         group=group.element('test'),\n",
    "# #         ground_truth=fo.Segmentation(\n",
    "# #             mask=mask.squeeze().numpy()\n",
    "# #         )\n",
    "# #     )\n",
    "#     # init group for slice\n",
    "#     group = fo.Group()\n",
    "#     # predictions, error maps, umaps and foreground dice\n",
    "#     DSC = torch.zeros((n_unets))\n",
    "#     for i in range(n_unets):\n",
    "#         # make sample\n",
    "        \n",
    "#         sample_image = fo.Sample(\n",
    "#             filepath=img_path,\n",
    "#             group=group.element(f'it_{i}'),\n",
    "#             ground_truth=fo.Segmentation(\n",
    "#                 mask=mask.squeeze().numpy()\n",
    "#             )\n",
    "#         )\n",
    "#         # U-Net - Predictions and errormaps\n",
    "#         unet_output = unets[i](img.unsqueeze(0))\n",
    "#         pred = torch.argmax(unet_output, dim=1, keepdims=True)\n",
    "#         err_map = (pred != mask)\n",
    "#         sample_image[f'pred_unet'] = fo.Segmentation(mask=pred.squeeze().numpy())\n",
    "#         sample_image[f'error_unet']  = fo.Segmentation(mask=err_map.squeeze().numpy())\n",
    "#         # umaps\n",
    "#         ## entropy\n",
    "#         umap_entropy = umap_generator_entropy(unet_output)\n",
    "#         sample_image[f'umap_entropy'] = fo.Heatmap(map=umap_entropy.squeeze().numpy())\n",
    "#         ## segmentation distortion - predictions with AE and UQ map\n",
    "#         model_output  = models[i](img.unsqueeze(0))\n",
    "#         model_pred    = torch.argmax(model_output[1], dim=0, keepdims=False)\n",
    "#         model_err_map = (model_pred != mask.squeeze())\n",
    "#         umap_ae = umap_generator_AE(model_output)\n",
    "#         sample_image[f'pred_ae'] = fo.Segmentation(mask=model_pred.squeeze().numpy())\n",
    "#         sample_image[f'error_ae']  = fo.Segmentation(mask=model_err_map.squeeze().numpy())\n",
    "#         sample_image[f'umap_ae'] = fo.Heatmap(map=umap_ae.squeeze().numpy())\n",
    "        \n",
    "#         # add sample to sample list\n",
    "#         samples += [sample_image]\n",
    "        \n",
    "#         # dice score\n",
    "#         DSC[i] = dcs(pred.squeeze(), mask.squeeze().int())        \n",
    "    \n",
    "#     # get dice related tags per sample\n",
    "#     DSC = DSC.mean()\n",
    "#     thresholds = [0.3, 0.5, 0.8]\n",
    "#     for threshold in thresholds:\n",
    "#         if DSC < threshold:\n",
    "#             sample_image.tags.append(f'DSC_below_{int((threshold * 100))}')\n",
    "        \n",
    "# #     # add sample to sample list\n",
    "# #     samples += [sample_image, sample_activation]\n",
    "    \n",
    "# # add samples to dataset\n",
    "# dataset.add_samples(samples)\n",
    "\n",
    "\n",
    "# # validate dataset\n",
    "# print(dataset)\n",
    "\n",
    "# # export dataset\n",
    "# export_dir = os.path.expanduser('~') + '/fiftyone/test-dataset'\n",
    "# dataset.export(export_dir=export_dir, dataset_type=fo.types.FiftyOneDataset)\n",
    "\n",
    "# # clean up. remove tmp\n",
    "# shutil.rmtree('tmp')\n",
    "# #dataset.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17de498d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
