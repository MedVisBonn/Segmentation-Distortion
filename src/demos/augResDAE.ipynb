{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36efec68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Please cite the following paper when using nnUNet:\n",
      "\n",
      "Isensee, F., Jaeger, P.F., Kohl, S.A.A. et al. \"nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation.\" Nat Methods (2020). https://doi.org/10.1038/s41592-020-01008-z\n",
      "\n",
      "\n",
      "If you have questions or suggestions, feel free to open an issue at https://github.com/MIC-DKFZ/nnUNet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "import os, sys\n",
    "import time\n",
    "import numpy as np\n",
    "import collections\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torchvision.transforms import Resize, CenterCrop\n",
    "from typing import Iterable, Dict, Callable, Tuple\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randrange\n",
    "\n",
    "from nnunet.training.model_restore import restore_model\n",
    "import batchgenerators\n",
    "from batchgenerators.dataloading.multi_threaded_augmenter import MultiThreadedAugmenter\n",
    "from batchgenerators.utilities.file_and_folder_operations import *\n",
    "from nnunet.paths import preprocessing_output_dir\n",
    "from nnunet.training.dataloading.dataset_loading import *\n",
    "from nnunet.training.network_training.nnUNetTrainerV2 import nnUNetTrainerV2\n",
    "from nnunet.run.load_pretrained_weights import load_pretrained_weights\n",
    "\n",
    "sys.path.append('..')\n",
    "from dataset import CalgaryCampinasDataset, ACDCDataset, MNMDataset\n",
    "from utils import EarlyStopping, epoch_average, average_metrics\n",
    "from model.dae import AugResDAE\n",
    "from model.unet import UNet2D\n",
    "from model.wrapper import Frankenstein\n",
    "from losses import MNMCriterionAE, SampleDice, UnetDice\n",
    "from trainer.ae_trainer import AETrainerACDC\n",
    "\n",
    "nnUnet_prefix = '../../../nnUNet/'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91e853b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define single image dataloader from batchgenerator example here:\n",
    "# https://github.com/MIC-DKFZ/batchgenerators/blob/master/batchgenerators/examples/example_ipynb.ipynb\n",
    "class SingleImageMultiViewDataLoader(batchgenerators.dataloading.data_loader.SlimDataLoaderBase):\n",
    "    def __init__(self, data: ACDCDataset, batch_size: int = 2, return_orig: str = False):\n",
    "        super(SingleImageMultiViewDataLoader, self).__init__(data, batch_size)\n",
    "        # data is now stored in self._data.\n",
    "        self.return_orig = return_orig\n",
    "    \n",
    "    def generate_train_batch(self):\n",
    "        \n",
    "        data = self._data[randrange(len(self._data))]\n",
    "        img = data['input'].numpy().astype(np.float32)\n",
    "        tar = data['target'][0].numpy().astype(np.float32)\n",
    "        \n",
    "        img_batched = np.tile(img, (self.batch_size, 1, 1, 1))\n",
    "        tar_batched = np.tile(tar, (self.batch_size, 1, 1, 1))\n",
    "        # now construct the dictionary and return it. np.float32 cast because most networks take float\n",
    "        out = {'data': img_batched, \n",
    "               'seg':  tar_batched}\n",
    "        \n",
    "        # if the original data is also needed, activate this flag to store it where augmentations\n",
    "        # cant find it.\n",
    "        if self.return_orig:\n",
    "            out['data_orig']   = data['input'].unsqueeze(0)\n",
    "            out['target_orig'] = data['target'].unsqueeze(0)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0f630fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset\n",
      "loading all case properties\n",
      "2023-08-15 08:54:12.040704: Using splits from existing split file: ../../../nnUNet/data/nnUNet_preprocessed/Task500_ACDC/splits_final.pkl\n",
      "2023-08-15 08:54:12.041838: The split file contains 5 splits.\n",
      "2023-08-15 08:54:12.041953: Desired fold for training: 0\n",
      "2023-08-15 08:54:12.042258: This split has 160 training and 40 validation cases.\n",
      "unpacking dataset\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "cfg = {\n",
    "    'debug': False,\n",
    "    'log': False,\n",
    "    'description': f'acdc_AugResDAE_0_test', #'mms_vae_for_nnUNet_fc3_0_bs50',\n",
    "    'project': 'MICCAI2023-loose_ends',\n",
    "\n",
    "    # Data params\n",
    "    'n': 0,\n",
    "    'root': '../../',\n",
    "    'data_path': 'data/mnm/',\n",
    "    #'train_vendor': 'B',\n",
    "    'unet': f'acdc_unet8_0',\n",
    "    'channel_out': 8,\n",
    "\n",
    "    # Hyperparams\n",
    "    'batch_size': 2,\n",
    "    'lr': 1e-4,\n",
    "    'augment': False,\n",
    "    'difference': True,\n",
    "    'loss': 'huber',  # huber or ce\n",
    "    'target': 'output', #gt or output\n",
    "    'reconstruction': True,\n",
    "    'augmentations': 'all',\n",
    "    'disabled_ids': ['shortcut0', 'shortcut1', 'shortcut2'], #['shortcut0', 'shortcut1', 'shortcut2']\n",
    "}\n",
    "\n",
    "description = cfg['description']\n",
    "root = cfg['root']\n",
    "\n",
    "# Unet\n",
    "unet_path = cfg['unet'] # + str(cfg['n'])\n",
    "unet = UNet2D(n_chans_in=1, n_chans_out=4, n_filters_init=cfg['channel_out']).cuda()\n",
    "model_path = f'{root}pre-trained-tmp/trained_UNets/{unet_path}_best.pt'\n",
    "state_dict = torch.load(model_path)['model_state_dict']\n",
    "unet.load_state_dict(state_dict)\n",
    "\n",
    "### Dataloader\n",
    "## Initialize trainer to get data loaders with data augmentations from training\n",
    "pkl_file          = nnUnet_prefix + 'data/nnUNet_preprocessed/Task500_ACDC/nnUNetPlansv2.1_plans_2D.pkl'\n",
    "fold              = 0\n",
    "output_folder     = nnUnet_prefix + 'results/nnUnet/nnUNet/2d/Task027_ACDC/nnUNetTrainerV2__nnUNetPlansv2.1/'\n",
    "dataset_directory = nnUnet_prefix + 'data/nnUNet_preprocessed/Task500_ACDC'\n",
    "\n",
    "trainer = nnUNetTrainerV2(pkl_file, 0, output_folder, dataset_directory)\n",
    "trainer.initialize()\n",
    "\n",
    "train_loader = trainer.tr_gen\n",
    "valid_loader = trainer.val_gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94cbc11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset\n",
      "loading all case properties\n",
      "loading dataset\n",
      "loading all case properties\n"
     ]
    }
   ],
   "source": [
    "if cfg['augmentations'] == 'all':\n",
    "    train_transforms = [t for t in train_loader.transform.transforms]\n",
    "    valid_transforms = [t for t in valid_loader.transform.transforms]\n",
    "elif cfg['augmentations'] == 'output_invariant':\n",
    "    data_only_transforms = (\n",
    "        batchgenerators.transforms.noise_transforms.GaussianNoiseTransform,\n",
    "        batchgenerators.transforms.noise_transforms.GaussianBlurTransform,\n",
    "        batchgenerators.transforms.color_transforms.BrightnessMultiplicativeTransform,\n",
    "        batchgenerators.transforms.color_transforms.ContrastAugmentationTransform,\n",
    "        batchgenerators.transforms.color_transforms.GammaTransform,\n",
    "        batchgenerators.transforms.utility_transforms.RemoveLabelTransform,\n",
    "        batchgenerators.transforms.utility_transforms.RenameTransform,\n",
    "        batchgenerators.transforms.utility_transforms.NumpyToTensor,\n",
    "        batchgenerators.transforms.utility_transforms.NumpyToTensor)\n",
    "\n",
    "    train_transforms = [t for t in train_loader.transform.transforms if isinstance(t, data_only_transforms)]\n",
    "    valid_transforms = [t for t in valid_loader.transform.transforms if isinstance(t, data_only_transforms)]\n",
    "    \n",
    "train_augmentor = batchgenerators.transforms.abstract_transforms.Compose(train_transforms)\n",
    "valid_augmentor = batchgenerators.transforms.abstract_transforms.Compose(valid_transforms)\n",
    "### - Load dataset and init batch generator\n",
    "train_data = ACDCDataset(data='train', debug=False)\n",
    "valid_data = ACDCDataset(data='val', debug=False)\n",
    "\n",
    "train_gen = SingleImageMultiViewDataLoader(train_data, batch_size=cfg['batch_size'], return_orig=True)\n",
    "train_gen = MultiThreadedAugmenter(train_gen, train_augmentor, 1, 1, seeds=None)\n",
    "valid_gen = SingleImageMultiViewDataLoader(valid_data, batch_size=cfg['batch_size'], return_orig=True)\n",
    "valid_gen = MultiThreadedAugmenter(valid_gen, valid_augmentor, 1, 1, seeds=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fcf2a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from typing import Iterable, Dict, Callable, Tuple, Union\n",
    "from copy import deepcopy\n",
    "\n",
    "class Frankenstein(nn.Module):\n",
    "    \"\"\"Wrapper class for segmentation models and feature transformations.\n",
    "\n",
    "    Wraps (a copy of) the segmentation model and attaches feature\n",
    "    trasformations to it via hooks (at potentially various positions\n",
    "    simultaneously). Additionally, it provides control utilities for the\n",
    "    hooks as well as different types for inference and training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        seg_model: nn.Module,\n",
    "        transformations: nn.ModuleDict,\n",
    "        disabled_ids: list = [],\n",
    "        copy: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.seg_model = deepcopy(seg_model) if copy else seg_model\n",
    "\n",
    "        self.transformations = transformations\n",
    "        self.disabled_ids = disabled_ids\n",
    "        self.transformation_handles = {}\n",
    "        self.train_transformation_handles = {}\n",
    "        self.inspect_transformation_handles = {}\n",
    "        self.training_data = {}\n",
    "        self.inspect_data = {}\n",
    "\n",
    "    def hook_train_transformations(self, transformations: Dict[str, nn.Module]) -> None:\n",
    "        for layer_id in transformations:\n",
    "            layer = self.seg_model.get_submodule(layer_id)\n",
    "            hook = self._get_train_transformation_hook(\n",
    "                transformations[layer_id], layer_id\n",
    "            )\n",
    "            self.train_transformation_handles[\n",
    "                layer_id\n",
    "            ] = layer.register_forward_pre_hook(hook)\n",
    "\n",
    "    def hook_transformations(\n",
    "        self, transformations: Dict[str, nn.Module], n_samples: int\n",
    "    ) -> None:\n",
    "        for layer_id in transformations:\n",
    "            layer = self.seg_model.get_submodule(layer_id)\n",
    "            hook = self._get_transformation_hook(transformations[layer_id], n_samples)\n",
    "            self.transformation_handles[layer_id] = layer.register_forward_pre_hook(\n",
    "                hook\n",
    "            )\n",
    "            \n",
    "    def hook_inspect_transformation(\n",
    "        self, \n",
    "        transformations: Dict[str, nn.Module], \n",
    "        n_samples: int,\n",
    "        arch: str = 'ae'\n",
    "    ) -> None:\n",
    "        for layer_id in transformations:\n",
    "            if layer_id not in self.disabled_ids:\n",
    "                layer = self.seg_model.get_submodule(layer_id)\n",
    "                hook  = self._get_inspect_transformation_hook(transformations[layer_id], layer_id, n_samples, arch)\n",
    "                self.inspect_transformation_handles[layer_id] = layer.register_forward_pre_hook(hook)\n",
    "            \n",
    "\n",
    "    def _get_train_transformation_hook(\n",
    "        self, transformation: nn.Module, layer_id: str\n",
    "    ) -> Callable:\n",
    "        def hook(module: nn.Module, x: Tuple[Tensor]) -> Tensor:\n",
    "            x_in, *_ = x  # tuple, alternatively use x_in = x[0]\n",
    "            x_orig = x_in[:1]\n",
    "            #x_views = x_in[1:]\n",
    "            x_in_denoised = transformation(x_in)\n",
    "            if layer_id not in self.disabled_ids:\n",
    "                mse = nn.functional.mse_loss(x_in_denoised, x_orig.detach(), reduction=\"mean\")\n",
    "\n",
    "                training_data = {\n",
    "                    \"mse\": mse,\n",
    "                }\n",
    "\n",
    "                self.training_data[layer_id] = training_data\n",
    "\n",
    "            return x_in_denoised\n",
    "\n",
    "        return hook\n",
    "    \n",
    "\n",
    "    def _get_transformation_hook(\n",
    "        self, transformation: nn.Module, n_samples: int = 1\n",
    "    ) -> Callable:\n",
    "        def hook(module: nn.Module, x: Tuple[Tensor]) -> Tensor:\n",
    "            x_in, *_ = x  # weird tuple, can use x_in = x[0]\n",
    "            if n_samples == 0:\n",
    "                return x\n",
    "            elif n_samples == -1:\n",
    "                x_in_new = transformation(x_in)\n",
    "                return x_in_new\n",
    "            else:\n",
    "                x_in_new = x_in.unsqueeze(1).repeat(1, n_samples, 1, 1, 1).flatten(0, 1)\n",
    "                x_in_new = transformation(x_in_new)\n",
    "                return torch.cat([x_in, x_in_new], dim=0)\n",
    "\n",
    "        return hook\n",
    "            \n",
    "        \n",
    "    def _get_inspect_transformation_hook(\n",
    "            self, \n",
    "            transformation: nn.Module, \n",
    "            layer_id: str, \n",
    "            n_samples: int,\n",
    "            arch: str = 'ae',\n",
    "        ) -> Callable:\n",
    "        \n",
    "        @torch.no_grad()\n",
    "        def hook(module: nn.Module, x: Tuple[Tensor]) -> Tensor:\n",
    "            x_in, *_ = x  # weird tuple, can use x_in = x[0]\n",
    "            if n_samples == 0:\n",
    "                return x\n",
    "            elif n_samples == -1:\n",
    "                mu, log_var, x_in_new = transformation(x_in)\n",
    "            else:\n",
    "                x_in_new = x_in.unsqueeze(1).repeat(1, n_samples, 1, 1, 1).flatten(0, 1)\n",
    "                if arch == 'ae':\n",
    "                    x_in_new = transformation(x_in_new)\n",
    "                elif arch == 'res_ae':\n",
    "                    x_in_new, prior, residual = transformation(x_in_new)\n",
    "                x_in_new = torch.cat([x_in, x_in_new], dim=0)\n",
    "                \n",
    "            if layer_id not in self.disabled_ids:\n",
    "                training_data = {\n",
    "                    'input'  : x_in_new[ :1],\n",
    "                    'recon'  : x_in_new[1: ],\n",
    "                }\n",
    "                \n",
    "                if arch == 'res_ae':\n",
    "                    training_data['prior'] = prior\n",
    "                    training_data['residual'] = residual\n",
    "                \n",
    "                self.inspect_data[layer_id] = training_data\n",
    "            \n",
    "            return x_in_new\n",
    "        \n",
    "        return hook\n",
    "   \n",
    "    \n",
    "\n",
    "    def remove_train_transformation_hook(self, layer_id: str) -> None:\n",
    "        self.train_transformation_handles[layer_id].remove()\n",
    "\n",
    "    def remove_transformation_hook(self, layer_id: str) -> None:\n",
    "        self.transformation_handles[layer_id].remove()\n",
    "        \n",
    "    def remove_inspect_transformation_hook(self, layer_id: str) -> None:\n",
    "        self.inspect_transformation_handles[layer_id].remove()\n",
    "\n",
    "    def remove_all_hooks(self):\n",
    "        if hasattr(self, \"train_transformation_handles\"):\n",
    "            for handle in self.train_transformation_handles:\n",
    "                self.train_transformation_handles[handle].remove()\n",
    "            self.train_transformation_handles = {}\n",
    "\n",
    "        if hasattr(self, \"transformation_handles\"):\n",
    "            for handle in self.transformation_handles:\n",
    "                self.transformation_handles[handle].remove()\n",
    "            self.transformation_handles = {}\n",
    "            \n",
    "        if hasattr(self, 'inspect_transformation_handles'):\n",
    "            for handle in self.inspect_transformation_handles:\n",
    "                self.inspect_transformation_handles[handle].remove()\n",
    "            self.inspect_transformation_handles = {}\n",
    "        \n",
    "\n",
    "    def freeze_seg_model(self):\n",
    "        self.seg_model.eval()\n",
    "        for param in self.seg_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def set_number_of_samples_to(self, n_samples: int):\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "    def disable(self, layer_ids: list) -> None:\n",
    "        for layer_id in layer_ids:\n",
    "            self.transformations[layer_id].turn_off()\n",
    "\n",
    "    def enable(self, layer_ids: list) -> None:\n",
    "        for layer_id in layer_ids:\n",
    "            self.transformations[layer_id].turn_on()\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return self.seg_model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "665769e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Resize, CenterCrop\n",
    "from typing import Iterable, Dict, Callable, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "from utils import EarlyStopping, epoch_average, average_metrics\n",
    "\n",
    "class AETrainerACDC:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model: nn.Module, \n",
    "        unet: nn.Module, \n",
    "        criterion: Callable, \n",
    "        train_loader: DataLoader,\n",
    "        valid_loader: DataLoader,\n",
    "        num_batches_per_epoch: int, \n",
    "        num_val_batches_per_epoch: int, \n",
    "        description: str,\n",
    "        root: str,\n",
    "        target: str = 'output', #gt\n",
    "        lr: float = 5e-4,\n",
    "        n_epochs: int = 1000, \n",
    "        patience: int = 5, \n",
    "        es_mode: str = 'min', \n",
    "        eval_metrics: Dict[str, nn.Module] = None, \n",
    "        log: bool = True,\n",
    "        debug: bool = False\n",
    "    ):\n",
    "        self.device       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model        = model.to(self.device)\n",
    "        self.unet         = unet.to(self.device)\n",
    "        self.criterion    = criterion\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "        self.num_batches_per_epoch = num_batches_per_epoch\n",
    "        self.num_val_batches_per_epoch = num_val_batches_per_epoch\n",
    "        self.root         = root\n",
    "        self.description  = description\n",
    "        self.target       = target\n",
    "        self.lr           = lr\n",
    "        self.n_epochs     = n_epochs\n",
    "        self.patience     = patience\n",
    "        self.es_mode      = es_mode\n",
    "        self.eval_metrics = eval_metrics\n",
    "        self.log          = log\n",
    "        self.debug        = debug\n",
    "        self.optimizer    = torch.optim.Adam(model.parameters(), lr=self.lr)\n",
    "        self.scheduler    = ReduceLROnPlateau(self.optimizer, 'min', patience=self.patience)\n",
    "        self.es           = EarlyStopping(mode=self.es_mode, patience=2*self.patience)\n",
    "        self.scaler       = GradScaler()\n",
    "        self.history      = {'train loss': [], 'valid loss' : [], 'train metrics': [], 'valid metrics': []}\n",
    "        if self.eval_metrics is not None:\n",
    "            self.history = {**self.history, **{key: [] for key in self.eval_metrics.keys()}}\n",
    "        self.training_time = 0\n",
    "        self.crop   = CenterCrop([256, 256])\n",
    "        \n",
    "        \n",
    "        if target == 'output':\n",
    "            assert self.criterion.id_loss == 'huber'\n",
    "        elif target == 'gt':\n",
    "            assert self.criterion.id_loss in ['ce', 'bce']\n",
    "\n",
    "        \n",
    "    def inference_step(self, x):\n",
    "        x = x.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            unet_out = self.unet(x).detach()\n",
    "        samples  = self.model(x)\n",
    "        return unet_out, samples\n",
    "\n",
    "\n",
    "    def train_epoch(self):\n",
    "        loss_list, metric_list, batch_sizes = [], [], []\n",
    "        \n",
    "        for it in range(self.num_batches_per_epoch):\n",
    "            batch = next(self.train_loader)\n",
    "            input_ = torch.cat([batch['data_orig'], batch['data']], dim=0)\n",
    "            input_ = self.crop(input_)\n",
    "\n",
    "            with autocast():\n",
    "                unet_out, samples = self.inference_step(input_)\n",
    "                \n",
    "                if self.target == 'output':\n",
    "                    loss, metrics = self.criterion(unet_out, samples, \n",
    "                                                  self.model.training_data)\n",
    "                elif self.target == 'gt':\n",
    "                    # take only first element from targets. Others are for multi scale supervision\n",
    "                    #target = batch['target'][0].long().cuda()\n",
    "                    target = torch.cat([batch['target_orig'], batch['target'][0]], dim=0)\n",
    "                    target = target.long().to(self.device)\n",
    "                    target = F.one_hot(target).squeeze(1).permute(0,3,1,2)\n",
    "                    target = self.crop(target)\n",
    "                    loss, metrics = self.criterion(target.to(self.device), samples, \n",
    "                                                   self.model.training_data)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "            \n",
    "            loss_list.append(loss.item())\n",
    "            metric_list.append(metrics)\n",
    "            batch_sizes.append(input_.shape[0])\n",
    "            \n",
    "        average_loss = epoch_average(loss_list, batch_sizes)\n",
    "        metrics      = average_metrics(metric_list, batch_sizes)\n",
    "#         print(metrics['output_diff'])\n",
    "        \n",
    "        self.history['train loss'].append(average_loss)\n",
    "        self.history['train metrics'].append(metrics)\n",
    "        \n",
    "        if self.log:\n",
    "            wandb.log({\n",
    "                'train_loss': average_loss,\n",
    "                'train_metrics': metrics\n",
    "            }, commit=False)\n",
    "\n",
    "        return average_loss\n",
    "    \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    # Same story as in train_epoch\n",
    "    def eval_epoch(self):\n",
    "        loss_list, metric_list, batch_sizes = [], [], []\n",
    "        if self.eval_metrics is not None:\n",
    "            epoch_metrics = {key: [] for key in self.eval_metrics.keys()}\n",
    "        for it in range(self.num_val_batches_per_epoch):\n",
    "            batch = next(self.valid_loader)\n",
    "            input_ = torch.cat([batch['data_orig'], batch['data']], dim=0)\n",
    "            input_ = self.crop(input_)\n",
    "            print(input_.shape)\n",
    "            \n",
    "            # take only first element from targets. Others are for multi scale supervision\n",
    "            target = torch.cat([batch['target_orig'], batch['target'][0]], dim=0)\n",
    "            target = target.long().to(self.device)\n",
    "            target = F.one_hot(target).squeeze(1).permute(0,3,1,2)\n",
    "            target = self.crop(target)\n",
    "            print(target.shape, input_.get_device())\n",
    "\n",
    "            unet_out, samples = self.inference_step(input_)\n",
    "                        \n",
    "            if self.target == 'output':\n",
    "                loss, metrics = self.criterion(\n",
    "                    unet_out, \n",
    "                    samples, \n",
    "                    self.model.training_data)\n",
    "                \n",
    "            elif self.target == 'gt':\n",
    "                loss, metrics = self.criterion(\n",
    "                    target.to(self.device), \n",
    "                    samples, \n",
    "                    self.model.training_data)\n",
    "\n",
    "            loss_list.append(loss.item())\n",
    "            metric_list.append(metrics)\n",
    "            batch_sizes.append(input_.shape[0])\n",
    "            \n",
    "            if self.eval_metrics is not None:\n",
    "                for key, metric in self.eval_metrics.items():\n",
    "                    epoch_metrics[key].append(metric(unet_out, samples, target).mean().detach().cpu())\n",
    "        \n",
    "#         print(\"hi\")\n",
    "#         print(batch_sizes)\n",
    "#         print(\"eval before avrg\", metrics['output_diff'], batch_sizes)       \n",
    "        average_loss = epoch_average(loss_list, batch_sizes)\n",
    "        metrics      = average_metrics(metric_list, batch_sizes)\n",
    "#         print(\"eval after avrg\", metrics['output_diff'])\n",
    "        self.history['valid loss'].append(average_loss)\n",
    "        self.history['valid metrics'].append(metrics)\n",
    "        if self.eval_metrics is not None:\n",
    "            for key, epoch_scores in epoch_metrics.items():\n",
    "                avrg = epoch_average(epoch_scores, batch_sizes)\n",
    "                self.history[key].append(avrg)\n",
    "                if self.log:\n",
    "                    wandb.log({\n",
    "                        key: avrg\n",
    "                    }, commit=False)\n",
    "        \n",
    "        if self.log:\n",
    "            wandb.log({\n",
    "                'valid_loss': average_loss,\n",
    "                'valid_metrics': metrics\n",
    "            }, commit=False)\n",
    "        \n",
    "        return average_loss\n",
    "    \n",
    "    \n",
    "    def save_hist(self):\n",
    "        if(not os.path.exists(self.root+'results-tmp/trainer_logs')):\n",
    "            os.makedirs(self.root+'results-tmp/trainer_logs')\n",
    "        savepath = f'{self.root}results-tmp/trainer_logs/{self.description}.npy'\n",
    "        np.save(savepath, self.history)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def save_model(self):\n",
    "        if(not os.path.exists(self.root+'pre-trained-tmp/trained_AEs')):\n",
    "            os.makedirs(self.root+'pre-trained-tmp/trained_AEs')\n",
    "        if(not os.path.exists(self.root+'results-tmp/trainer_logs')):\n",
    "            os.makedirs(self.root+'results-tmp/trainer_logs')\n",
    "        savepath = f'{self.root}pre-trained-tmp/trained_AEs/{self.description}_best.pt'\n",
    "        torch.save({\n",
    "        'model_state_dict': self.model.state_dict(),\n",
    "        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, savepath)\n",
    "        self.save_hist()\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def load_model(self):\n",
    "        savepath = f'{self.root}pre-trained-tmp/trained_AEs/{self.description}_best.pt'\n",
    "        print(savepath)\n",
    "        checkpoint = torch.load(savepath)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        savepath = f'{self.root}results-tmp/trainer_logs/{self.description}.npy'\n",
    "        self.history = np.load(savepath,allow_pickle='TRUE').item()\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def plot_history(self):\n",
    "        plt.style.use('seaborn')\n",
    "        fig, ax = plt.subplots(1,2)\n",
    "        fig.set_size_inches(12,8)\n",
    "        fig.suptitle(self.description, fontsize=15)\n",
    "        ax[0].plot(self.history['train loss'], label='train loss', c='lightcoral', lw=3)\n",
    "        ax[0].plot(self.history['valid loss'], label='valid loss', c='cornflowerblue', lw=3)\n",
    "        ax[0].set_xlabel(\"epoch\")\n",
    "        ax[0].set_ylabel(\"loss\")\n",
    "        ax[0].legend()\n",
    "        ax[0].set_title('Training and Validation Losses')\n",
    "        \n",
    "        loss_array = np.array(self.history['Sample Volumetric Dice'])\n",
    "        ax[1].plot(self.history['Sample Volumetric Dice'], label='Sample', c='tab:blue', lw=3)\n",
    "        ax[1].plot(self.history['UNet Volumetric Dice'], label='UNet', c='tab:red', lw=3)\n",
    "        ax[1].set_xlabel(\"Epoch\")\n",
    "        ax[1].set_ylabel(\"Volumetric Dice\")\n",
    "        ax[1].legend(loc=\"lower right\")\n",
    "        ax[1].set_title(f'Volumetric Dice of Validation Set\\nBest Sample Dice: {loss_array.max()} @ Epoch {loss_array.argmax()+1}')\n",
    "        #plt.show()\n",
    "        \n",
    "        return fig\n",
    "\n",
    "    \n",
    "    def fit(self):\n",
    "        best_es_metric = 1e25 if self.es_mode == 'min' else -1e25\n",
    "        progress_bar = tqdm(range(self.n_epochs), total=self.n_epochs, position=0, leave=True)\n",
    "        \n",
    "        self.model.remove_all_hooks()\n",
    "        self.model.training_data = {}\n",
    "        self.model.hook_train_transformations(self.model.transformations)\n",
    "        self.unet.eval()\n",
    "        self.model.freeze_seg_model()\n",
    "\n",
    "        self.model.eval()\n",
    "        valid_loss = self.eval_epoch()\n",
    "        self.training_time = time.time()\n",
    "        \n",
    "        if self.log:\n",
    "            wandb.log({}, commit=True)\n",
    "        \n",
    "        for epoch in progress_bar:\n",
    "            #print(f\"Model param sum: {torch.tensor([param.sum() for param in trainer.network.parameters()]).sum()}\")\n",
    "\n",
    "            \n",
    "            self.model.train()\n",
    "            self.model.freeze_seg_model()\n",
    "            \n",
    "            train_loss = self.train_epoch()\n",
    "            self.model.eval()\n",
    "            valid_loss = self.eval_epoch()\n",
    "            self.scheduler.step(valid_loss)\n",
    "            \n",
    "            #epoch_summary = [f\"Epoch {epoch+1}\"] + [f\" - {key}: {self.history[key][-1]:.4f} |\" for key in self.history] + [ f\"ES epochs: {self.es.num_bad_epochs}\"]\n",
    "            keys = ['train loss', 'valid loss', 'Sample Volumetric Dice', 'UNet Volumetric Dice']\n",
    "            epoch_summary = [f\"Epoch {epoch+1}\"] + [f\" - {key}: {self.history[key][-1]:.4f} |\" for key in keys] + [ f\"ES epochs: {self.es.num_bad_epochs}\"]\n",
    "            progress_bar.set_description(\"\".join(epoch_summary))\n",
    "            es_metric = list(self.history.values())[1][-1]\n",
    "            \n",
    "            if self.log:\n",
    "                wandb.log({}, commit=True)\n",
    "            \n",
    "            if self.es_mode == 'min':\n",
    "                if es_metric < best_es_metric:\n",
    "                    best_es_metric = es_metric\n",
    "                    if not self.debug:\n",
    "                        self.save_model()\n",
    "            else:\n",
    "                if es_metric > best_es_metric:\n",
    "                    best_es_metric = es_metric\n",
    "                    if not self.debug:\n",
    "                        self.save_model()\n",
    "            if(self.es.step(es_metric)):\n",
    "                print('Early stopping triggered!')\n",
    "                break\n",
    "                \n",
    "        self.training_time = time.time() - self.training_time\n",
    "        if not self.debug:\n",
    "            self.save_hist()\n",
    "        self.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26d3baf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "### VAE Params\n",
    "layer_ids = ['shortcut0', 'shortcut1', 'shortcut2', 'up3']\n",
    "\n",
    "                   #    channel, spatial, latent,  depth, block \n",
    "dae_map   = {\n",
    "     #'shortcut0': [         8,     256,    128,     6,      1],\n",
    "     #'shortcut1': [        16,     128,    128,     5,      1],\n",
    "     #'shortcut2': [        32,      64,    128,     4,      1],\n",
    "     'up3':       [        64,      32,    128,     3,      1]}\n",
    "\n",
    "cfg['dae_map'] = dae_map\n",
    "if cfg['log']:\n",
    "    run = wandb.init(reinit=True, \n",
    "                     name=cfg['description'],\n",
    "                     project=cfg['project'], \n",
    "                     config=cfg)\n",
    "    cfg = wandb.config\n",
    "\n",
    "\n",
    "DAEs = nn.ModuleDict({key: AugResDAE(in_channels = dae_map[key][0], \n",
    "                                     in_dim      = dae_map[key][1],\n",
    "                                     latent_dim  = dae_map[key][2],\n",
    "                                     depth       = dae_map[key][3],\n",
    "                                     block_size  = dae_map[key][4]) for key in dae_map})\n",
    "\n",
    "for layer_id in cfg['disabled_ids']:\n",
    "     DAEs[layer_id] = nn.Identity()\n",
    "\n",
    "\n",
    "model = Frankenstein(unet, \n",
    "                     DAEs, \n",
    "                     disabled_ids=cfg['disabled_ids'],\n",
    "                     copy=True)\n",
    "\n",
    "model.cuda()\n",
    "print()\n",
    "if cfg['log']:\n",
    "    wandb.watch(model)\n",
    "\n",
    "criterion = MNMCriterionAE(\n",
    "    loss=cfg['loss'], \n",
    "    recon=cfg['reconstruction'], \n",
    "    diff=cfg['difference']\n",
    ")\n",
    "\n",
    "eval_metrics = {'Sample Volumetric Dice': SampleDice(data='MNM'),\n",
    "                'UNet Volumetric Dice': UnetDice(data='MNM')}\n",
    "\n",
    "vae_trainer = AETrainerACDC(model=model, \n",
    "                            unet=unet, \n",
    "                            criterion=criterion, \n",
    "                            train_loader=train_gen, \n",
    "                            valid_loader=valid_gen,\n",
    "                            num_batches_per_epoch=trainer.num_batches_per_epoch,\n",
    "                            num_val_batches_per_epoch=trainer.num_val_batches_per_epoch,\n",
    "                            root=root,\n",
    "                            target=cfg['target'],\n",
    "                            description=description,\n",
    "                            lr=cfg['lr'], \n",
    "                            eval_metrics=eval_metrics, \n",
    "                            log=cfg['log'],\n",
    "                            n_epochs=250, \n",
    "                            patience=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a1df55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0534f8a96dd14cc7b1fca8a0bb96a46f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 256, 256])\n",
      "torch.Size([3, 4, 256, 256]) -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [11,0,0], thread: [64,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [11,0,0], thread: [65,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [11,0,0], thread: [66,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [11,0,0], thread: [67,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [11,0,0], thread: [68,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [11,0,0], thread: [69,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [11,0,0], thread: [70,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [11,0,0], thread: [71,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [11,0,0], thread: [72,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [11,0,0], thread: [73,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [11,0,0], thread: [74,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [11,0,0], thread: [75,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [11,0,0], thread: [76,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [11,0,0], thread: [77,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [11,0,0], thread: [78,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [11,0,0], thread: [79,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [11,0,0], thread: [80,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [11,0,0], thread: [81,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [11,0,0], thread: [82,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [11,0,0], thread: [83,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [11,0,0], thread: [84,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [11,0,0], thread: [85,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [11,0,0], thread: [86,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [11,0,0], thread: [87,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [11,0,0], thread: [88,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [11,0,0], thread: [89,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [11,0,0], thread: [90,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [11,0,0], thread: [91,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [11,0,0], thread: [92,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [11,0,0], thread: [93,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [39,0,0], thread: [85,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [39,0,0], thread: [86,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [39,0,0], thread: [88,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [39,0,0], thread: [89,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [39,0,0], thread: [90,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [39,0,0], thread: [91,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [39,0,0], thread: [92,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [39,0,0], thread: [93,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [60,0,0], thread: [89,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [60,0,0], thread: [90,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [60,0,0], thread: [91,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [60,0,0], thread: [92,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [60,0,0], thread: [93,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [88,0,0], thread: [88,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [88,0,0], thread: [89,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [88,0,0], thread: [90,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [88,0,0], thread: [91,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [88,0,0], thread: [92,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [88,0,0], thread: [93,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [107,0,0], thread: [89,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [107,0,0], thread: [90,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [107,0,0], thread: [91,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [107,0,0], thread: [92,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n",
      "/opt/conda/conda-bld/pytorch_1656352465323/work/aten/src/ATen/native/cuda/ScatterGatherKernel.cu:365: operator(): block: [107,0,0], thread: [93,0,0] Assertion `idx_dim >= 0 && idx_dim < index_size && \"index out of bounds\"` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvae_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mAETrainerACDC.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfreeze_seg_model()\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m--> 270\u001b[0m valid_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog:\n",
      "File \u001b[0;32m~/anaconda3/envs/default/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mAETrainerACDC.eval_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    148\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrop(target)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mprint\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape, input_\u001b[38;5;241m.\u001b[39mget_device())\n\u001b[0;32m--> 151\u001b[0m unet_out, samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    154\u001b[0m     loss, metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(\n\u001b[1;32m    155\u001b[0m         unet_out, \n\u001b[1;32m    156\u001b[0m         samples, \n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtraining_data)\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mAETrainerACDC.inference_step\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minference_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 76\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     78\u001b[0m         unet_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munet(x)\u001b[38;5;241m.\u001b[39mdetach()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "vae_trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4b31a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "16384 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a6cd40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807d0659",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
