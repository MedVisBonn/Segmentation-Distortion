{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db3b26bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os, sys\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "\n",
    "from torchvision.transforms import Resize, CenterCrop\n",
    "from typing import Iterable, Dict, Callable, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('..')\n",
    "import wandb\n",
    "from tqdm.auto import tqdm\n",
    "from utils import EarlyStopping, epoch_average, average_metrics\n",
    "\n",
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from itertools import chain\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "import sys\n",
    "from batchgenerators.utilities.file_and_folder_operations import *\n",
    "from nnunet.training.dataloading.dataset_loading import *\n",
    "from nnunet.training.network_training.nnUNetTrainerV2 import nnUNetTrainerV2\n",
    "\n",
    "from dataset import CalgaryCampinasDataset\n",
    "from model.ae import AE\n",
    "from model.unet import UNet2D\n",
    "from model.wrapper import Frankenstein\n",
    "from losses import MNMCriterionAE, CalgaryCriterionAE, SampleDice, UnetDice\n",
    "from trainer.ae_trainer import AETrainerCalgary, AETrainerACDC\n",
    "\n",
    "\n",
    "nnUnet_prefix = '../../../nnUNet/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a060be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23e127cd0f349d0b710dd47658955df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../resources/trained_VAEs/calgary_ae_test0_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread ChkStopThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lennartz/anaconda3/envs/default/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/lennartz/anaconda3/envs/default/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lennartz/anaconda3/envs/default/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 170, in check_status\n",
      "    status_response = self._interface.communicate_stop_status()\n",
      "  File \"/home/lennartz/anaconda3/envs/default/lib/python3.8/site-packages/wandb/sdk/interface/interface.py\", line 127, in communicate_stop_status\n",
      "    resp = self._communicate_stop_status(status)\n",
      "  File \"/home/lennartz/anaconda3/envs/default/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\", line 395, in _communicate_stop_status\n",
      "    resp = self._communicate(req, local=True)\n",
      "  File \"/home/lennartz/anaconda3/envs/default/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\", line 226, in _communicate\n",
      "    return self._communicate_async(rec, local=local).get(timeout=timeout)\n",
      "  File \"/home/lennartz/anaconda3/envs/default/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\", line 231, in _communicate_async\n",
      "    raise Exception(\"The wandb backend process has shutdown\")\n",
      "Exception: The wandb backend process has shutdown\n",
      "Exception in thread NetStatThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lennartz/anaconda3/envs/default/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/lennartz/anaconda3/envs/default/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lennartz/anaconda3/envs/default/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 152, in check_network_status\n",
      "    status_response = self._interface.communicate_network_status()\n",
      "  File \"/home/lennartz/anaconda3/envs/default/lib/python3.8/site-packages/wandb/sdk/interface/interface.py\", line 138, in communicate_network_status\n",
      "    resp = self._communicate_network_status(status)\n",
      "  File \"/home/lennartz/anaconda3/envs/default/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\", line 405, in _communicate_network_status\n",
      "    resp = self._communicate(req, local=True)\n",
      "  File \"/home/lennartz/anaconda3/envs/default/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\", line 226, in _communicate\n",
      "    return self._communicate_async(rec, local=local).get(timeout=timeout)\n",
      "  File \"/home/lennartz/anaconda3/envs/default/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\", line 231, in _communicate_async\n",
      "    raise Exception(\"The wandb backend process has shutdown\")\n",
      "Exception: The wandb backend process has shutdown\n"
     ]
    }
   ],
   "source": [
    "cfg = {\n",
    "    'debug': False,\n",
    "    'log': True,\n",
    "    'description': 'calgary_ae_test',\n",
    "    'project': 'MICCAI2023',\n",
    "\n",
    "    # Data params\n",
    "    'n': 0,\n",
    "    'root': '../../',\n",
    "    'data_path': 'data/conp-dataset/projects/calgary-campinas/CC359/Reconstructed/',\n",
    "    'train_site': 6,\n",
    "    'unet': 'calgary_unet',\n",
    "    'channel_out': 8,\n",
    "    \n",
    "    # Hyperparams\n",
    "    'batch_size': 64,\n",
    "    'augment': False,\n",
    "    'difference': True,\n",
    "    'loss': 'huber',\n",
    "    'target': 'output',\n",
    "    'identity_layers': ['shortcut0', 'shortcut1', 'shortcut2'],\n",
    "    \n",
    "    # outputs\n",
    "    'plot_dir': '../experiments/unet/calgary/logs/'\n",
    "}\n",
    "\n",
    "\n",
    "description = cfg['description'] + str(cfg['n'])\n",
    "if cfg['augment']:\n",
    "    description += 'augment'\n",
    "\n",
    "### data loading \n",
    "root      = cfg['root']\n",
    "data_path = root + cfg['data_path']\n",
    "train_set = CalgaryCampinasDataset(data_path=data_path, \n",
    "                                   site=cfg['train_site'], \n",
    "                                   augment=cfg['augment'], \n",
    "                                   normalize=True, \n",
    "                                   split='train', \n",
    "                                   debug=cfg['debug'])\n",
    "\n",
    "valid_set = CalgaryCampinasDataset(data_path=data_path, \n",
    "                                   site=cfg['train_site'], \n",
    "                                   normalize=True, \n",
    "                                   split='validation', \n",
    "                                   debug=cfg['debug'])\n",
    "\n",
    "train_loader = DataLoader(train_set, \n",
    "                          batch_size=cfg['batch_size'], \n",
    "                          shuffle=True, \n",
    "                          drop_last=False,\n",
    "                          num_workers=10)\n",
    "\n",
    "valid_loader = DataLoader(valid_set, \n",
    "                          batch_size=cfg['batch_size'], \n",
    "                          shuffle=False, \n",
    "                          drop_last=False,\n",
    "                          num_workers=10)\n",
    "\n",
    "\n",
    "### Unet\n",
    "unet_path = cfg['unet'] + str(cfg['n'])\n",
    "seg_model = UNet2D(n_chans_in=1, n_chans_out=1, n_filters_init=cfg['channel_out']).to(0)\n",
    "model_path = f'{root}pre-trained-tmp/trained_UNets/{unet_path}_best.pt'\n",
    "state_dict = torch.load(model_path)['model_state_dict']\n",
    "seg_model.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "### AE Params\n",
    "layer_ids = ['shortcut0', 'shortcut1', 'shortcut2', 'up3']\n",
    "\n",
    "\n",
    "                   # channel, spatial, latent, depth\n",
    "ae_map   = {'up3': [     64,      32,     64,     2]}\n",
    "\n",
    "\n",
    "AEs = nn.ModuleDict({layer_id: AE(in_channels = ae_map[layer_id][0], \n",
    "                                  in_dim      = ae_map[layer_id][1],\n",
    "                                  latent_dim  = ae_map[layer_id][2],\n",
    "                                  depth       = ae_map[layer_id][3],\n",
    "                                  block_size  = 4) \n",
    "                          for layer_id in layer_ids if layer_id not in cfg['identity_layers']})\n",
    "\n",
    "\n",
    "for layer_id in cfg['identity_layers']:\n",
    "    AEs[layer_id] = nn.Identity()\n",
    "\n",
    "model = Frankenstein(seg_model, \n",
    "                     AEs, \n",
    "                     disabled_ids=cfg['identity_layers'],\n",
    "                     copy=True)\n",
    "\n",
    "criterion = CalgaryCriterionAE(loss=cfg['loss'])\n",
    "\n",
    "eval_metrics = {'Sample Volumetric Dice': SampleDice(data='calgary'),\n",
    "                'UNet Volumetric Dice': UnetDice(data='calgary')}\n",
    "\n",
    "trainer = AETrainerCalgary(model=model, \n",
    "                    unet=seg_model, \n",
    "                    criterion=criterion, \n",
    "                    train_loader=train_loader, \n",
    "                    valid_loader=valid_loader, \n",
    "                    root=root,\n",
    "                    target=cfg['target'],\n",
    "                    description=description,\n",
    "                    lr=1e-4, \n",
    "                    eval_metrics=eval_metrics, \n",
    "                    log=cfg['log'],\n",
    "                    n_epochs=1,\n",
    "                    patience=4) #20\n",
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1c5ec3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset\n",
      "loading all case properties\n",
      "2023-06-26 15:24:32.872452: Using splits from existing split file: ../../../nnUNet/data/nnUNet_preprocessed/Task500_ACDC/splits_final.pkl\n",
      "2023-06-26 15:24:32.873705: The split file contains 5 splits.\n",
      "2023-06-26 15:24:32.873985: Desired fold for training: 0\n",
      "2023-06-26 15:24:32.874429: This split has 160 training and 40 validation cases.\n",
      "unpacking dataset\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjlennartz\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/lennartz/repos/Segmentation-Distortion/src/demos/wandb/run-20230626_152434-36ka13wq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jlennartz/MICCAI2023/runs/36ka13wq\" target=\"_blank\">acdc_ae_test</a></strong> to <a href=\"https://wandb.ai/jlennartz/MICCAI2023\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cfg = {\n",
    "        'debug': False,\n",
    "        'log': False,\n",
    "        'description': f'acdc_ae_test', #'mms_vae_for_nnUNet_fc3_0_bs50',\n",
    "        'project': 'MICCAI2023',\n",
    "\n",
    "        # Data params\n",
    "        'n': 0,\n",
    "        'root': '../../',\n",
    "        'data_path': 'data/mnm/',\n",
    "        'train_vendor': 'B',\n",
    "        'unet': f'acdc_unet8_0',\n",
    "        'channel_out': 8,\n",
    "\n",
    "        # Hyperparams\n",
    "        'batch_size': 32,\n",
    "        'augment': False,\n",
    "        'difference': True,\n",
    "        'loss': 'huber',  # huber or ce\n",
    "        'target': 'output', #gt or output\n",
    "        'disabled_ids': ['shortcut0', 'shortcut1', 'shortcut2']\n",
    "}\n",
    "\n",
    "description = cfg['description']\n",
    "root = cfg['root']\n",
    "\n",
    "# Unet\n",
    "unet_path = cfg['unet'] # + str(cfg['n'])\n",
    "unet = UNet2D(n_chans_in=1, n_chans_out=4, n_filters_init=cfg['channel_out']).to(0)\n",
    "model_path = f'{root}pre-trained-tmp/trained_UNets/{unet_path}_best.pt'\n",
    "state_dict = torch.load(model_path)['model_state_dict']\n",
    "unet.load_state_dict(state_dict)\n",
    "\n",
    "### Dataloader\n",
    "## Initialize trainer to get data loaders with data augmentations from training\n",
    "pkl_file          = nnUnet_prefix + 'data/nnUNet_preprocessed/Task500_ACDC/nnUNetPlansv2.1_plans_2D.pkl'\n",
    "fold              = 0\n",
    "output_folder     = nnUnet_prefix + 'results/nnUnet/nnUNet/2d/Task027_ACDC/nnUNetTrainerV2__nnUNetPlansv2.1/'\n",
    "dataset_directory = nnUnet_prefix + 'data/nnUNet_preprocessed/Task500_ACDC'\n",
    "\n",
    "trainer = nnUNetTrainerV2(pkl_file, 0, output_folder, dataset_directory)\n",
    "trainer.initialize()\n",
    "\n",
    "train_loader = trainer.tr_gen\n",
    "valid_loader = trainer.val_gen\n",
    "\n",
    "\n",
    "### VAE Params\n",
    "layer_ids = ['shortcut0', 'shortcut1', 'shortcut2', 'up3']\n",
    "\n",
    "                   #    channel, spatial, latent,  depth, block \n",
    "dae_map   = {'up3': [        64,      32,    128,     2,      4]}\n",
    "\n",
    "cfg['dae_map'] = dae_map\n",
    "if cfg['log']:\n",
    "    run = wandb.init(reinit=True, \n",
    "                     name=cfg['description'],\n",
    "                     project=cfg['project'], \n",
    "                     config=cfg)\n",
    "    cfg = wandb.config\n",
    "\n",
    "\n",
    "DAEs = nn.ModuleDict({'up3': AE(in_channels = dae_map['up3'][0], \n",
    "                                    in_dim      = dae_map['up3'][1],\n",
    "                                    latent_dim  = dae_map['up3'][2],\n",
    "                                    depth       = dae_map['up3'][3],\n",
    "                                    block_size  = dae_map['up3'][4])})\n",
    "\n",
    "for layer_id in cfg['disabled_ids']:\n",
    "     DAEs[layer_id] = nn.Identity()\n",
    "\n",
    "\n",
    "model = Frankenstein(unet, \n",
    "                     DAEs, \n",
    "                     disabled_ids=cfg['disabled_ids'],\n",
    "                     copy=True)\n",
    "\n",
    "model.cuda()\n",
    "print()\n",
    "criterion    = MNMCriterionAE(loss=cfg['loss'], diff=cfg['difference'])\n",
    "eval_metrics = {'Sample Volumetric Dice': SampleDice(data='MNM'),\n",
    "                'UNet Volumetric Dice': UnetDice(data='MNM')}\n",
    "\n",
    "vae_trainer = AETrainerACDC(model=model, \n",
    "                         unet=unet, \n",
    "                         criterion=criterion, \n",
    "                         train_loader=train_loader, \n",
    "                         valid_loader=valid_loader, \n",
    "                         num_batches_per_epoch=trainer.num_batches_per_epoch,\n",
    "                         num_val_batches_per_epoch=trainer.num_val_batches_per_epoch,\n",
    "                         root=root,\n",
    "                         target=cfg['target'],\n",
    "                         description=description,\n",
    "                         lr=1e-4, \n",
    "                         eval_metrics=eval_metrics, \n",
    "                         log=cfg['log'],\n",
    "                         n_epochs=1, \n",
    "                         patience=8)\n",
    "\n",
    "\n",
    "vae_trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91332908",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
